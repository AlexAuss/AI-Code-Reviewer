
Overview
The Final Project Report (6–8 pages) is the polished, complete version of your semester project.
You may reuse material from your Midterm Report, but you must revise and correct any
content that was inaccurate, incomplete, or outdated. If you reuse midterm content without fixing
errors, you may lose points in the corresponding sections.
Required Sections (6–8 pages)

1. Introduction & Problem Definition (0.5–1 page)
Describe:
• Your research question or task
• Motivation and real-world relevance
• Connections to course topics (prompting, RAG, evaluation, etc.)
Ensure this reflects the final scope and direction of your project.

2. Dataset (0.5–1 page)
Provide a clear description of:
• Dataset sources, size, structure, and characteristics
• Preprocessing and filtering steps
• Ethical considerations (bias, privacy, licensing)
Update this section to match the dataset actually used in your final implementation.
For the overview, you will get the idea from the "Midterm_report.txt" file. For the preprocessing and filtering step, you could see our implementation from "src/utils/Data_Merger_Module.py". Also write something about the biasness of the dataset if possible. Here is the link of the source of our dataset: "https://zenodo.org/records/6900648". You can link this as well as the footernote in the latex. 

3. Methodology (2–2.5 pages)
This is one of the core sections of the report. Include:
• System or architecture diagrams(I have diagram which I have given you as attachment)
Review all of the code to see the updated implementations from "src/utils/Data_Merger_Module.py"(Unified Dataset Generation) -> "src/indexing/build_indexes.py"(Indexes) + "src/indexing/build_bm25s_index.py + convert_faiss_ivf.py" (Optimized indexes for fast retrieval, this is one optimization we have done after midterm) + "src/indexing/setup_mongodb.py + db_config.py"(for metadata store in mongodb) -> "src/indexing/hybrid_retirever.py"(Dense + Sparse Retriever + RRF fusion) -> "src/indexing/evaluation_deepseek.py + evaluation_qwen.py" (For LLM generation and evaluation pipeline)(we want ignored BertScore as it didn't get good results, but don't include anything about BertScore in that section, just include ROUGE-L F1 - for review comment and CodeBleu Score - for refined code)
Regarding prompting strategy: see the implementation of "src/utils/llm_model.py"(talk how explicitly mentioning model about '+' and '-' let us get good results as model could understand our motive to get the review of code changes not the code actually)

• Prompting strategy, retrieval pipeline, evaluation workflow(talk about those very well from our implementation)

• Hyperparameters, implementation details, and engineering decisions(see "src/evaluation/find_optimal_k") - we tune top k value and threshold limit see the implementation and for the evaluation result see the file "data/indexes/evaluation_results.jsonl".
• Any constraints (API limits, computation time) - (Alex - API limits, I will give someinformation about it later) - keep this part empty and make a note so that I could include the information later here

This section should provide enough detail for reproducibility.


4. Final Results (2–2.5 pages)
This is the most important section of the report. Replace any midterm preliminary results with
final, complete results.

For the results see the file "running_averages.txt". 

For the final results 
• Quantitative results: e.g., evaluations - get the results from above file, say about the challenges we faced solving the CodeBleu and Tree-sitter framework version compatibility issues and how we solved it. 
• Qualitative examples: e.g., model outputs, comparisons, and error cases - we will provide a sample images showing our model input output.

• Comparisons: e.g., baselines, alternative prompts, RAG variants, 
• Interpretation: explain observed patterns and system behavior
Use tables, plots, and figures where appropriate.

While generating latex keep placeholder for diagrams. we will generate based on the caption you will provide.


5. Short Reflection, Limitations & Future Work (0.5–1 page total)
This section should be concise to emphasize methodology and results.
• Reflection: Key lessons learned or meaningful changes since the midterm - (mention it here - challenges, what we faced during running evaluations - Alex), mention the optimization part of both indexing(meaningful changes like it was taking around 20 seconds genarting retrieved examples(top 3) after improving it we got retrieval examples within 2~3 seconds), 
• Limitations: Dataset issues(mention issue - Alex), model weaknesses(we couldn't use the higher parameter model which has better capabilities due to our contrained resources), error patterns(Alex), evaluation constraints(Models is not doing better(Deepseek, Qwen - 0.5, 1.5, 3), we couldn't run pass@k for refined code as it taking too much time, we are getting negative results on BertScore(for review comment) which is not expected)
• Future Work: Realistic improvements or extensions(bertscore integration to see why it is getting negative, try with fine-tuning - tell RAG's limitation though we need more powerful GPU's to experiments with this)


Formatting Requirements
• 6–8 pages, single-column layout
• 11 pt font, 1-inch margins
• Figures and tables strongly encouraged
• Include title of the project and team member names on page 1