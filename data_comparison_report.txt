Dataset comparison report
========================

Dataset: Code_Refinement
Path: Datasets/Code_Refinement/ref-train.jsonl
Total lines (records): 150406
Top keys and frequencies:
  old_hunk: 150406
  oldf: 150406
  hunk: 150406
  comment: 150406
  ids: 150406
  repo: 150406
  ghid: 150406
  old: 150406
  new: 150406
  lang: 150406

Samples (first records):
--- Sample 1 ---
{
  "old_hunk": "@@ -48,23 +59,29 @@ bool TransformationAddGlobalVariable::IsApplicable(\n   if (!pointer_type) {\n     return false;\n   }\n-  // ... with Private storage class.\n-  if (pointer_type->storage_class() != SpvStorageClassPrivate) {\n+  // ... with the right storage class.\n+  if (pointer_type->storage_class() != storage_class) {\n     return false;\n   }\n-  // The initializer id must be the id of a constant.  Check this with the\n-  // constant manager.\n-  auto constant_id = ir_context->get_constant_mgr()->GetConstantsFromIds(\n-      {message_.initializer_id()});\n-  if (constant_id.empty()) {\n-    return false;\n-  }\n-  assert(constant_id.size() == 1 &&\n-         \"We asked for the constant associated with a single id; we should \"\n-         \"get a single constant.\");\n-  // The type of the constant must match the pointee type of the pointer.\n-  if (pointer_type->pointee_type() != constant_id[0]->type()) {\n-    return false;\n+  if (message_.initializer_id()) {\n+    // An initializer is not allowed if the storage class is Workgroup.\n+    if (storage_class == SpvStorageClassWorkgroup) {\n+      return false;\n+    }",
  "oldf": "// Copyright (c) 2019 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n#include \"source/fuzz/transformation_add_global_variable.h\"\n\n#include \"source/fuzz/fuzzer_util.h\"\n\nnamespace spvtools {\nnamespace fuzz {\n\nTransformationAddGlobalVariable::TransformationAddGlobalVariable(\n    const spvtools::fuzz::protobufs::TransformationAddGlobalVariable& message)\n    : message_(message) {}\n\nTransformationAddGlobalVariable::TransformationAddGlobalVariable(\n    uint32_t fresh_id, uint32_t type_id, SpvStorageClass storage_class,\n    uint32_t initializer_id, bool value_is_irrelevant) {\n  message_.set_fresh_id(fresh_id);\n  message_.set_type_id(type_id);\n  message_.set_storage_class(storage_class);\n  message_.set_initializer_id(initializer_id);\n  message_.set_value_is_irrelevant(value_is_irrelevant);\n}\n\nbool TransformationAddGlobalVariable::IsApplicable(\n    opt::IRContext* ir_context, const TransformationContext& /*unused*/) const {\n  // The result id must be fresh.\n  if (!fuzzerutil::IsFreshId(ir_context, message_.fresh_id())) {\n    return false;\n  }\n\n  // The storage class must be Private or Workgroup.\n  auto storage_class = static_cast<SpvStorageClass>(message_.storage_class());\n  switch (storage_class) {\n    case SpvStorageClassPrivate:\n    case SpvStorageClassWorkgroup:\n      break;\n    default:\n      return false;\n  }\n  // The type id must correspond to a type.\n  auto type = ir_context->get_type_mgr()->GetType(message_.type_id());\n  if (!type) {\n    return false;\n  }\n  // That type must be a pointer type ...\n  auto pointer_type = type->AsPointer();\n  if (!pointer_type) {\n    return false;\n  }\n  // ... with the right storage class.\n  if (pointer_type->storage_class() != storage_class) {\n    return false;\n  }\n  if (message_.initializer_id()) {\n    // An initializer is not allowed if the storage class is Workgroup.\n    if (storage_class == SpvStorageClassWorkgroup) {\n      return false;\n    }\n    // The initializer id must be the id of a constant.  Check this with the\n    // constant manager.\n    auto constant_id = ir_context->get_constant_mgr()->GetConstantsFromIds(\n        {message_.initializer_id()});\n    if (constant_id.empty()) {\n      return false;\n    }\n    assert(constant_id.size() == 1 &&\n           \"We asked for the constant associated with a single id; we should \"\n           \"get a single constant.\");\n    // The type of the constant must match the pointee type of the pointer.\n    if (pointer_type->pointee_type() != constant_id[0]->type()) {\n      return false;\n    }\n  }\n  return true;\n}\n\nvoid TransformationAddGlobalVariable::Apply(\n    opt::IRContext* ir_context,\n    TransformationContext* transformation_context) const {\n  opt::Instruction::OperandList input_operands;\n  input_operands.push_back(\n      {SPV_OPERAND_TYPE_STORAGE_CLASS, {message_.storage_class()}});\n  if (message_.initializer_id()) {\n    input_operands.push_back(\n        {SPV_OPERAND_TYPE_ID, {message_.initializer_id()}});\n  }\n  ir_context->module()->AddGlobalValue(MakeUnique<opt::Instruction>(\n      ir_context, SpvOpVariable, message_.type_id(), message_.fresh_id(),\n      input_operands));\n  fuzzerutil::UpdateModuleIdBound(ir_context, message_.fresh_id());\n\n  if (GlobalVariablesMustBeDeclaredInEntryPointInterfaces(ir_context)) {\n    // Conservatively add this global to the interface of every entry point in\n    // the module.  This means that the global is available for other\n    // transformations to use.\n    //\n    // A downside of this is that the global will be in the interface even if it\n    // ends up never being used.\n    //\n    // TODO(https://github.com/KhronosGroup/SPIRV-Tools/issues/3111) revisit\n    //  this if a more thorough approach to entry point interfaces is taken.\n    for (auto& entry_point : ir_context->module()->entry_points()) {\n      entry_point.AddOperand({SPV_OPERAND_TYPE_ID, {message_.fresh_id()}});\n    }\n  }\n\n  if (message_.value_is_irrelevant()) {\n    transformation_context->GetFactManager()->AddFactValueOfPointeeIsIrrelevant(\n        message_.fresh_id());\n  }\n\n  // We have added an instruction to the module, so need to be careful about the\n  // validity of existing analyses.\n  ir_context->InvalidateAnalysesExceptFor(\n      opt::IRContext::Analysis::kAnalysisNone);\n}\n\nprotobufs::Transformation TransformationAddGlobalVariable::ToMessage() const {\n  protobufs::Transformation result;\n  *result.mutable_add_global_variable() = message_;\n  return result;\n}\n\nbool TransformationAddGlobalVariable::\n    GlobalVariablesMustBeDeclaredInEntryPointInterfaces(\n        opt::IRContext* ir_context) {\n  // TODO(afd): We capture the universal environments for which this requirement\n  //  holds.  The check should be refined on demand for other target\n  //  environments.\n  switch (ir_context->grammar().target_env()) {\n    case SPV_ENV_UNIVERSAL_1_0:\n    case SPV_ENV_UNIVERSAL_1_1:\n    case SPV_ENV_UNIVERSAL_1_2:\n    case SPV_ENV_UNIVERSAL_1_3:\n      return false;\n    default:\n      return true;\n  }\n}\n\n}  // namespace fuzz\n}  // namespace spvtools\n",
  "hunk": "@@ -66,6 +66,9 @@ bool TransformationAddGlobalVariable::IsApplicable(\n   if (message_.initializer_id()) {\n     // An initializer is not allowed if the storage class is Workgroup.\n     if (storage_class == SpvStorageClassWorkgroup) {\n+      assert(false &&\n+             \"By construction this transformation should not have an \"\n+             \"initializer when Workgroup storage class is used.\");\n       return false;\n     }\n     // The initializer id must be the id of a constant.  Check this with the\n",
  "comment": "Maybe assert false",
  "ids": [
    24794,
    "17d785e3dce428351b39d1290ec28a383342c8cb",
    "91569fc31a46c739ae5ccf18437d5b4782ca3c3a"
  ],
  "repo": "KhronosGroup/SPIRV-Tools",
  "ghid": 3277,
  "old": "   if (message_.initializer_id()) {\n     // An initializer is not allowed if the storage class is Workgroup.\n     if (storage_class == SpvStorageClassWorkgroup) {\n       return false;\n     }\n     // The initializer id must be the id of a constant.  Check this with the",
  "new": "   if (message_.initializer_id()) {\n     // An initializer is not allowed if the storage class is Workgroup.\n     if (storage_class == SpvStorageClassWorkgroup) {\n+      assert(false &&\n+             \"By construction this transformation should not have an \"\n+             \"initializer when Workgroup storage class is used.\");\n       return false;\n     }\n     // The initializer id must be the id of a constant.  Check this with the",
  "lang": "cpp"
}

--- Sample 2 ---
{
  "old_hunk": "@@ -13,21 +13,17 @@\n \n public class OnThisDayActivity extends SingleFragmentActivity<OnThisDayFragment> {\n     public static final String AGE = \"age\";\n+    public static final String YEAR = \"year\";\n     public static final String WIKISITE = \"wikisite\";\n \n-    public static Intent newIntent(@NonNull Context context, int age, WikiSite wikiSite, InvokeSource invokeSource) {\n+    public static Intent newIntent(@NonNull Context context, int age, WikiSite wikiSite, InvokeSource invokeSource, int year) {",
  "oldf": "package org.wikipedia.feed.onthisday;\n\nimport android.content.Context;\nimport android.content.Intent;\n\nimport androidx.annotation.NonNull;\n\nimport org.wikipedia.Constants.InvokeSource;\nimport org.wikipedia.activity.SingleFragmentActivity;\nimport org.wikipedia.dataclient.WikiSite;\n\nimport static org.wikipedia.Constants.INTENT_EXTRA_INVOKE_SOURCE;\n\npublic class OnThisDayActivity extends SingleFragmentActivity<OnThisDayFragment> {\n    public static final String AGE = \"age\";\n    public static final String YEAR = \"year\";\n    public static final String WIKISITE = \"wikisite\";\n\n    public static Intent newIntent(@NonNull Context context, int age, WikiSite wikiSite, InvokeSource invokeSource, int year) {\n        return new Intent(context, OnThisDayActivity.class)\n                .putExtra(AGE, age)\n                .putExtra(WIKISITE, wikiSite)\n                .putExtra(YEAR, year)\n                .putExtra(INTENT_EXTRA_INVOKE_SOURCE, invokeSource);\n    }\n\n    @Override\n    protected OnThisDayFragment createFragment() {\n        return OnThisDayFragment.newInstance(getIntent().getIntExtra(AGE, 0), getIntent().getParcelableExtra(WIKISITE));\n    }\n}\n",
  "hunk": "@@ -16,7 +16,7 @@ public class OnThisDayActivity extends SingleFragmentActivity<OnThisDayFragment>\n     public static final String YEAR = \"year\";\n     public static final String WIKISITE = \"wikisite\";\n \n-    public static Intent newIntent(@NonNull Context context, int age, WikiSite wikiSite, InvokeSource invokeSource, int year) {\n+    public static Intent newIntent(@NonNull Context context, int age, @NonNull WikiSite wikiSite, @NonNull InvokeSource invokeSource, int year) {\n         return new Intent(context, OnThisDayActivity.class)\n                 .putExtra(AGE, age)\n                 .putExtra(WIKISITE, wikiSite)\n",
  "comment": "Would it be better if add annotations to the parameters?",
  "ids": [
    20887,
    "3e39aa16bb601c90e73b98249a9dcd77f17d511e",
    "9ee2d9cfbcafbce2b81669cc6adf3936e4b7b94f"
  ],
  "repo": "wikimedia/apps-android-wikipedia",
  "ghid": 1602,
  "old": "     public static final String YEAR = \"year\";\n     public static final String WIKISITE = \"wikisite\";\n-    public static Intent newIntent(@NonNull Context context, int age, WikiSite wikiSite, InvokeSource invokeSource, int year) {\n         return new Intent(context, OnThisDayActivity.class)\n                 .putExtra(AGE, age)\n                 .putExtra(WIKISITE, wikiSite)",
  "new": "     public static final String YEAR = \"year\";\n     public static final String WIKISITE = \"wikisite\";\n+    public static Intent newIntent(@NonNull Context context, int age, @NonNull WikiSite wikiSite, @NonNull InvokeSource invokeSource, int year) {\n         return new Intent(context, OnThisDayActivity.class)\n                 .putExtra(AGE, age)\n                 .putExtra(WIKISITE, wikiSite)",
  "lang": "java"
}

--- Sample 3 ---
{
  "old_hunk": "@@ -80,7 +80,7 @@ def target_url\n     return external_url if external_url\n     return \"#{Discourse::base_uri}#{post.url}\" if post\n     return topic.relative_url if topic\n-    return \"#{category.url}/#{category.id}\" if category\n+    return \"#{category.url}\" if category",
  "oldf": "# frozen_string_literal: true\n\nclass Permalink < ActiveRecord::Base\n  belongs_to :topic\n  belongs_to :post\n  belongs_to :category\n  belongs_to :tag\n\n  before_validation :normalize_url\n\n  class Normalizer\n    attr_reader :source\n\n    def initialize(source)\n      @source = source\n      if source.present?\n        @rules = source.split(\"|\").map do |rule|\n          parse_rule(rule)\n        end.compact\n      end\n    end\n\n    def parse_rule(rule)\n      return unless rule =~ /\\/.*\\//\n\n      escaping = false\n      regex = +\"\"\n      sub = +\"\"\n      c = 0\n\n      rule.chars.each do |l|\n        c += 1 if !escaping && l == \"/\"\n        escaping = l == \"\\\\\"\n\n        if c > 1\n          sub << l\n        else\n          regex << l\n        end\n      end\n\n      if regex.length > 1\n        [Regexp.new(regex[1..-1]), sub[1..-1] || \"\"]\n      end\n\n    end\n\n    def normalize(url)\n      return url unless @rules\n      @rules.each do |(regex, sub)|\n        url = url.sub(regex, sub)\n      end\n\n      url\n    end\n\n  end\n\n  def self.normalize_url(url)\n    if url\n      url = url.strip\n      url = url[1..-1] if url[0, 1] == '/'\n    end\n\n    normalizations = SiteSetting.permalink_normalizations\n\n    @normalizer = Normalizer.new(normalizations) unless @normalizer && @normalizer.source == normalizations\n    @normalizer.normalize(url)\n  end\n\n  def self.find_by_url(url)\n    find_by(url: normalize_url(url))\n  end\n\n  def normalize_url\n    self.url = Permalink.normalize_url(url) if url\n  end\n\n  def target_url\n    return external_url if external_url\n    return \"#{Discourse::base_uri}#{post.url}\" if post\n    return topic.relative_url if topic\n    return \"#{category.url}\" if category\n    return tag.full_url if tag\n    nil\n  end\n\n  def self.filter_by(url = nil)\n    permalinks = Permalink\n      .includes(:topic, :post, :category, :tag)\n      .order('permalinks.created_at desc')\n\n    permalinks.where!('url ILIKE :url OR external_url ILIKE :url', url: \"%#{url}%\") if url.present?\n    permalinks.limit!(100)\n    permalinks.to_a\n  end\nend\n\n# == Schema Information\n#\n# Table name: permalinks\n#\n#  id           :integer          not null, primary key\n#  url          :string(1000)     not null\n#  topic_id     :integer\n#  post_id      :integer\n#  category_id  :integer\n#  created_at   :datetime         not null\n#  updated_at   :datetime         not null\n#  external_url :string(1000)\n#  tag_id       :integer\n#\n# Indexes\n#\n#  index_permalinks_on_url  (url) UNIQUE\n#\n",
  "hunk": "@@ -80,7 +80,7 @@ class Permalink < ActiveRecord::Base\n     return external_url if external_url\n     return \"#{Discourse::base_uri}#{post.url}\" if post\n     return topic.relative_url if topic\n-    return \"#{category.url}\" if category\n+    return category.url if category\n     return tag.full_url if tag\n     nil\n   end\n",
  "comment": "```suggestion return category.url if category ```",
  "ids": [
    81755,
    "1f6f246f4a1ccc855ecefe34e316d8bbb83512c0",
    "2948445f139ceb4b435465d3715a73c6f105acc7"
  ],
  "repo": "discourse/discourse",
  "ghid": 9972,
  "old": "     return external_url if external_url\n     return \"#{Discourse::base_uri}#{post.url}\" if post\n     return topic.relative_url if topic\n-    return \"#{category.url}\" if category\n     return tag.full_url if tag\n     nil\n   end",
  "new": "     return external_url if external_url\n     return \"#{Discourse::base_uri}#{post.url}\" if post\n     return topic.relative_url if topic\n+    return category.url if category\n     return tag.full_url if tag\n     nil\n   end",
  "lang": "rb"
}

--- Sample 4 ---
{
  "old_hunk": "@@ -250,13 +250,22 @@ func (c *twoPhaseCommitter) prewriteSingleBatch(bo *Backoffer, batch batchKeys)\n \tfor i, k := range batch.keys {\n \t\tmutations[i] = c.mutations[string(k)]\n \t}\n+\n+\tskipCheck := false\n+\toptSkipCheck := c.txn.us.GetOption(kv.SkipCheckForWrite)\n+\tif optSkipCheck != nil {",
  "oldf": "// Copyright 2016 PingCAP, Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage tikv\n\nimport (\n\t\"bytes\"\n\t\"math\"\n\t\"sync\"\n\n\t\"github.com/juju/errors\"\n\t\"github.com/ngaut/log\"\n\tpb \"github.com/pingcap/kvproto/pkg/kvrpcpb\"\n\t\"github.com/pingcap/tidb/kv\"\n\t\"github.com/pingcap/tidb/sessionctx/binloginfo\"\n\t\"github.com/pingcap/tipb/go-binlog\"\n\t\"golang.org/x/net/context\"\n)\n\ntype twoPhaseCommitAction int\n\nconst (\n\tactionPrewrite twoPhaseCommitAction = 1\n\tactionCommit   twoPhaseCommitAction = 2\n\tactionCleanup  twoPhaseCommitAction = 3\n)\n\nfunc (ca twoPhaseCommitAction) String() string {\n\tswitch ca {\n\tcase actionPrewrite:\n\t\treturn \"prewrite\"\n\tcase actionCommit:\n\t\treturn \"commit\"\n\tcase actionCleanup:\n\t\treturn \"cleanup\"\n\t}\n\treturn \"unknown\"\n}\n\n// twoPhaseCommitter executes a two-phase commit protocol.\ntype twoPhaseCommitter struct {\n\tstore     *tikvStore\n\ttxn       *tikvTxn\n\tstartTS   uint64\n\tkeys      [][]byte\n\tmutations map[string]*pb.Mutation\n\tlockTTL   uint64\n\tcommitTS  uint64\n\tmu        struct {\n\t\tsync.RWMutex\n\t\twrittenKeys [][]byte\n\t\tcommitted   bool\n\t}\n}\n\n// newTwoPhaseCommitter creates a twoPhaseCommitter.\nfunc newTwoPhaseCommitter(txn *tikvTxn) (*twoPhaseCommitter, error) {\n\tvar keys [][]byte\n\tvar size int\n\tmutations := make(map[string]*pb.Mutation)\n\terr := txn.us.WalkBuffer(func(k kv.Key, v []byte) error {\n\t\tif len(v) > 0 {\n\t\t\tmutations[string(k)] = &pb.Mutation{\n\t\t\t\tOp:    pb.Op_Put,\n\t\t\t\tKey:   k,\n\t\t\t\tValue: v,\n\t\t\t}\n\t\t} else {\n\t\t\tmutations[string(k)] = &pb.Mutation{\n\t\t\t\tOp:  pb.Op_Del,\n\t\t\t\tKey: k,\n\t\t\t}\n\t\t}\n\t\tkeys = append(keys, k)\n\t\tsize += len(k) + len(v)\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Trace(err)\n\t}\n\t// Transactions without Put/Del, only Locks are readonly.\n\t// We can skip commit directly.\n\tif len(keys) == 0 {\n\t\treturn nil, nil\n\t}\n\tfor _, lockKey := range txn.lockKeys {\n\t\tif _, ok := mutations[string(lockKey)]; !ok {\n\t\t\tmutations[string(lockKey)] = &pb.Mutation{\n\t\t\t\tOp:  pb.Op_Lock,\n\t\t\t\tKey: lockKey,\n\t\t\t}\n\t\t\tkeys = append(keys, lockKey)\n\t\t\tsize += len(lockKey)\n\t\t}\n\t}\n\ttxnWriteKVCountHistogram.Observe(float64(len(keys)))\n\ttxnWriteSizeHistogram.Observe(float64(size / 1024))\n\n\t// Increase lockTTL for large transactions.\n\t// The formula is `ttl = ttlFactor * sqrt(sizeInMiB)`.\n\t// When writeSize <= 256K, ttl is defaultTTL (3s);\n\t// When writeSize is 1MiB, 100MiB, or 400MiB, ttl is 6s, 60s, 120s correspondingly;\n\t// When writeSize >= 400MiB, ttl is maxTTL (120s).\n\tvar lockTTL uint64\n\tif size > txnCommitBatchSize {\n\t\tsizeMiB := float64(size) / 1024 / 1024\n\t\tlockTTL = uint64(float64(ttlFactor) * math.Sqrt(float64(sizeMiB)))\n\t\tif lockTTL < defaultLockTTL {\n\t\t\tlockTTL = defaultLockTTL\n\t\t}\n\t\tif lockTTL > maxLockTTL {\n\t\t\tlockTTL = maxLockTTL\n\t\t}\n\t}\n\n\treturn &twoPhaseCommitter{\n\t\tstore:     txn.store,\n\t\ttxn:       txn,\n\t\tstartTS:   txn.StartTS(),\n\t\tkeys:      keys,\n\t\tmutations: mutations,\n\t\tlockTTL:   lockTTL,\n\t}, nil\n}\n\nfunc (c *twoPhaseCommitter) primary() []byte {\n\treturn c.keys[0]\n}\n\n// doActionOnKeys groups keys into primary batch and secondary batches, if primary batch exists in the key,\n// it does action on primary batch first, then on secondary batches. If action is commit, secondary batches\n// is done in background goroutine.\nfunc (c *twoPhaseCommitter) doActionOnKeys(bo *Backoffer, action twoPhaseCommitAction, keys [][]byte) error {\n\tif len(keys) == 0 {\n\t\treturn nil\n\t}\n\tgroups, firstRegion, err := c.store.regionCache.GroupKeysByRegion(bo, keys)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\n\tvar batches []batchKeys\n\tvar sizeFunc = c.keySize\n\tif action == actionPrewrite {\n\t\tsizeFunc = c.keyValueSize\n\t}\n\t// Make sure the group that contains primary key goes first.\n\tbatches = appendBatchBySize(batches, firstRegion, groups[firstRegion], sizeFunc, txnCommitBatchSize)\n\tdelete(groups, firstRegion)\n\tfor id, g := range groups {\n\t\tbatches = appendBatchBySize(batches, id, g, sizeFunc, txnCommitBatchSize)\n\t}\n\n\tfirstIsPrimary := bytes.Equal(keys[0], c.primary())\n\tif firstIsPrimary {\n\t\terr = c.doActionOnBatches(bo, action, batches[:1])\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tbatches = batches[1:]\n\t}\n\tif action == actionCommit {\n\t\t// Commit secondary batches in background goroutine to reduce latency.\n\t\tgo func() {\n\t\t\te := c.doActionOnBatches(bo, action, batches)\n\t\t\tif e != nil {\n\t\t\t\tlog.Warnf(\"2PC async doActionOnBatches %s err: %v\", action, e)\n\t\t\t}\n\t\t}()\n\t} else {\n\t\terr = c.doActionOnBatches(bo, action, batches)\n\t}\n\treturn errors.Trace(err)\n}\n\n// doActionOnBatches does action to batches in parallel.\nfunc (c *twoPhaseCommitter) doActionOnBatches(bo *Backoffer, action twoPhaseCommitAction, batches []batchKeys) error {\n\tif len(batches) == 0 {\n\t\treturn nil\n\t}\n\tvar singleBatchActionFunc func(bo *Backoffer, batch batchKeys) error\n\tswitch action {\n\tcase actionPrewrite:\n\t\tsingleBatchActionFunc = c.prewriteSingleBatch\n\tcase actionCommit:\n\t\tsingleBatchActionFunc = c.commitSingleBatch\n\tcase actionCleanup:\n\t\tsingleBatchActionFunc = c.cleanupSingleBatch\n\t}\n\tif len(batches) == 1 {\n\t\te := singleBatchActionFunc(bo, batches[0])\n\t\tif e != nil {\n\t\t\tlog.Warnf(\"2PC doActionOnBatches %s failed: %v, tid: %d\", action, e, c.startTS)\n\t\t}\n\t\treturn errors.Trace(e)\n\t}\n\n\t// For prewrite, stop sending other requests after receiving first error.\n\tvar cancel context.CancelFunc\n\tif action == actionPrewrite {\n\t\tcancel = bo.WithCancel()\n\t}\n\n\t// Concurrently do the work for each batch.\n\tch := make(chan error, len(batches))\n\tfor _, batch := range batches {\n\t\tgo func(batch batchKeys) {\n\t\t\tch <- singleBatchActionFunc(bo.Fork(), batch)\n\t\t}(batch)\n\t}\n\tvar err error\n\tfor i := 0; i < len(batches); i++ {\n\t\tif e := <-ch; e != nil {\n\t\t\tlog.Warnf(\"2PC doActionOnBatches %s failed: %v, tid: %d\", action, e, c.startTS)\n\t\t\tif cancel != nil {\n\t\t\t\t// Cancel other requests and return the first error.\n\t\t\t\tcancel()\n\t\t\t\treturn errors.Trace(e)\n\t\t\t}\n\t\t\terr = e\n\t\t}\n\t}\n\treturn errors.Trace(err)\n}\n\nfunc (c *twoPhaseCommitter) keyValueSize(key []byte) int {\n\tsize := len(key)\n\tif mutation := c.mutations[string(key)]; mutation != nil {\n\t\tsize += len(mutation.Value)\n\t}\n\treturn size\n}\n\nfunc (c *twoPhaseCommitter) keySize(key []byte) int {\n\treturn len(key)\n}\n\nfunc (c *twoPhaseCommitter) prewriteSingleBatch(bo *Backoffer, batch batchKeys) error {\n\tmutations := make([]*pb.Mutation, len(batch.keys))\n\tfor i, k := range batch.keys {\n\t\tmutations[i] = c.mutations[string(k)]\n\t}\n\n\tskipCheck := false\n\toptSkipCheck := c.txn.us.GetOption(kv.SkipCheckForWrite)\n\tif optSkipCheck != nil {\n\t\tif skip, ok := optSkipCheck.(bool); ok && skip {\n\t\t\tskipCheck = true\n\t\t}\n\t}\n\treq := &pb.Request{\n\t\tType: pb.MessageType_CmdPrewrite,\n\t\tCmdPrewriteReq: &pb.CmdPrewriteRequest{\n\t\t\tMutations:           mutations,\n\t\t\tPrimaryLock:         c.primary(),\n\t\t\tStartVersion:        c.startTS,\n\t\t\tLockTtl:             c.lockTTL,\n\t\t\tSkipConstraintCheck: skipCheck,\n\t\t},\n\t}\n\n\tfor {\n\t\tresp, err := c.store.SendKVReq(bo, req, batch.region, readTimeoutShort)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif regionErr := resp.GetRegionError(); regionErr != nil {\n\t\t\terr = bo.Backoff(boRegionMiss, errors.New(regionErr.String()))\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t\terr = c.prewriteKeys(bo, batch.keys)\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tprewriteResp := resp.GetCmdPrewriteResp()\n\t\tif prewriteResp == nil {\n\t\t\treturn errors.Trace(errBodyMissing)\n\t\t}\n\t\tkeyErrs := prewriteResp.GetErrors()\n\t\tif len(keyErrs) == 0 {\n\t\t\t// We need to cleanup all written keys if transaction aborts.\n\t\t\tc.mu.Lock()\n\t\t\tdefer c.mu.Unlock()\n\t\t\tc.mu.writtenKeys = append(c.mu.writtenKeys, batch.keys...)\n\t\t\treturn nil\n\t\t}\n\t\tvar locks []*Lock\n\t\tfor _, keyErr := range keyErrs {\n\t\t\tlock, err1 := extractLockFromKeyErr(keyErr)\n\t\t\tif err1 != nil {\n\t\t\t\treturn errors.Trace(err1)\n\t\t\t}\n\t\t\tlog.Debugf(\"2PC prewrite encounters lock: %v\", lock)\n\t\t\tlocks = append(locks, lock)\n\t\t}\n\t\tok, err := c.store.lockResolver.ResolveLocks(bo, locks)\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tif !ok {\n\t\t\terr = bo.Backoff(boTxnLock, errors.Errorf(\"2PC prewrite lockedKeys: %d\", len(locks)))\n\t\t\tif err != nil {\n\t\t\t\treturn errors.Trace(err)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (c *twoPhaseCommitter) commitSingleBatch(bo *Backoffer, batch batchKeys) error {\n\treq := &pb.Request{\n\t\tType: pb.MessageType_CmdCommit,\n\t\tCmdCommitReq: &pb.CmdCommitRequest{\n\t\t\tStartVersion:  c.startTS,\n\t\t\tKeys:          batch.keys,\n\t\t\tCommitVersion: c.commitTS,\n\t\t},\n\t}\n\n\t// If we fail to receive response for the request that commits primary key, it will be undetermined whether this\n\t// transaction has been successfully committed.\n\t// Under this circumstance,  we can not declare the commit is complete (may lead to data lost), nor can we throw\n\t// an error (may lead to the duplicated key error when upper level restarts the transaction). Currently the best\n\t// workaround seems to be an infinite retry util server recovers and returns a success or failure response.\n\tif bytes.Compare(batch.keys[0], c.primary()) == 0 {\n\t\tbo = NewBackoffer(commitPrimaryMaxBackoff, bo.ctx)\n\t}\n\n\tresp, err := c.store.SendKVReq(bo, req, batch.region, readTimeoutShort)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tif regionErr := resp.GetRegionError(); regionErr != nil {\n\t\terr = bo.Backoff(boRegionMiss, errors.New(regionErr.String()))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t// re-split keys and commit again.\n\t\terr = c.commitKeys(bo, batch.keys)\n\t\treturn errors.Trace(err)\n\t}\n\tcommitResp := resp.GetCmdCommitResp()\n\tif commitResp == nil {\n\t\treturn errors.Trace(errBodyMissing)\n\t}\n\tif keyErr := commitResp.GetError(); keyErr != nil {\n\t\tc.mu.RLock()\n\t\tdefer c.mu.RUnlock()\n\t\terr = errors.Errorf(\"2PC commit failed: %v\", keyErr.String())\n\t\tif c.mu.committed {\n\t\t\t// No secondary key could be rolled back after it's primary key is committed.\n\t\t\t// There must be a serious bug somewhere.\n\t\t\tlog.Errorf(\"2PC failed commit key after primary key committed: %v, tid: %d\", err, c.startTS)\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\t// The transaction maybe rolled back by concurrent transactions.\n\t\tlog.Warnf(\"2PC failed commit primary key: %v, retry later, tid: %d\", err, c.startTS)\n\t\treturn errors.Annotate(err, txnRetryableMark)\n\t}\n\n\tc.mu.Lock()\n\tdefer c.mu.Unlock()\n\t// Group that contains primary key is always the first.\n\t// We mark transaction's status committed when we receive the first success response.\n\tc.mu.committed = true\n\treturn nil\n}\n\nfunc (c *twoPhaseCommitter) cleanupSingleBatch(bo *Backoffer, batch batchKeys) error {\n\treq := &pb.Request{\n\t\tType: pb.MessageType_CmdBatchRollback,\n\t\tCmdBatchRollbackReq: &pb.CmdBatchRollbackRequest{\n\t\t\tKeys:         batch.keys,\n\t\t\tStartVersion: c.startTS,\n\t\t},\n\t}\n\tresp, err := c.store.SendKVReq(bo, req, batch.region, readTimeoutShort)\n\tif err != nil {\n\t\treturn errors.Trace(err)\n\t}\n\tif regionErr := resp.GetRegionError(); regionErr != nil {\n\t\terr = bo.Backoff(boRegionMiss, errors.New(regionErr.String()))\n\t\tif err != nil {\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\terr = c.cleanupKeys(bo, batch.keys)\n\t\treturn errors.Trace(err)\n\t}\n\tif keyErr := resp.GetCmdBatchRollbackResp().GetError(); keyErr != nil {\n\t\terr = errors.Errorf(\"2PC cleanup failed: %s\", keyErr)\n\t\tlog.Errorf(\"2PC failed cleanup key: %v, tid: %d\", err, c.startTS)\n\t\treturn errors.Trace(err)\n\t}\n\treturn nil\n}\n\nfunc (c *twoPhaseCommitter) prewriteKeys(bo *Backoffer, keys [][]byte) error {\n\treturn c.doActionOnKeys(bo, actionPrewrite, keys)\n}\n\nfunc (c *twoPhaseCommitter) commitKeys(bo *Backoffer, keys [][]byte) error {\n\treturn c.doActionOnKeys(bo, actionCommit, keys)\n}\n\nfunc (c *twoPhaseCommitter) cleanupKeys(bo *Backoffer, keys [][]byte) error {\n\treturn c.doActionOnKeys(bo, actionCleanup, keys)\n}\n\n// The max time a Txn may use (in ms) from its startTS to commitTS.\n// We use it to guarantee GC worker will not influence any active txn. The value\n// should be less than `gcRunInterval`.\nconst maxTxnTimeUse = 590000\n\n// execute executes the two-phase commit protocol.\nfunc (c *twoPhaseCommitter) execute() error {\n\tctx := context.Background()\n\tdefer func() {\n\t\t// Always clean up all written keys if the txn does not commit.\n\t\tc.mu.RLock()\n\t\twrittenKeys := c.mu.writtenKeys\n\t\tcommitted := c.mu.committed\n\t\tc.mu.RUnlock()\n\t\tif !committed {\n\t\t\tgo func() {\n\t\t\t\terr := c.cleanupKeys(NewBackoffer(cleanupMaxBackoff, ctx), writtenKeys)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Infof(\"2PC cleanup err: %v, tid: %d\", err, c.startTS)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Infof(\"2PC clean up done, tid: %d\", c.startTS)\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\t}()\n\n\tbinlogChan := c.prewriteBinlog()\n\terr := c.prewriteKeys(NewBackoffer(prewriteMaxBackoff, ctx), c.keys)\n\tif binlogChan != nil {\n\t\tbinlogErr := <-binlogChan\n\t\tif binlogErr != nil {\n\t\t\treturn errors.Trace(binlogErr)\n\t\t}\n\t}\n\tif err != nil {\n\t\tlog.Warnf(\"2PC failed on prewrite: %v, tid: %d\", err, c.startTS)\n\t\treturn errors.Trace(err)\n\t}\n\n\tcommitTS, err := c.store.getTimestampWithRetry(NewBackoffer(tsoMaxBackoff, ctx))\n\tif err != nil {\n\t\tlog.Warnf(\"2PC get commitTS failed: %v, tid: %d\", err, c.startTS)\n\t\treturn errors.Trace(err)\n\t}\n\tc.commitTS = commitTS\n\n\tif c.store.oracle.IsExpired(c.startTS, maxTxnTimeUse) {\n\t\terr = errors.Errorf(\"txn takes too much time, start: %d, commit: %d\", c.startTS, c.commitTS)\n\t\treturn errors.Annotate(err, txnRetryableMark)\n\t}\n\n\terr = c.commitKeys(NewBackoffer(commitMaxBackoff, ctx), c.keys)\n\tif err != nil {\n\t\tif !c.mu.committed {\n\t\t\tlog.Warnf(\"2PC failed on commit: %v, tid: %d\", err, c.startTS)\n\t\t\treturn errors.Trace(err)\n\t\t}\n\t\tlog.Warnf(\"2PC succeed with error: %v, tid: %d\", err, c.startTS)\n\t}\n\treturn nil\n}\n\nfunc (c *twoPhaseCommitter) prewriteBinlog() chan error {\n\tif !c.shouldWriteBinlog() {\n\t\treturn nil\n\t}\n\tch := make(chan error, 1)\n\tgo func() {\n\t\tbin := c.txn.us.GetOption(kv.BinlogData).(*binlog.Binlog)\n\t\tbin.StartTs = int64(c.startTS)\n\t\tif bin.Tp == binlog.BinlogType_Prewrite {\n\t\t\tbin.PrewriteKey = c.keys[0]\n\t\t}\n\t\terr := binloginfo.WriteBinlog(bin, c.store.clusterID)\n\t\tch <- errors.Trace(err)\n\t}()\n\treturn ch\n}\n\nfunc (c *twoPhaseCommitter) writeFinishBinlog(tp binlog.BinlogType, commitTS int64) {\n\tif !c.shouldWriteBinlog() {\n\t\treturn\n\t}\n\tbin := c.txn.us.GetOption(kv.BinlogData).(*binlog.Binlog)\n\tbin.Tp = tp\n\tbin.CommitTs = commitTS\n\tgo func() {\n\t\terr := binloginfo.WriteBinlog(bin, c.store.clusterID)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"failed to write binlog: %v\", err)\n\t\t}\n\t}()\n}\n\nfunc (c *twoPhaseCommitter) shouldWriteBinlog() bool {\n\tif binloginfo.PumpClient == nil {\n\t\treturn false\n\t}\n\t_, ok := c.txn.us.GetOption(kv.BinlogData).(*binlog.Binlog)\n\treturn ok\n}\n\n// TiKV recommends each RPC packet should be less than ~1MB. We keep each packet's\n// Key+Value size below 4KB.\nconst txnCommitBatchSize = 4 * 1024\n\n// batchKeys is a batch of keys in the same region.\ntype batchKeys struct {\n\tregion RegionVerID\n\tkeys   [][]byte\n}\n\n// appendBatchBySize appends keys to []batchKeys. It may split the keys to make\n// sure each batch's size does not exceed the limit.\nfunc appendBatchBySize(b []batchKeys, region RegionVerID, keys [][]byte, sizeFn func([]byte) int, limit int) []batchKeys {\n\tvar start, end int\n\tfor start = 0; start < len(keys); start = end {\n\t\tvar size int\n\t\tfor end = start; end < len(keys) && size < limit; end++ {\n\t\t\tsize += sizeFn(keys[end])\n\t\t}\n\t\tb = append(b, batchKeys{\n\t\t\tregion: region,\n\t\t\tkeys:   keys[start:end],\n\t\t})\n\t}\n\treturn b\n}\n",
  "hunk": "@@ -253,10 +253,8 @@ func (c *twoPhaseCommitter) prewriteSingleBatch(bo *Backoffer, batch batchKeys)\n \n \tskipCheck := false\n \toptSkipCheck := c.txn.us.GetOption(kv.SkipCheckForWrite)\n-\tif optSkipCheck != nil {\n-\t\tif skip, ok := optSkipCheck.(bool); ok && skip {\n-\t\t\tskipCheck = true\n-\t\t}\n+\tif skip, ok := optSkipCheck.(bool); ok && skip {\n+\t\tskipCheck = true\n \t}\n \treq := &pb.Request{\n \t\tType: pb.MessageType_CmdPrewrite,\n",
  "comment": "We don't need to check not nil before try to assert to bool.",
  "ids": [
    177560,
    "027f2c8b97ec3b3684069c85e602eb76e2559696",
    "bdf35dc64d5fd2c9136900d2cdb7f78779ac05c8"
  ],
  "repo": "pingcap/tidb",
  "ghid": 2288,
  "old": " \tskipCheck := false\n \toptSkipCheck := c.txn.us.GetOption(kv.SkipCheckForWrite)\n-\tif optSkipCheck != nil {\n-\t\tif skip, ok := optSkipCheck.(bool); ok && skip {\n-\t\t\tskipCheck = true\n-\t\t}\n \t}\n \treq := &pb.Request{\n \t\tType: pb.MessageType_CmdPrewrite,",
  "new": " \tskipCheck := false\n \toptSkipCheck := c.txn.us.GetOption(kv.SkipCheckForWrite)\n+\tif skip, ok := optSkipCheck.(bool); ok && skip {\n+\t\tskipCheck = true\n \t}\n \treq := &pb.Request{\n \t\tType: pb.MessageType_CmdPrewrite,",
  "lang": "go"
}

--- Sample 5 ---
{
  "old_hunk": "@@ -1659,6 +1659,80 @@ def process_asg(self, client, asg):\n             raise\n \n \n+@ASG.action_registry.register('update')\n+class Update(Action):\n+    \"\"\"Action to update ASG configuration settings\n+\n+    :example:\n+\n+    .. code-block:: yaml\n+\n+            policies:\n+              - name: set-asg-instance-lifetime\n+                resource: asg\n+                filters:\n+                  - MaxInstanceLifetime: empty\n+                actions:\n+                  - type: update\n+                    max-instance-lifetime: 604800  # (7 days)\n+\n+              - name: set-asg-by-policy\n+                resource: asg\n+                actions:\n+                  - type: update\n+                    default-cooldown: 600\n+                    max-instance-lifetime: 0      # (clear it)\n+                    new-instances-protected-from-scale-in: true\n+                    capacity-rebalance: true\n+    \"\"\"\n+\n+    schema = type_schema(\n+        'update',\n+        **{\n+            'default-cooldown': {'type': 'integer', 'minimum': 0},\n+            'max-instance-lifetime': {\n+                \"anyOf\": [\n+                    {'enum': [0]},\n+                    {'type': 'integer', 'minimum': 86400}\n+                ]\n+            },\n+            'new-instances-protected-from-scale-in': {'type': 'boolean'},\n+            'capacity-rebalance': {'type': 'boolean'},\n+        }\n+    )\n+    permissions = (\"autoscaling:UpdateAutoScalingGroup\",)\n+    settings_map = {\n+        \"default-cooldown\": \"DefaultCooldown\",\n+        \"max-instance-lifetime\": \"MaxInstanceLifetime\",\n+        \"new-instances-protected-from-scale-in\": \"NewInstancesProtectedFromScaleIn\",\n+        \"capacity-rebalance\": \"CapacityRebalance\"\n+    }\n+\n+    def process(self, asgs):\n+        client = local_session(self.manager.session_factory).client('autoscaling')\n+\n+        settings = {}\n+        for k, v in self.settings_map.items():\n+            if k in self.data:\n+                settings[v] = self.data.get(k)\n+\n+        with self.executor_factory(max_workers=2) as w:\n+            futures = {}\n+            for a in asgs:\n+                futures[w.submit(self.process_asg, client, a, settings)] = a\n+            for f in as_completed(futures):\n+                if f.exception():",
  "oldf": "# Copyright The Cloud Custodian Authors.\n# SPDX-License-Identifier: Apache-2.0\nfrom botocore.client import ClientError\n\nfrom collections import Counter\nfrom concurrent.futures import as_completed\n\nfrom dateutil.parser import parse\n\nimport itertools\nimport time\n\nfrom c7n.actions import Action, AutoTagUser\nfrom c7n.exceptions import PolicyValidationError\nfrom c7n.filters import ValueFilter, AgeFilter, Filter\nfrom c7n.filters.offhours import OffHour, OnHour\nimport c7n.filters.vpc as net_filters\n\nfrom c7n.manager import resources\nfrom c7n import query\nfrom c7n.resources.securityhub import PostFinding\nfrom c7n.tags import TagActionFilter, DEFAULT_TAG, TagCountFilter, TagTrim, TagDelayedAction\nfrom c7n.utils import (\n    local_session, type_schema, chunks, get_retry, select_keys)\n\nfrom .ec2 import deserialize_user_data\n\n\n@resources.register('asg')\nclass ASG(query.QueryResourceManager):\n\n    class resource_type(query.TypeInfo):\n        service = 'autoscaling'\n        arn = 'AutoScalingGroupARN'\n        arn_type = 'autoScalingGroup'\n        arn_separator = \":\"\n        id = name = 'AutoScalingGroupName'\n        date = 'CreatedTime'\n        dimension = 'AutoScalingGroupName'\n        enum_spec = ('describe_auto_scaling_groups', 'AutoScalingGroups', None)\n        filter_name = 'AutoScalingGroupNames'\n        filter_type = 'list'\n        config_type = 'AWS::AutoScaling::AutoScalingGroup'\n        cfn_type = 'AWS::AutoScaling::AutoScalingGroup'\n\n        default_report_fields = (\n            'AutoScalingGroupName',\n            'CreatedTime',\n            'LaunchConfigurationName',\n            'count:Instances',\n            'DesiredCapacity',\n            'HealthCheckType',\n            'list:LoadBalancerNames',\n        )\n\n    retry = staticmethod(get_retry(('ResourceInUse', 'Throttling',)))\n\n\nASG.filter_registry.register('offhour', OffHour)\nASG.filter_registry.register('onhour', OnHour)\nASG.filter_registry.register('tag-count', TagCountFilter)\nASG.filter_registry.register('marked-for-op', TagActionFilter)\nASG.filter_registry.register('network-location', net_filters.NetworkLocation)\n\n\nclass LaunchInfo:\n\n    permissions = (\"ec2:DescribeLaunchTemplateVersions\",\n                   \"autoscaling:DescribeLaunchConfigurations\",)\n\n    def __init__(self, manager):\n        self.manager = manager\n\n    def initialize(self, asgs):\n        self.templates = self.get_launch_templates(asgs)\n        self.configs = self.get_launch_configs(asgs)\n        return self\n\n    def get_launch_templates(self, asgs):\n        tmpl_mgr = self.manager.get_resource_manager('launch-template-version')\n        # template ids include version identifiers\n        template_ids = list(tmpl_mgr.get_asg_templates(asgs))\n        if not template_ids:\n            return {}\n        return {\n            (t['LaunchTemplateId'],\n             str(t.get('c7n:VersionAlias', t['VersionNumber']))): t['LaunchTemplateData']\n            for t in tmpl_mgr.get_resources(template_ids)}\n\n    def get_launch_configs(self, asgs):\n        \"\"\"Return a mapping of launch configs for the given set of asgs\"\"\"\n        config_names = set()\n        for a in asgs:\n            if 'LaunchConfigurationName' not in a:\n                continue\n            config_names.add(a['LaunchConfigurationName'])\n        if not config_names:\n            return {}\n        lc_resources = self.manager.get_resource_manager('launch-config')\n        if len(config_names) < 5:\n            configs = lc_resources.get_resources(list(config_names))\n        else:\n            configs = lc_resources.resources()\n        return {\n            cfg['LaunchConfigurationName']: cfg for cfg in configs\n            if cfg['LaunchConfigurationName'] in config_names}\n\n    def get_launch_id(self, asg):\n        lid = asg.get('LaunchConfigurationName')\n        if lid is not None:\n            # We've noticed trailing white space allowed in some asgs\n            return lid.strip()\n\n        lid = asg.get('LaunchTemplate')\n        if lid is not None:\n            return (lid['LaunchTemplateId'], lid.get('Version', '$Default'))\n\n        if 'MixedInstancesPolicy' in asg:\n            mip_spec = asg['MixedInstancesPolicy'][\n                'LaunchTemplate']['LaunchTemplateSpecification']\n            return (mip_spec['LaunchTemplateId'], mip_spec.get('Version', '$Default'))\n\n        # we've noticed some corner cases where the asg name is the lc name, but not\n        # explicitly specified as launchconfiguration attribute.\n        lid = asg['AutoScalingGroupName']\n        return lid\n\n    def get(self, asg):\n        lid = self.get_launch_id(asg)\n        if isinstance(lid, tuple):\n            return self.templates.get(lid)\n        else:\n            return self.configs.get(lid)\n\n    def items(self):\n        return itertools.chain(*(\n            self.configs.items(), self.templates.items()))\n\n    def get_image_ids(self):\n        image_ids = {}\n        for cid, c in self.items():\n            if c.get('ImageId'):\n                image_ids.setdefault(c['ImageId'], []).append(cid)\n        return image_ids\n\n    def get_image_map(self):\n        # The describe_images api historically would return errors\n        # on an unknown ami in the set of images ids passed in.\n        # It now just silently drops those items, which is actually\n        # ideally for our use case.\n        #\n        # We used to do some balancing of picking up our asgs using\n        # the resource manager abstraction to take advantage of\n        # resource caching, but then we needed to do separate api\n        # calls to intersect with third party amis. Given the new\n        # describe behavior, we'll just do the api call to fetch the\n        # amis, it doesn't seem to have any upper bound on number of\n        # ImageIds to pass (Tested with 1k+ ImageIds)\n        #\n        # Explicitly use a describe source. Can't use a config source\n        # since it won't have state for third party ami, we auto\n        # propagate source normally. Can't use a cache either as their\n        # not in the account.\n        return {i['ImageId']: i for i in\n                self.manager.get_resource_manager(\n                    'ami').get_source('describe').get_resources(\n                        list(self.get_image_ids()), cache=False)}\n\n    def get_security_group_ids(self):\n        # return set of security group ids for given asg\n        sg_ids = set()\n        for k, v in self.items():\n            sg_ids.update(v.get('SecurityGroupIds', ()))\n            sg_ids.update(v.get('SecurityGroups', ()))\n        return sg_ids\n\n\n@ASG.filter_registry.register('security-group')\nclass SecurityGroupFilter(net_filters.SecurityGroupFilter):\n\n    RelatedIdsExpression = \"\"\n\n    permissions = ('ec2:DescribeSecurityGroups',) + LaunchInfo.permissions\n\n    def get_related_ids(self, asgs):\n        return self.launch_info.get_security_group_ids()\n\n    def process(self, asgs, event=None):\n        self.launch_info = LaunchInfo(self.manager).initialize(asgs)\n        return super(SecurityGroupFilter, self).process(asgs, event)\n\n\n@ASG.filter_registry.register('subnet')\nclass SubnetFilter(net_filters.SubnetFilter):\n\n    RelatedIdsExpression = \"\"\n\n    def get_related_ids(self, asgs):\n        subnet_ids = set()\n        for asg in asgs:\n            subnet_ids.update(\n                [sid.strip() for sid in asg.get('VPCZoneIdentifier', '').split(',')])\n        return subnet_ids\n\n\n@ASG.filter_registry.register('launch-config')\nclass LaunchConfigFilter(ValueFilter):\n    \"\"\"Filter asg by launch config attributes.\n\n    This will also filter to launch template data in addition\n    to launch configurations.\n\n    :example:\n\n    .. code-block:: yaml\n\n        policies:\n          - name: launch-configs-with-public-address\n            resource: asg\n            filters:\n              - type: launch-config\n                key: AssociatePublicIpAddress\n                value: true\n    \"\"\"\n    schema = type_schema(\n        'launch-config', rinherit=ValueFilter.schema)\n    schema_alias = False\n    permissions = (\"autoscaling:DescribeLaunchConfigurations\",)\n\n    def process(self, asgs, event=None):\n        self.launch_info = LaunchInfo(self.manager).initialize(asgs)\n        return super(LaunchConfigFilter, self).process(asgs, event)\n\n    def __call__(self, asg):\n        return self.match(self.launch_info.get(asg))\n\n\nclass ConfigValidFilter(Filter):\n\n    def get_permissions(self):\n        return list(itertools.chain(*[\n            self.manager.get_resource_manager(m).get_permissions()\n            for m in ('subnet', 'security-group', 'key-pair', 'elb',\n                      'app-elb-target-group', 'ebs-snapshot', 'ami')]))\n\n    def validate(self):\n        if self.manager.data.get('mode'):\n            raise PolicyValidationError(\n                \"invalid-config makes too many queries to be run in lambda\")\n        return self\n\n    def initialize(self, asgs):\n        self.launch_info = LaunchInfo(self.manager).initialize(asgs)\n        # pylint: disable=attribute-defined-outside-init\n        self.subnets = self.get_subnets()\n        self.security_groups = self.get_security_groups()\n        self.key_pairs = self.get_key_pairs()\n        self.elbs = self.get_elbs()\n        self.appelb_target_groups = self.get_appelb_target_groups()\n        self.snapshots = self.get_snapshots()\n        self.images, self.image_snaps = self.get_images()\n\n    def get_subnets(self):\n        manager = self.manager.get_resource_manager('subnet')\n        return {s['SubnetId'] for s in manager.resources()}\n\n    def get_security_groups(self):\n        manager = self.manager.get_resource_manager('security-group')\n        return {s['GroupId'] for s in manager.resources()}\n\n    def get_key_pairs(self):\n        manager = self.manager.get_resource_manager('key-pair')\n        return {k['KeyName'] for k in manager.resources()}\n\n    def get_elbs(self):\n        manager = self.manager.get_resource_manager('elb')\n        return {e['LoadBalancerName'] for e in manager.resources()}\n\n    def get_appelb_target_groups(self):\n        manager = self.manager.get_resource_manager('app-elb-target-group')\n        return {a['TargetGroupArn'] for a in manager.resources()}\n\n    def get_images(self):\n        images = self.launch_info.get_image_map()\n        image_snaps = set()\n\n        for a in images.values():\n            # Capture any snapshots, images strongly reference their\n            # snapshots, and some of these will be third party in the\n            # case of a third party image.\n            for bd in a.get('BlockDeviceMappings', ()):\n                if 'Ebs' not in bd or 'SnapshotId' not in bd['Ebs']:\n                    continue\n                image_snaps.add(bd['Ebs']['SnapshotId'].strip())\n        return set(images), image_snaps\n\n    def get_snapshots(self):\n        snaps = set()\n        for cid, cfg in self.launch_info.items():\n            for bd in cfg.get('BlockDeviceMappings', ()):\n                if 'Ebs' not in bd or 'SnapshotId' not in bd['Ebs']:\n                    continue\n                snaps.add(bd['Ebs']['SnapshotId'].strip())\n        manager = self.manager.get_resource_manager('ebs-snapshot')\n        return {s['SnapshotId'] for s in manager.get_resources(\n                list(snaps), cache=False)}\n\n    def process(self, asgs, event=None):\n        self.initialize(asgs)\n        return super(ConfigValidFilter, self).process(asgs, event)\n\n    def get_asg_errors(self, asg):\n        errors = []\n        subnets = asg.get('VPCZoneIdentifier', '').split(',')\n\n        for subnet in subnets:\n            subnet = subnet.strip()\n            if subnet not in self.subnets:\n                errors.append(('invalid-subnet', subnet))\n\n        for elb in asg['LoadBalancerNames']:\n            elb = elb.strip()\n            if elb not in self.elbs:\n                errors.append(('invalid-elb', elb))\n\n        for appelb_target in asg.get('TargetGroupARNs', []):\n            appelb_target = appelb_target.strip()\n            if appelb_target not in self.appelb_target_groups:\n                errors.append(('invalid-appelb-target-group', appelb_target))\n\n        cfg_id = self.launch_info.get_launch_id(asg)\n        cfg = self.launch_info.get(asg)\n\n        if cfg is None:\n            errors.append(('invalid-config', cfg_id))\n            self.log.debug(\n                \"asg:%s no launch config or template found\" % asg['AutoScalingGroupName'])\n            asg['Invalid'] = errors\n            return True\n\n        for sg in itertools.chain(*(\n                cfg.get('SecurityGroups', ()), cfg.get('SecurityGroupIds', ()))):\n            sg = sg.strip()\n            if sg not in self.security_groups:\n                errors.append(('invalid-security-group', sg))\n\n        if cfg.get('KeyName') and cfg['KeyName'].strip() not in self.key_pairs:\n            errors.append(('invalid-key-pair', cfg['KeyName']))\n\n        if cfg.get('ImageId') and cfg['ImageId'].strip() not in self.images:\n            errors.append(('invalid-image', cfg['ImageId']))\n\n        for bd in cfg.get('BlockDeviceMappings', ()):\n            if 'Ebs' not in bd or 'SnapshotId' not in bd['Ebs']:\n                continue\n            snapshot_id = bd['Ebs']['SnapshotId'].strip()\n            if snapshot_id in self.image_snaps:\n                continue\n            if snapshot_id not in self.snapshots:\n                errors.append(('invalid-snapshot', bd['Ebs']['SnapshotId']))\n        return errors\n\n\n@ASG.filter_registry.register('valid')\nclass ValidConfigFilter(ConfigValidFilter):\n    \"\"\"Filters autoscale groups to find those that are structurally valid.\n\n    This operates as the inverse of the invalid filter for multi-step\n    workflows.\n\n    See details on the invalid filter for a list of checks made.\n\n    :example:\n\n      .. code-block:: yaml\n\n          policies:\n            - name: asg-valid-config\n              resource: asg\n              filters:\n               - valid\n    \"\"\"\n\n    schema = type_schema('valid')\n\n    def __call__(self, asg):\n        errors = self.get_asg_errors(asg)\n        return not bool(errors)\n\n\n@ASG.filter_registry.register('invalid')\nclass InvalidConfigFilter(ConfigValidFilter):\n    \"\"\"Filter autoscale groups to find those that are structurally invalid.\n\n    Structurally invalid means that the auto scale group will not be able\n    to launch an instance succesfully as the configuration has\n\n    - invalid subnets\n    - invalid security groups\n    - invalid key pair name\n    - invalid launch config volume snapshots\n    - invalid amis\n    - invalid health check elb (slower)\n\n    Internally this tries to reuse other resource managers for better\n    cache utilization.\n\n    :example:\n\n        .. code-block:: yaml\n\n            policies:\n              - name: asg-invalid-config\n                resource: asg\n                filters:\n                  - invalid\n    \"\"\"\n    schema = type_schema('invalid')\n\n    def __call__(self, asg):\n        errors = self.get_asg_errors(asg)\n        if errors:\n            asg['Invalid'] = errors\n            return True\n\n\n@ASG.filter_registry.register('not-encrypted')\nclass NotEncryptedFilter(Filter):\n    \"\"\"Check if an ASG is configured to have unencrypted volumes.\n\n    Checks both the ami snapshots and the launch configuration.\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-unencrypted\n                resource: asg\n                filters:\n                  - type: not-encrypted\n                    exclude_image: true\n    \"\"\"\n\n    schema = type_schema('not-encrypted', exclude_image={'type': 'boolean'})\n    permissions = (\n        'ec2:DescribeImages',\n        'ec2:DescribeSnapshots',\n        'autoscaling:DescribeLaunchConfigurations')\n\n    images = unencrypted_configs = unencrypted_images = None\n\n    # TODO: resource-manager, notfound err mgr\n\n    def process(self, asgs, event=None):\n        self.launch_info = LaunchInfo(self.manager).initialize(asgs)\n        self.images = self.launch_info.get_image_map()\n\n        if not self.data.get('exclude_image'):\n            self.unencrypted_images = self.get_unencrypted_images()\n\n        self.unencrypted_launch = self.get_unencrypted_configs()\n        return super(NotEncryptedFilter, self).process(asgs, event)\n\n    def __call__(self, asg):\n        launch = self.launch_info.get(asg)\n        if not launch:\n            self.log.warning(\n                \"ASG %s instances: %d has missing config or template\",\n                asg['AutoScalingGroupName'], len(asg['Instances']))\n            return False\n\n        launch_id = self.launch_info.get_launch_id(asg)\n        unencrypted = []\n        if not self.data.get('exclude_image'):\n            if launch['ImageId'] in self.unencrypted_images:\n                unencrypted.append('Image')\n\n        if launch_id in self.unencrypted_launch:\n            unencrypted.append('LaunchConfig')\n        if unencrypted:\n            asg['Unencrypted'] = unencrypted\n        return bool(unencrypted)\n\n    def get_unencrypted_images(self):\n        \"\"\"retrieve images which have unencrypted snapshots referenced.\"\"\"\n        unencrypted_images = set()\n        for i in self.images.values():\n            for bd in i['BlockDeviceMappings']:\n                if 'Ebs' in bd and not bd['Ebs'].get('Encrypted'):\n                    unencrypted_images.add(i['ImageId'])\n                    break\n        return unencrypted_images\n\n    def get_unencrypted_configs(self):\n        \"\"\"retrieve configs that have unencrypted ebs voluems referenced.\"\"\"\n        unencrypted_configs = set()\n        snaps = {}\n\n        for cid, c in self.launch_info.items():\n            image = self.images.get(c.get('ImageId', ''))\n            # image deregistered/unavailable or exclude_image set\n            if image is not None:\n                image_block_devs = {\n                    bd['DeviceName'] for bd in\n                    image['BlockDeviceMappings'] if 'Ebs' in bd}\n            else:\n                image_block_devs = set()\n            for bd in c.get('BlockDeviceMappings', ()):\n                if 'Ebs' not in bd:\n                    continue\n                # Launch configs can shadow image devices, images have\n                # precedence.\n                if bd['DeviceName'] in image_block_devs:\n                    continue\n                if 'SnapshotId' in bd['Ebs']:\n                    snaps.setdefault(\n                        bd['Ebs']['SnapshotId'].strip(), []).append(cid)\n                elif not bd['Ebs'].get('Encrypted'):\n                    unencrypted_configs.add(cid)\n        if not snaps:\n            return unencrypted_configs\n\n        for s in self.get_snapshots(list(snaps.keys())):\n            if not s.get('Encrypted'):\n                unencrypted_configs.update(snaps[s['SnapshotId']])\n        return unencrypted_configs\n\n    def get_snapshots(self, snap_ids):\n        \"\"\"get snapshots corresponding to id, but tolerant of invalid id's.\"\"\"\n        return self.manager.get_resource_manager('ebs-snapshot').get_resources(\n            snap_ids, cache=False)\n\n\n@ASG.filter_registry.register('image-age')\nclass ImageAgeFilter(AgeFilter):\n    \"\"\"Filter asg by image age (in days).\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-older-image\n                resource: asg\n                filters:\n                  - type: image-age\n                    days: 90\n                    op: ge\n    \"\"\"\n    permissions = (\n        \"ec2:DescribeImages\",\n        \"autoscaling:DescribeLaunchConfigurations\")\n\n    date_attribute = \"CreationDate\"\n    schema = type_schema(\n        'image-age',\n        op={'$ref': '#/definitions/filters_common/comparison_operators'},\n        days={'type': 'number'})\n\n    def process(self, asgs, event=None):\n        self.launch_info = LaunchInfo(self.manager).initialize(asgs)\n        self.images = self.launch_info.get_image_map()\n        return super(ImageAgeFilter, self).process(asgs, event)\n\n    def get_resource_date(self, asg):\n        cfg = self.launch_info.get(asg)\n        if cfg is None:\n            cfg = {}\n        ami = self.images.get(cfg.get('ImageId'), {})\n        return parse(ami.get(\n            self.date_attribute, \"2000-01-01T01:01:01.000Z\"))\n\n\n@ASG.filter_registry.register('image')\nclass ImageFilter(ValueFilter):\n    \"\"\"Filter asg by image\n\n    :example:\n\n    .. code-block:: yaml\n\n        policies:\n          - name: non-windows-asg\n            resource: asg\n            filters:\n              - type: image\n                key: Platform\n                value: Windows\n                op: ne\n    \"\"\"\n    permissions = (\n        \"ec2:DescribeImages\",\n        \"autoscaling:DescribeLaunchConfigurations\")\n\n    schema = type_schema('image', rinherit=ValueFilter.schema)\n    schema_alias = True\n\n    def process(self, asgs, event=None):\n        self.launch_info = LaunchInfo(self.manager).initialize(asgs)\n        self.images = self.launch_info.get_image_map()\n        return super(ImageFilter, self).process(asgs, event)\n\n    def __call__(self, i):\n        image = self.images.get(self.launch_info.get(i).get('ImageId', None))\n        # Finally, if we have no image...\n        if not image:\n            self.log.warning(\n                \"Could not locate image for instance:%s ami:%s\" % (\n                    i['InstanceId'], i[\"ImageId\"]))\n            # Match instead on empty skeleton?\n            return False\n        return self.match(image)\n\n\n@ASG.filter_registry.register('vpc-id')\nclass VpcIdFilter(ValueFilter):\n    \"\"\"Filters ASG based on the VpcId\n\n    This filter is available as a ValueFilter as the vpc-id is not natively\n    associated to the results from describing the autoscaling groups.\n\n    :example:\n\n    .. code-block:: yaml\n\n        policies:\n          - name: asg-vpc-xyz\n            resource: asg\n            filters:\n              - type: vpc-id\n                value: vpc-12ab34cd\n    \"\"\"\n\n    schema = type_schema(\n        'vpc-id', rinherit=ValueFilter.schema)\n    schema['properties'].pop('key')\n    schema_alias = False\n    permissions = ('ec2:DescribeSubnets',)\n\n    # TODO: annotation\n\n    def __init__(self, data, manager=None):\n        super(VpcIdFilter, self).__init__(data, manager)\n        self.data['key'] = 'VpcId'\n\n    def process(self, asgs, event=None):\n        subnets = {}\n        for a in asgs:\n            subnet_ids = a.get('VPCZoneIdentifier', '')\n            if not subnet_ids:\n                continue\n            subnets.setdefault(subnet_ids.split(',')[0], []).append(a)\n\n        subnet_manager = self.manager.get_resource_manager('subnet')\n        # Invalid subnets on asgs happen, so query all\n        all_subnets = {s['SubnetId']: s for s in subnet_manager.resources()}\n\n        for s, s_asgs in subnets.items():\n            if s not in all_subnets:\n                self.log.warning(\n                    \"invalid subnet %s for asgs: %s\",\n                    s, [a['AutoScalingGroupName'] for a in s_asgs])\n                continue\n            for a in s_asgs:\n                a['VpcId'] = all_subnets[s]['VpcId']\n        return super(VpcIdFilter, self).process(asgs)\n\n\n@ASG.filter_registry.register('progagated-tags')  # compatibility\n@ASG.filter_registry.register('propagated-tags')\nclass PropagatedTagFilter(Filter):\n    \"\"\"Filter ASG based on propagated tags\n\n    This filter is designed to find all autoscaling groups that have a list\n    of tag keys (provided) that are set to propagate to new instances. Using\n    this will allow for easy validation of asg tag sets are in place across an\n    account for compliance.\n\n    :example:\n\n       .. code-block:: yaml\n\n            policies:\n              - name: asg-non-propagated-tags\n                resource: asg\n                filters:\n                  - type: propagated-tags\n                    keys: [\"ABC\", \"BCD\"]\n                    match: false\n                    propagate: true\n    \"\"\"\n    schema = type_schema(\n        'progagated-tags',\n        aliases=('propagated-tags',),\n        keys={'type': 'array', 'items': {'type': 'string'}},\n        match={'type': 'boolean'},\n        propagate={'type': 'boolean'})\n    permissions = (\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"autoscaling:DescribeAutoScalingGroups\")\n\n    def process(self, asgs, event=None):\n        keys = self.data.get('keys', [])\n        match = self.data.get('match', True)\n        results = []\n        for asg in asgs:\n            if self.data.get('propagate', True):\n                tags = [t['Key'] for t in asg.get('Tags', []) if t[\n                    'Key'] in keys and t['PropagateAtLaunch']]\n                if match and all(k in tags for k in keys):\n                    results.append(asg)\n                if not match and not all(k in tags for k in keys):\n                    results.append(asg)\n            else:\n                tags = [t['Key'] for t in asg.get('Tags', []) if t[\n                    'Key'] in keys and not t['PropagateAtLaunch']]\n                if match and all(k in tags for k in keys):\n                    results.append(asg)\n                if not match and not all(k in tags for k in keys):\n                    results.append(asg)\n        return results\n\n\n@ASG.action_registry.register('post-finding')\nclass AsgPostFinding(PostFinding):\n\n    resource_type = 'AwsAutoScalingAutoScalingGroup'\n    launch_info = LaunchInfo(None)\n\n    def format_resource(self, r):\n        envelope, payload = self.format_envelope(r)\n        details = select_keys(r, [\n            'CreatedTime', 'HealthCheckType', 'HealthCheckGracePeriod', 'LoadBalancerNames'])\n        lid = self.launch_info.get_launch_id(r)\n        if isinstance(lid, tuple):\n            lid = \"%s:%s\" % lid\n        details['CreatedTime'] = details['CreatedTime'].isoformat()\n        # let's arbitrarily cut off key information per security hub's restrictions...\n        details['LaunchConfigurationName'] = lid[:32]\n        payload.update(details)\n        return envelope\n\n\n@ASG.action_registry.register('auto-tag-user')\nclass AutoScaleAutoTagUser(AutoTagUser):\n\n    schema = type_schema(\n        'auto-tag-user',\n        propagate={'type': 'boolean'},\n        rinherit=AutoTagUser.schema)\n    schema_alias = False\n\n    def set_resource_tags(self, tags, resources):\n        tag_action = self.manager.action_registry.get('tag')\n        tag_action(\n            {'tags': tags, 'propagate': self.data.get('propagate', False)},\n            self.manager).process(resources)\n\n\n@ASG.action_registry.register('tag-trim')\nclass GroupTagTrim(TagTrim):\n    \"\"\"Action to trim the number of tags to avoid hitting tag limits\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-tag-trim\n                resource: asg\n                filters:\n                  - type: tag-count\n                    count: 10\n                actions:\n                  - type: tag-trim\n                    space: 1\n                    preserve:\n                      - OwnerName\n                      - OwnerContact\n    \"\"\"\n\n    max_tag_count = 10\n    permissions = ('autoscaling:DeleteTags',)\n\n    def process_tag_removal(self, client, resource, candidates):\n        tags = []\n        for t in candidates:\n            tags.append(\n                dict(Key=t, ResourceType='auto-scaling-group',\n                     ResourceId=resource['AutoScalingGroupName']))\n        client.delete_tags(Tags=tags)\n\n\n@ASG.filter_registry.register('capacity-delta')\nclass CapacityDelta(Filter):\n    \"\"\"Filter returns ASG that have less instances than desired or required\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-capacity-delta\n                resource: asg\n                filters:\n                  - capacity-delta\n    \"\"\"\n\n    schema = type_schema('capacity-delta')\n\n    def process(self, asgs, event=None):\n        return [\n            a for a in asgs if len(\n                a['Instances']) < a['DesiredCapacity'] or len(\n                    a['Instances']) < a['MinSize']]\n\n\n@ASG.filter_registry.register('user-data')\nclass UserDataFilter(ValueFilter):\n    \"\"\"Filter on ASG's whose launch configs have matching userdata.\n    Note: It is highly recommended to use regexes with the ?sm flags, since Custodian\n    uses re.match() and userdata spans multiple lines.\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: lc_userdata\n                resource: asg\n                filters:\n                  - type: user-data\n                    op: regex\n                    value: (?smi).*password=\n                actions:\n                  - delete\n    \"\"\"\n\n    schema = type_schema('user-data', rinherit=ValueFilter.schema)\n    schema_alias = False\n    batch_size = 50\n    annotation = 'c7n:user-data'\n\n    def __init__(self, data, manager):\n        super(UserDataFilter, self).__init__(data, manager)\n        self.data['key'] = '\"c7n:user-data\"'\n\n    def get_permissions(self):\n        return self.manager.get_resource_manager('asg').get_permissions()\n\n    def process(self, asgs, event=None):\n        '''Get list of autoscaling groups whose launch configs match the\n        user-data filter.\n\n        :return: List of ASG's with matching launch configs\n        '''\n        self.data['key'] = '\"c7n:user-data\"'\n        launch_info = LaunchInfo(self.manager).initialize(asgs)\n\n        results = []\n        for asg in asgs:\n            launch_config = launch_info.get(asg)\n            if self.annotation not in launch_config:\n                if not launch_config.get('UserData'):\n                    asg[self.annotation] = None\n                else:\n                    asg[self.annotation] = deserialize_user_data(\n                        launch_config['UserData'])\n            if self.match(asg):\n                results.append(asg)\n        return results\n\n\n@ASG.action_registry.register('resize')\nclass Resize(Action):\n    \"\"\"Action to resize the min/max/desired instances in an ASG\n\n    There are several ways to use this action:\n\n    1. set min/desired to current running instances\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-resize\n                resource: asg\n                filters:\n                  - capacity-delta\n                actions:\n                  - type: resize\n                    desired-size: \"current\"\n\n    2. apply a fixed resize of min, max or desired, optionally saving the\n       previous values to a named tag (for restoring later):\n\n    .. code-block:: yaml\n\n            policies:\n              - name: offhours-asg-off\n                resource: asg\n                filters:\n                  - type: offhour\n                    offhour: 19\n                    default_tz: bst\n                actions:\n                  - type: resize\n                    min-size: 0\n                    desired-size: 0\n                    save-options-tag: OffHoursPrevious\n\n    3. restore previous values for min/max/desired from a tag:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: offhours-asg-on\n                resource: asg\n                filters:\n                  - type: onhour\n                    onhour: 8\n                    default_tz: bst\n                actions:\n                  - type: resize\n                    restore-options-tag: OffHoursPrevious\n\n    \"\"\"\n\n    schema = type_schema(\n        'resize',\n        **{\n            'min-size': {'type': 'integer', 'minimum': 0},\n            'max-size': {'type': 'integer', 'minimum': 0},\n            'desired-size': {\n                \"anyOf\": [\n                    {'enum': [\"current\"]},\n                    {'type': 'integer', 'minimum': 0}\n                ]\n            },\n            # support previous key name with underscore\n            'desired_size': {\n                \"anyOf\": [\n                    {'enum': [\"current\"]},\n                    {'type': 'integer', 'minimum': 0}\n                ]\n            },\n            'save-options-tag': {'type': 'string'},\n            'restore-options-tag': {'type': 'string'},\n        }\n    )\n    permissions = (\n        'autoscaling:UpdateAutoScalingGroup',\n        'autoscaling:CreateOrUpdateTags'\n    )\n\n    def process(self, asgs):\n        # ASG parameters to save to/restore from a tag\n        asg_params = ['MinSize', 'MaxSize', 'DesiredCapacity']\n\n        # support previous param desired_size when desired-size is not present\n        if 'desired_size' in self.data and 'desired-size' not in self.data:\n            self.data['desired-size'] = self.data['desired_size']\n\n        client = local_session(self.manager.session_factory).client(\n            'autoscaling')\n        for a in asgs:\n            tag_map = {t['Key']: t['Value'] for t in a.get('Tags', [])}\n            update = {}\n            current_size = len(a['Instances'])\n\n            if 'restore-options-tag' in self.data:\n                # we want to restore all ASG size params from saved data\n                self.log.debug(\n                    'Want to restore ASG %s size from tag %s' %\n                    (a['AutoScalingGroupName'], self.data['restore-options-tag']))\n                if self.data['restore-options-tag'] in tag_map:\n                    for field in tag_map[self.data['restore-options-tag']].split(';'):\n                        (param, value) = field.split('=')\n                        if param in asg_params:\n                            update[param] = int(value)\n\n            else:\n                # we want to resize, parse provided params\n                if 'min-size' in self.data:\n                    update['MinSize'] = self.data['min-size']\n\n                if 'max-size' in self.data:\n                    update['MaxSize'] = self.data['max-size']\n\n                if 'desired-size' in self.data:\n                    if self.data['desired-size'] == 'current':\n                        update['DesiredCapacity'] = min(current_size, a['DesiredCapacity'])\n                        if 'MinSize' not in update:\n                            # unless we were given a new value for min_size then\n                            # ensure it is at least as low as current_size\n                            update['MinSize'] = min(current_size, a['MinSize'])\n                    elif type(self.data['desired-size']) == int:\n                        update['DesiredCapacity'] = self.data['desired-size']\n\n            if update:\n                self.log.debug('ASG %s size: current=%d, min=%d, max=%d, desired=%d'\n                    % (a['AutoScalingGroupName'], current_size, a['MinSize'],\n                    a['MaxSize'], a['DesiredCapacity']))\n\n                if 'save-options-tag' in self.data:\n                    # save existing ASG params to a tag before changing them\n                    self.log.debug('Saving ASG %s size to tag %s' %\n                        (a['AutoScalingGroupName'], self.data['save-options-tag']))\n                    tags = [dict(\n                        Key=self.data['save-options-tag'],\n                        PropagateAtLaunch=False,\n                        Value=';'.join({'%s=%d' % (param, a[param]) for param in asg_params}),\n                        ResourceId=a['AutoScalingGroupName'],\n                        ResourceType='auto-scaling-group',\n                    )]\n                    self.manager.retry(client.create_or_update_tags, Tags=tags)\n\n                self.log.debug('Resizing ASG %s with %s' % (a['AutoScalingGroupName'],\n                    str(update)))\n                self.manager.retry(\n                    client.update_auto_scaling_group,\n                    AutoScalingGroupName=a['AutoScalingGroupName'],\n                    **update)\n            else:\n                self.log.debug('nothing to resize')\n\n\n@ASG.action_registry.register('remove-tag')\n@ASG.action_registry.register('untag')  # compatibility\n@ASG.action_registry.register('unmark')  # compatibility\nclass RemoveTag(Action):\n    \"\"\"Action to remove tag/tags from an ASG\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-remove-unnecessary-tags\n                resource: asg\n                filters:\n                  - \"tag:UnnecessaryTag\": present\n                actions:\n                  - type: remove-tag\n                    key: UnnecessaryTag\n    \"\"\"\n\n    schema = type_schema(\n        'remove-tag',\n        aliases=('untag', 'unmark'),\n        tags={'type': 'array', 'items': {'type': 'string'}},\n        key={'type': 'string'})\n\n    permissions = ('autoscaling:DeleteTags',)\n    batch_size = 1\n\n    def process(self, asgs):\n        error = False\n        tags = self.data.get('tags', [])\n        if not tags:\n            tags = [self.data.get('key', DEFAULT_TAG)]\n        client = local_session(self.manager.session_factory).client('autoscaling')\n\n        with self.executor_factory(max_workers=2) as w:\n            futures = {}\n            for asg_set in chunks(asgs, self.batch_size):\n                futures[w.submit(\n                    self.process_resource_set, client, asg_set, tags)] = asg_set\n            for f in as_completed(futures):\n                asg_set = futures[f]\n                if f.exception():\n                    error = f.exception()\n                    self.log.exception(\n                        \"Exception untagging asg:%s tag:%s error:%s\" % (\n                            \", \".join([a['AutoScalingGroupName']\n                                       for a in asg_set]),\n                            self.data.get('key', DEFAULT_TAG),\n                            f.exception()))\n        if error:\n            raise error\n\n    def process_resource_set(self, client, asgs, tags):\n        tag_set = []\n        for a in asgs:\n            for t in tags:\n                tag_set.append(dict(\n                    Key=t, ResourceType='auto-scaling-group',\n                    ResourceId=a['AutoScalingGroupName']))\n        self.manager.retry(client.delete_tags, Tags=tag_set)\n\n\n@ASG.action_registry.register('tag')\n@ASG.action_registry.register('mark')\nclass Tag(Action):\n    \"\"\"Action to add a tag to an ASG\n\n    The *propagate* parameter can be used to specify that the tag being added\n    will need to be propagated down to each ASG instance associated or simply\n    to the ASG itself.\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-add-owner-tag\n                resource: asg\n                filters:\n                  - \"tag:OwnerName\": absent\n                actions:\n                  - type: tag\n                    key: OwnerName\n                    value: OwnerName\n                    propagate: true\n    \"\"\"\n\n    schema = type_schema(\n        'tag',\n        key={'type': 'string'},\n        value={'type': 'string'},\n        tags={'type': 'object'},\n        # Backwards compatibility\n        tag={'type': 'string'},\n        msg={'type': 'string'},\n        propagate={'type': 'boolean'},\n        aliases=('mark',)\n    )\n    permissions = ('autoscaling:CreateOrUpdateTags',)\n    batch_size = 1\n\n    def get_tag_set(self):\n        tags = []\n        key = self.data.get('key', self.data.get('tag', DEFAULT_TAG))\n        value = self.data.get(\n            'value', self.data.get(\n                'msg', 'AutoScaleGroup does not meet policy guidelines'))\n        if key and value:\n            tags.append({'Key': key, 'Value': value})\n\n        for k, v in self.data.get('tags', {}).items():\n            tags.append({'Key': k, 'Value': v})\n\n        return tags\n\n    def process(self, asgs):\n        tags = self.get_tag_set()\n        error = None\n\n        client = self.get_client()\n        with self.executor_factory(max_workers=2) as w:\n            futures = {}\n            for asg_set in chunks(asgs, self.batch_size):\n                futures[w.submit(\n                    self.process_resource_set, client, asg_set, tags)] = asg_set\n            for f in as_completed(futures):\n                asg_set = futures[f]\n                if f.exception():\n                    self.log.exception(\n                        \"Exception tagging tag:%s error:%s asg:%s\" % (\n                            tags,\n                            f.exception(),\n                            \", \".join([a['AutoScalingGroupName']\n                                       for a in asg_set])))\n        if error:\n            raise error\n\n    def process_resource_set(self, client, asgs, tags):\n        tag_params = []\n        propagate = self.data.get('propagate', False)\n        for t in tags:\n            if 'PropagateAtLaunch' not in t:\n                t['PropagateAtLaunch'] = propagate\n        for t in tags:\n            for a in asgs:\n                atags = dict(t)\n                atags['ResourceType'] = 'auto-scaling-group'\n                atags['ResourceId'] = a['AutoScalingGroupName']\n                tag_params.append(atags)\n                a.setdefault('Tags', []).append(atags)\n        self.manager.retry(client.create_or_update_tags, Tags=tag_params)\n\n    def get_client(self):\n        return local_session(self.manager.session_factory).client('autoscaling')\n\n\n@ASG.action_registry.register('propagate-tags')\nclass PropagateTags(Action):\n    \"\"\"Propagate tags to an asg instances.\n\n    In AWS changing an asg tag does not automatically propagate to\n    extant instances even if the tag is set to propagate. It only\n    is applied to new instances.\n\n    This action exists to ensure that extant instances also have these\n    propagated tags set, and can also trim older tags not present on\n    the asg anymore that are present on instances.\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-propagate-required\n                resource: asg\n                filters:\n                  - \"tag:OwnerName\": present\n                actions:\n                  - type: propagate-tags\n                    tags:\n                      - OwnerName\n\n    \"\"\"\n\n    schema = type_schema(\n        'propagate-tags',\n        tags={'type': 'array', 'items': {'type': 'string'}},\n        trim={'type': 'boolean'})\n    permissions = ('ec2:DeleteTags', 'ec2:CreateTags')\n\n    def validate(self):\n        if not isinstance(self.data.get('tags', []), (list, tuple)):\n            raise ValueError(\"No tags specified\")\n        return self\n\n    def process(self, asgs):\n        if not asgs:\n            return\n        if self.data.get('trim', False):\n            self.instance_map = self.get_instance_map(asgs)\n        with self.executor_factory(max_workers=3) as w:\n            instance_count = sum(list(w.map(self.process_asg, asgs)))\n            self.log.info(\"Applied tags to %d instances\" % instance_count)\n\n    def process_asg(self, asg):\n        instance_ids = [i['InstanceId'] for i in asg['Instances']]\n        tag_map = {t['Key']: t['Value'] for t in asg.get('Tags', [])\n                   if t['PropagateAtLaunch'] and not t['Key'].startswith('aws:')}\n\n        if self.data.get('tags'):\n            tag_map = {\n                k: v for k, v in tag_map.items()\n                if k in self.data['tags']}\n\n        if not tag_map and not self.get('trim', False):\n            self.log.error(\n                'No tags found to propagate on asg:{} tags configured:{}'.format(\n                    asg['AutoScalingGroupName'], self.data.get('tags')))\n\n        tag_set = set(tag_map)\n        client = local_session(self.manager.session_factory).client('ec2')\n\n        if self.data.get('trim', False):\n            instances = [self.instance_map[i] for i in instance_ids]\n            self.prune_instance_tags(client, asg, tag_set, instances)\n\n        if not self.manager.config.dryrun and instance_ids and tag_map:\n            client.create_tags(\n                Resources=instance_ids,\n                Tags=[{'Key': k, 'Value': v} for k, v in tag_map.items()])\n        return len(instance_ids)\n\n    def prune_instance_tags(self, client, asg, tag_set, instances):\n        \"\"\"Remove tags present on all asg instances which are not present\n        on the asg.\n        \"\"\"\n        instance_tags = Counter()\n        instance_count = len(instances)\n\n        remove_tags = []\n        extra_tags = []\n\n        for i in instances:\n            instance_tags.update([\n                t['Key'] for t in i['Tags']\n                if not t['Key'].startswith('aws:')])\n        for k, v in instance_tags.items():\n            if not v >= instance_count:\n                extra_tags.append(k)\n                continue\n            if k not in tag_set:\n                remove_tags.append(k)\n\n        if remove_tags:\n            self.log.debug(\"Pruning asg:%s instances:%d of old tags: %s\" % (\n                asg['AutoScalingGroupName'], instance_count, remove_tags))\n        if extra_tags:\n            self.log.debug(\"Asg: %s has uneven tags population: %s\" % (\n                asg['AutoScalingGroupName'], instance_tags))\n        # Remove orphan tags\n        remove_tags.extend(extra_tags)\n\n        if not self.manager.config.dryrun:\n            client.delete_tags(\n                Resources=[i['InstanceId'] for i in instances],\n                Tags=[{'Key': t} for t in remove_tags])\n\n    def get_instance_map(self, asgs):\n        instance_ids = [\n            i['InstanceId'] for i in\n            list(itertools.chain(*[\n                g['Instances']\n                for g in asgs if g['Instances']]))]\n        if not instance_ids:\n            return {}\n        return {i['InstanceId']: i for i in\n                self.manager.get_resource_manager(\n                    'ec2').get_resources(instance_ids)}\n\n\n@ASG.action_registry.register('rename-tag')\nclass RenameTag(Action):\n    \"\"\"Rename a tag on an AutoScaleGroup.\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-rename-owner-tag\n                resource: asg\n                filters:\n                  - \"tag:OwnerNames\": present\n                actions:\n                  - type: rename-tag\n                    propagate: true\n                    source: OwnerNames\n                    dest: OwnerName\n    \"\"\"\n\n    schema = type_schema(\n        'rename-tag', required=['source', 'dest'],\n        propagate={'type': 'boolean'},\n        source={'type': 'string'},\n        dest={'type': 'string'})\n\n    def get_permissions(self):\n        permissions = (\n            'autoscaling:CreateOrUpdateTags',\n            'autoscaling:DeleteTags')\n        if self.data.get('propagate', True):\n            permissions += ('ec2:CreateTags', 'ec2:DeleteTags')\n        return permissions\n\n    def process(self, asgs):\n        source = self.data.get('source')\n        dest = self.data.get('dest')\n        count = len(asgs)\n\n        filtered = []\n        for a in asgs:\n            for t in a.get('Tags'):\n                if t['Key'] == source:\n                    filtered.append(a)\n                    break\n        asgs = filtered\n        self.log.info(\"Filtered from %d asgs to %d\", count, len(asgs))\n        self.log.info(\n            \"Renaming %s to %s on %d asgs\", source, dest, len(filtered))\n        with self.executor_factory(max_workers=3) as w:\n            list(w.map(self.process_asg, asgs))\n\n    def process_asg(self, asg):\n        \"\"\"Move source tag to destination tag.\n\n        Check tag count on asg\n        Create new tag tag\n        Delete old tag\n        Check tag count on instance\n        Create new tag\n        Delete old tag\n        \"\"\"\n        source_tag = self.data.get('source')\n        tag_map = {t['Key']: t for t in asg.get('Tags', [])}\n        source = tag_map[source_tag]\n        destination_tag = self.data.get('dest')\n        propagate = self.data.get('propagate', True)\n        client = local_session(\n            self.manager.session_factory).client('autoscaling')\n        # technically safer to create first, but running into\n        # max tags constraints, otherwise.\n        #\n        # delete_first = len([t for t in tag_map if not t.startswith('aws:')])\n        client.delete_tags(Tags=[\n            {'ResourceId': asg['AutoScalingGroupName'],\n             'ResourceType': 'auto-scaling-group',\n             'Key': source_tag,\n             'Value': source['Value']}])\n        client.create_or_update_tags(Tags=[\n            {'ResourceId': asg['AutoScalingGroupName'],\n             'ResourceType': 'auto-scaling-group',\n             'PropagateAtLaunch': propagate,\n             'Key': destination_tag,\n             'Value': source['Value']}])\n        if propagate:\n            self.propagate_instance_tag(source, destination_tag, asg)\n\n    def propagate_instance_tag(self, source, destination_tag, asg):\n        client = local_session(self.manager.session_factory).client('ec2')\n        client.delete_tags(\n            Resources=[i['InstanceId'] for i in asg['Instances']],\n            Tags=[{\"Key\": source['Key']}])\n        client.create_tags(\n            Resources=[i['InstanceId'] for i in asg['Instances']],\n            Tags=[{'Key': destination_tag, 'Value': source['Value']}])\n\n\n@ASG.action_registry.register('mark-for-op')\nclass MarkForOp(TagDelayedAction):\n    \"\"\"Action to create a delayed action for a later date\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-suspend-schedule\n                resource: asg\n                filters:\n                  - type: value\n                    key: MinSize\n                    value: 2\n                actions:\n                  - type: mark-for-op\n                    tag: custodian_suspend\n                    message: \"Suspending: {op}@{action_date}\"\n                    op: suspend\n                    days: 7\n    \"\"\"\n\n    schema = type_schema(\n        'mark-for-op',\n        op={'type': 'string'},\n        key={'type': 'string'},\n        tag={'type': 'string'},\n        tz={'type': 'string'},\n        msg={'type': 'string'},\n        message={'type': 'string'},\n        days={'type': 'number', 'minimum': 0},\n        hours={'type': 'number', 'minimum': 0})\n    schema_alias = False\n    default_template = (\n        'AutoScaleGroup does not meet org policy: {op}@{action_date}')\n\n    def get_config_values(self):\n        d = {\n            'op': self.data.get('op', 'stop'),\n            'tag': self.data.get('key', self.data.get('tag', DEFAULT_TAG)),\n            'msg': self.data.get('message', self.data.get('msg', self.default_template)),\n            'tz': self.data.get('tz', 'utc'),\n            'days': self.data.get('days', 0),\n            'hours': self.data.get('hours', 0)}\n        d['action_date'] = self.generate_timestamp(\n            d['days'], d['hours'])\n        return d\n\n\n@ASG.action_registry.register('suspend')\nclass Suspend(Action):\n    \"\"\"Action to suspend ASG processes and instances\n\n    AWS ASG suspend/resume and process docs\n     https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-suspend-processes\n                resource: asg\n                filters:\n                  - \"tag:SuspendTag\": present\n                actions:\n                  - type: suspend\n    \"\"\"\n    permissions = (\"autoscaling:SuspendProcesses\", \"ec2:StopInstances\")\n\n    ASG_PROCESSES = [\n        \"Launch\",\n        \"Terminate\",\n        \"HealthCheck\",\n        \"ReplaceUnhealthy\",\n        \"AZRebalance\",\n        \"AlarmNotification\",\n        \"ScheduledActions\",\n        \"AddToLoadBalancer\"]\n\n    schema = type_schema(\n        'suspend',\n        exclude={\n            'type': 'array',\n            'title': 'ASG Processes to not suspend',\n            'items': {'enum': ASG_PROCESSES}})\n\n    ASG_PROCESSES = set(ASG_PROCESSES)\n\n    def process(self, asgs):\n        with self.executor_factory(max_workers=3) as w:\n            list(w.map(self.process_asg, asgs))\n\n    def process_asg(self, asg):\n        \"\"\"Multistep process to stop an asg aprori of setup\n\n        - suspend processes\n        - stop instances\n        \"\"\"\n        session = local_session(self.manager.session_factory)\n        asg_client = session.client('autoscaling')\n        processes = list(self.ASG_PROCESSES.difference(\n            self.data.get('exclude', ())))\n\n        try:\n            self.manager.retry(\n                asg_client.suspend_processes,\n                ScalingProcesses=processes,\n                AutoScalingGroupName=asg['AutoScalingGroupName'])\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'ValidationError':\n                return\n            raise\n        ec2_client = session.client('ec2')\n        try:\n            instance_ids = [i['InstanceId'] for i in asg['Instances']]\n            if not instance_ids:\n                return\n            retry = get_retry((\n                'RequestLimitExceeded', 'Client.RequestLimitExceeded'))\n            retry(ec2_client.stop_instances, InstanceIds=instance_ids)\n        except ClientError as e:\n            if e.response['Error']['Code'] in (\n                    'InvalidInstanceID.NotFound',\n                    'IncorrectInstanceState'):\n                self.log.warning(\"Erroring stopping asg instances %s %s\" % (\n                    asg['AutoScalingGroupName'], e))\n                return\n            raise\n\n\n@ASG.action_registry.register('resume')\nclass Resume(Action):\n    \"\"\"Resume a suspended autoscale group and its instances\n\n    Parameter 'delay' is the amount of time (in seconds) to wait\n    between resuming instances in the asg, and restarting the internal\n    asg processed which gives some grace period before health checks\n    turn on within the ASG (default value: 30)\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-resume-processes\n                resource: asg\n                filters:\n                  - \"tag:Resume\": present\n                actions:\n                  - type: resume\n                    delay: 300\n\n    \"\"\"\n    schema = type_schema('resume', delay={'type': 'number'})\n    permissions = (\"autoscaling:ResumeProcesses\", \"ec2:StartInstances\")\n\n    def process(self, asgs):\n        original_count = len(asgs)\n        asgs = [a for a in asgs if a['SuspendedProcesses']]\n        self.delay = self.data.get('delay', 30)\n        self.log.debug(\"Filtered from %d to %d suspended asgs\",\n                       original_count, len(asgs))\n\n        session = local_session(self.manager.session_factory)\n        ec2_client = session.client('ec2')\n        asg_client = session.client('autoscaling')\n\n        with self.executor_factory(max_workers=3) as w:\n            futures = {}\n            for a in asgs:\n                futures[w.submit(self.resume_asg_instances, ec2_client, a)] = a\n            for f in as_completed(futures):\n                if f.exception():\n                    self.log.error(\"Traceback resume asg:%s instances error:%s\" % (\n                        futures[f]['AutoScalingGroupName'],\n                        f.exception()))\n                    continue\n\n        self.log.debug(\"Sleeping for asg health check grace\")\n        time.sleep(self.delay)\n\n        with self.executor_factory(max_workers=3) as w:\n            futures = {}\n            for a in asgs:\n                futures[w.submit(self.resume_asg, asg_client, a)] = a\n            for f in as_completed(futures):\n                if f.exception():\n                    self.log.error(\"Traceback resume asg:%s error:%s\" % (\n                        futures[f]['AutoScalingGroupName'],\n                        f.exception()))\n\n    def resume_asg_instances(self, ec2_client, asg):\n        \"\"\"Resume asg instances.\n        \"\"\"\n        instance_ids = [i['InstanceId'] for i in asg['Instances']]\n        if not instance_ids:\n            return\n        retry = get_retry((\n            'RequestLimitExceeded', 'Client.RequestLimitExceeded'))\n        retry(ec2_client.start_instances, InstanceIds=instance_ids)\n\n    def resume_asg(self, asg_client, asg):\n        \"\"\"Resume asg processes.\n        \"\"\"\n        self.manager.retry(\n            asg_client.resume_processes,\n            AutoScalingGroupName=asg['AutoScalingGroupName'])\n\n\n@ASG.action_registry.register('delete')\nclass Delete(Action):\n    \"\"\"Action to delete an ASG\n\n    The 'force' parameter is needed when deleting an ASG that has instances\n    attached to it.\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-delete-bad-encryption\n                resource: asg\n                filters:\n                  - type: not-encrypted\n                    exclude_image: true\n                actions:\n                  - type: delete\n                    force: true\n    \"\"\"\n\n    schema = type_schema('delete', force={'type': 'boolean'})\n    permissions = (\"autoscaling:DeleteAutoScalingGroup\",)\n\n    def process(self, asgs):\n        client = local_session(\n            self.manager.session_factory).client('autoscaling')\n        for asg in asgs:\n            self.process_asg(client, asg)\n\n    def process_asg(self, client, asg):\n        force_delete = self.data.get('force', False)\n        try:\n            self.manager.retry(\n                client.delete_auto_scaling_group,\n                AutoScalingGroupName=asg['AutoScalingGroupName'],\n                ForceDelete=force_delete)\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'ValidationError':\n                return\n            raise\n\n\n@ASG.action_registry.register('update')\nclass Update(Action):\n    \"\"\"Action to update ASG configuration settings\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: set-asg-instance-lifetime\n                resource: asg\n                filters:\n                  - MaxInstanceLifetime: empty\n                actions:\n                  - type: update\n                    max-instance-lifetime: 604800  # (7 days)\n\n              - name: set-asg-by-policy\n                resource: asg\n                actions:\n                  - type: update\n                    default-cooldown: 600\n                    max-instance-lifetime: 0      # (clear it)\n                    new-instances-protected-from-scale-in: true\n                    capacity-rebalance: true\n    \"\"\"\n\n    schema = type_schema(\n        'update',\n        **{\n            'default-cooldown': {'type': 'integer', 'minimum': 0},\n            'max-instance-lifetime': {\n                \"anyOf\": [\n                    {'enum': [0]},\n                    {'type': 'integer', 'minimum': 86400}\n                ]\n            },\n            'new-instances-protected-from-scale-in': {'type': 'boolean'},\n            'capacity-rebalance': {'type': 'boolean'},\n        }\n    )\n    permissions = (\"autoscaling:UpdateAutoScalingGroup\",)\n    settings_map = {\n        \"default-cooldown\": \"DefaultCooldown\",\n        \"max-instance-lifetime\": \"MaxInstanceLifetime\",\n        \"new-instances-protected-from-scale-in\": \"NewInstancesProtectedFromScaleIn\",\n        \"capacity-rebalance\": \"CapacityRebalance\"\n    }\n\n    def process(self, asgs):\n        client = local_session(self.manager.session_factory).client('autoscaling')\n\n        settings = {}\n        for k, v in self.settings_map.items():\n            if k in self.data:\n                settings[v] = self.data.get(k)\n\n        with self.executor_factory(max_workers=2) as w:\n            futures = {}\n            for a in asgs:\n                futures[w.submit(self.process_asg, client, a, settings)] = a\n            for f in as_completed(futures):\n                if f.exception():\n                    self.log.error(\"Error while updating asg:%s error:%s\" % (\n                        futures[f]['AutoScalingGroupName'],\n                        f.exception()))\n\n    def process_asg(self, client, asg, settings):\n        self.manager.retry(\n            client.update_auto_scaling_group,\n            AutoScalingGroupName=asg['AutoScalingGroupName'],\n            **settings)\n\n\n@resources.register('launch-config')\nclass LaunchConfig(query.QueryResourceManager):\n\n    class resource_type(query.TypeInfo):\n        service = 'autoscaling'\n        arn_type = 'launchConfiguration'\n        id = name = 'LaunchConfigurationName'\n        date = 'CreatedTime'\n        enum_spec = (\n            'describe_launch_configurations', 'LaunchConfigurations', None)\n        filter_name = 'LaunchConfigurationNames'\n        filter_type = 'list'\n        cfn_type = config_type = 'AWS::AutoScaling::LaunchConfiguration'\n\n\n@LaunchConfig.filter_registry.register('age')\nclass LaunchConfigAge(AgeFilter):\n    \"\"\"Filter ASG launch configuration by age (in days)\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-launch-config-old\n                resource: launch-config\n                filters:\n                  - type: age\n                    days: 90\n                    op: ge\n    \"\"\"\n\n    date_attribute = \"CreatedTime\"\n    schema = type_schema(\n        'age',\n        op={'$ref': '#/definitions/filters_common/comparison_operators'},\n        days={'type': 'number'})\n\n\n@LaunchConfig.filter_registry.register('unused')\nclass UnusedLaunchConfig(Filter):\n    \"\"\"Filters all launch configurations that are not in use but exist\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-unused-launch-config\n                resource: launch-config\n                filters:\n                  - unused\n    \"\"\"\n\n    schema = type_schema('unused')\n\n    def get_permissions(self):\n        return self.manager.get_resource_manager('asg').get_permissions()\n\n    def process(self, configs, event=None):\n        asgs = self.manager.get_resource_manager('asg').resources()\n        used = {a.get('LaunchConfigurationName', a['AutoScalingGroupName'])\n                for a in asgs if not a.get('LaunchTemplate')}\n        return [c for c in configs if c['LaunchConfigurationName'] not in used]\n\n\n@LaunchConfig.action_registry.register('delete')\nclass LaunchConfigDelete(Action):\n    \"\"\"Filters all unused launch configurations\n\n    :example:\n\n    .. code-block:: yaml\n\n            policies:\n              - name: asg-unused-launch-config-delete\n                resource: launch-config\n                filters:\n                  - unused\n                actions:\n                  - delete\n    \"\"\"\n\n    schema = type_schema('delete')\n    permissions = (\"autoscaling:DeleteLaunchConfiguration\",)\n\n    def process(self, configs):\n        client = local_session(self.manager.session_factory).client('autoscaling')\n\n        for c in configs:\n            self.process_config(client, c)\n\n    def process_config(self, client, config):\n        try:\n            client.delete_launch_configuration(\n                LaunchConfigurationName=config[\n                    'LaunchConfigurationName'])\n        except ClientError as e:\n            # Catch already deleted\n            if e.response['Error']['Code'] == 'ValidationError':\n                return\n            raise\n\n\n@resources.register('scaling-policy')\nclass ScalingPolicy(query.QueryResourceManager):\n\n    class resource_type(query.TypeInfo):\n        service = 'autoscaling'\n        arn_type = \"scalingPolicy\"\n        id = name = 'PolicyName'\n        date = 'CreatedTime'\n        enum_spec = (\n            'describe_policies', 'ScalingPolicies', None\n        )\n        filter_name = 'PolicyNames'\n        filter_type = 'list'\n        cfn_type = 'AWS::AutoScaling::ScalingPolicy'\n\n\n@ASG.filter_registry.register('scaling-policy')\nclass ScalingPolicyFilter(ValueFilter):\n\n    \"\"\"Filter asg by scaling-policies attributes.\n\n    :example:\n\n    .. code-block:: yaml\n\n        policies:\n          - name: scaling-policies-with-target-tracking\n            resource: asg\n            filters:\n              - type: scaling-policy\n                key: PolicyType\n                value: \"TargetTrackingScaling\"\n\n    \"\"\"\n\n    schema = type_schema(\n        'scaling-policy', rinherit=ValueFilter.schema\n    )\n    schema_alias = False\n    permissions = (\"autoscaling:DescribePolicies\",)\n    annotate = False  # no default value annotation on policy\n    annotation_key = 'c7n:matched-policies'\n\n    def get_scaling_policies(self, asgs):\n        policies = self.manager.get_resource_manager('scaling-policy').resources()\n        policy_map = {}\n        for policy in policies:\n            policy_map.setdefault(\n                policy['AutoScalingGroupName'], []).append(policy)\n        return policy_map\n\n    def process(self, asgs, event=None):\n        self.policy_map = self.get_scaling_policies(asgs)\n        return super(ScalingPolicyFilter, self).process(asgs, event)\n\n    def __call__(self, asg):\n        asg_policies = self.policy_map.get(asg['AutoScalingGroupName'], ())\n        matched = []\n        for policy in asg_policies:\n            if self.match(policy):\n                matched.append(policy)\n        if matched:\n            asg[self.annotation_key] = matched\n        return bool(matched)\n",
  "hunk": "@@ -1718,6 +1718,7 @@ class Update(Action):\n \n         with self.executor_factory(max_workers=2) as w:\n             futures = {}\n+            error = None\n             for a in asgs:\n                 futures[w.submit(self.process_asg, client, a, settings)] = a\n             for f in as_completed(futures):\n",
  "comment": "per current error handling best practices, its fine to continue for other resources in this action, but we want to stop policy exec by raising at the end of processing. ```python # pseudo error = None if f.exception() error = f.exception() if error: raise error ```",
  "ids": [
    37578,
    "e09747ee4da1979f473bdcd7b59834eb9a92ca70",
    "d0cddf051ed05c06db8206bc08cdac4d792ba4a3"
  ],
  "repo": "cloud-custodian/cloud-custodian",
  "ghid": 6612,
  "old": "         with self.executor_factory(max_workers=2) as w:\n             futures = {}\n             for a in asgs:\n                 futures[w.submit(self.process_asg, client, a, settings)] = a\n             for f in as_completed(futures):",
  "new": "         with self.executor_factory(max_workers=2) as w:\n             futures = {}\n+            error = None\n             for a in asgs:\n                 futures[w.submit(self.process_asg, client, a, settings)] = a\n             for f in as_completed(futures):",
  "lang": "py"
}



Dataset: Comment_Generation
Path: Datasets/Comment_Generation/msg-train.jsonl
Total lines (records): 117739
Top keys and frequencies:
  oldf: 117739
  patch: 117739
  msg: 117739
  id: 117739
  y: 117739

Samples (first records):
--- Sample 1 ---
{
  "oldf": "//    |  /           |\n//    ' /   __| _` | __|  _ \\   __|\n//    . \\  |   (   | |   (   |\\__ `\n//   _|\\_\\_|  \\__,_|\\__|\\___/ ____/\n//                   Multi-Physics\n//\n//  License:\t\t BSD License\n//\t\t\t\t\t Kratos default license: kratos/license.txt\n//\n//  Main authors:    Vicente Mataix Ferrandiz\n//\n\n// System includes\n#include <limits>\n\n// External includes\n\n\n// Project includes\n#include \"testing/testing.h\"\n\n// Utility includes\n#include \"utilities/math_utils.h\"\n\nnamespace Kratos \n{\n    namespace Testing \n    {\n        /// Tests\n       \n        /** Checks if the area of the triangle is calculated correctly using Heron equation.\n         * Checks if the area of the triangle is calculated correctly using Heron equation.\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsHeronTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n            \n            const double area = MathUtils<double>::Heron<false>(std::sqrt(2.0), 1.0, 1.0);\n\n            KRATOS_CHECK_NEAR(area, 0.5, tolerance);\n        }\n        \n        /** Checks if it gives you the absolute value of a given value\n         * Checks if It gives you the absolute value of a given value\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsAbsTest, KratosCoreMathUtilsFastSuite) \n        {\n            const double absolute = MathUtils<double>::Abs(-1.0);\n\n            KRATOS_CHECK_EQUAL(absolute, 1.0);\n        }\n        \n        /** Checks if it gives you the minimum value of a given value\n         * Checks if It gives you the minimum value of a given value\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsMinTest, KratosCoreMathUtilsFastSuite) \n        {\n            const double min = MathUtils<double>::Min(0.0,1.0);\n\n            KRATOS_CHECK_EQUAL(min, 0.0);\n        }\n        \n        /** Checks if it gives you the maximum value of a given value\n         * Checks if It gives you the maximum value of a given value\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsMaxTest, KratosCoreMathUtilsFastSuite) \n        {\n            const double max = MathUtils<double>::Max(0.0,1.0);\n\n            KRATOS_CHECK_EQUAL(max, 1.0);\n        }\n        \n        /** Checks if it calculates the determinant of a 1x1, 2x2, 3x3 and 4x4 matrix \n         * Checks if it calculates the determinant of a 1x1, 2x2, 3x3 and 4x4 matrix \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsDetMatTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n            \n            boost::numeric::ublas::bounded_matrix<double, 1, 1> mat11 = ZeroMatrix(1, 1);\n            mat11(0,0) = 1.0;\n            \n            double det = MathUtils<double>::DetMat<1>(mat11);\n\n            KRATOS_CHECK_NEAR(det, 1.0, tolerance);\n            \n            boost::numeric::ublas::bounded_matrix<double, 2, 2> mat22 = ZeroMatrix(2, 2);\n            mat22(0,0) = 1.0;\n            mat22(1,1) = 1.0;\n            \n            det = MathUtils<double>::DetMat<2>(mat22);\n\n            KRATOS_CHECK_NEAR(det, 1.0, tolerance);\n            \n            boost::numeric::ublas::bounded_matrix<double, 3, 3> mat33 = ZeroMatrix(3, 3);\n            mat33(0,0) = 1.0;\n            mat33(1,1) = 1.0;\n            mat33(2,2) = 1.0;\n            \n            det = MathUtils<double>::DetMat<3>(mat33);\n\n            KRATOS_CHECK_NEAR(det, 1.0, tolerance);\n            \n            boost::numeric::ublas::bounded_matrix<double, 4, 4> mat44 = ZeroMatrix(4, 4);\n            mat44(0,0) = 1.0;\n            mat44(1,1) = 1.0;\n            mat44(2,2) = 1.0;\n            mat44(3,3) = 1.0;\n            \n            det = MathUtils<double>::DetMat<4>(mat44);\n\n            KRATOS_CHECK_NEAR(det, 1.0, tolerance);\n        }\n        \n        /** Checks if it calculates the generalized determinant of a non-square matrix\n         * Checks if it calculates the generalized determinant of a non-square matrix\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsGenDetMatTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n            \n            Matrix mat23 = ZeroMatrix(2, 3);\n            mat23(0,0) = 1.0;\n            mat23(1,1) = 1.0;\n            \n            double det = MathUtils<double>::GeneralizedDet(mat23);\n            \n            KRATOS_CHECK_NEAR(det, 1.0, tolerance);\n            \n            Matrix mat55 = ZeroMatrix(5, 5);\n            mat55(0,0) =   1.0;\n            mat55(1,1) =   1.0;\n            mat55(2,2) =   1.0;\n            mat55(3,3) =   1.0;\n            mat55(2,3) = - 1.0;\n            mat55(3,2) =   1.0;\n            mat55(4,4) =   2.0;\n            \n            det = MathUtils<double>::Det(mat55);\n\n            KRATOS_CHECK_NEAR(det, 4.0, tolerance);\n        }\n        \n        /** Checks if it calculates the inverse of a 1x1, 2x2, 3x3 and 4x4 matrix \n         * Checks if it calculates the inverse of a 1x1, 2x2, 3x3 and 4x4 matrix \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsInvMatTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n\n            boost::numeric::ublas::bounded_matrix<double, 1, 1> mat11;\n            mat11(0,0) = 0.896308;\n            \n            double det;\n            const boost::numeric::ublas::bounded_matrix<double, 1, 1> inv11 = MathUtils<double>::InvertMatrix<1>(mat11, det);\n            const boost::numeric::ublas::bounded_matrix<double, 1, 1> I11 = prod(inv11, mat11);\n            \n            KRATOS_CHECK_NEAR(I11(0,0), 1.0, tolerance);\n            \n            boost::numeric::ublas::bounded_matrix<double, 2, 2> mat22;\n            mat22(0,0) = 0.670005;\n            mat22(0,1) = 0.853367;\n            mat22(1,0) = 1.47006;\n            mat22(1,1) = 1.00029;\n            \n            const boost::numeric::ublas::bounded_matrix<double, 2, 2> inv22 = MathUtils<double>::InvertMatrix<2>(mat22, det);\n            const boost::numeric::ublas::bounded_matrix<double, 2, 2> I22 = prod(inv22, mat22);\n            \n            for (unsigned int i = 0; i < 2; i++)\n            {\n                for (unsigned int j = 0; j < 2; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I22(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I22(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            boost::numeric::ublas::bounded_matrix<double, 3, 3> mat33;\n            mat33(0,0) = 0.678589;\n            mat33(0,1) = 0.386213;\n            mat33(0,2) = 0.371126;\n            mat33(1,0) = 1.01524;\n            mat33(1,1) = 0.403437;\n            mat33(1,2) = 1.03755;\n            mat33(2,0) = 0.450516;\n            mat33(2,1) = 1.08225;\n            mat33(2,2) = 0.972831;\n            \n            const boost::numeric::ublas::bounded_matrix<double, 3, 3> inv33 = MathUtils<double>::InvertMatrix<3>(mat33, det);\n            const boost::numeric::ublas::bounded_matrix<double, 3, 3> I33 = prod(inv33, mat33);\n            \n            for (unsigned int i = 0; i < 3; i++)\n            {\n                for (unsigned int j = 0; j < 3; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I33(i,j), 1.0, tolerance);\n                    }\n                    else\n                    {\n                        KRATOS_CHECK_NEAR(I33(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            boost::numeric::ublas::bounded_matrix<double, 4, 4> mat44;\n            mat44(0,0) = 0.00959158;\n            mat44(0,1) = 0.466699;\n            mat44(0,2) = 0.167357;\n            mat44(0,3) = 0.255465;\n            mat44(1,0) = 1.6356;\n            mat44(1,1) = 0.387988;\n            mat44(1,2) = 1.17823;\n            mat44(1,3) = 1.38661;\n            mat44(2,0) = 2.57105;\n            mat44(2,1) = 1.63057;\n            mat44(2,2) = 2.5713;\n            mat44(2,3) = 1.73297;\n            mat44(3,0) = 3.40005;\n            mat44(3,1) = 1.94218;\n            mat44(3,2) = 2.58081;\n            mat44(3,3) = 3.3083;\n            \n            const boost::numeric::ublas::bounded_matrix<double, 4, 4> inv44 = MathUtils<double>::InvertMatrix<4>(mat44, det);\n            const boost::numeric::ublas::bounded_matrix<double, 4, 4> I44 = prod(inv44, mat44);\n            \n            for (unsigned int i = 0; i < 4; i++)\n            {\n                for (unsigned int j = 0; j < 4; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I44(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I44(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n        }\n        \n        /** Checks if it calculates the inverse of a 1x1, 2x2, 3x3 and 4x4 matrix \n         * Checks if it calculates the inverse of a 1x1, 2x2, 3x3 and 4x4 matrix \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsInvertMatrixTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n            \n            double det;\n            Matrix inv;\n            Matrix I;\n            \n            unsigned int i_dim = 1;\n            \n            Matrix mat = ZeroMatrix(i_dim, i_dim);\n            \n            mat(0,0) = 0.346432;\n\n            MathUtils<double>::InvertMatrix(mat,inv, det);\n            \n            I = prod(inv, mat);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            i_dim = 2;\n            mat.resize(i_dim, i_dim, false);\n            \n            mat(0,0) = 0.833328;\n            mat(0,1) = 0.491166;\n            mat(1,0) = 0.81167;\n            mat(1,1) = 1.17205;\n\n            MathUtils<double>::InvertMatrix(mat,inv, det);\n            \n            I = prod(inv, mat);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            i_dim = 3;\n            mat.resize(i_dim, i_dim, false);\n            \n            mat(0,0) = 0.371083;\n            mat(0,1) = 0.392607;\n            mat(0,2) = 0.306494;\n            mat(1,0) = 0.591012;\n            mat(1,1) = 1.00733;\n            mat(1,2) = 1.07727;\n            mat(2,0) = 0.0976054;\n            mat(2,1) = 2.54893;\n            mat(2,2) = 1.23981;\n\n            MathUtils<double>::InvertMatrix(mat,inv, det);\n            \n            I = prod(inv, mat);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            i_dim = 4;\n            mat.resize(i_dim, i_dim, false);\n            \n            mat(0,1) = 0.979749;\n            mat(0,2) = 0.494393;\n            mat(0,3) = 0.23073;\n            mat(1,0) = 1.79224;\n            mat(1,1) = 0.198842;\n            mat(1,2) = 0.074485;\n            mat(1,3) = 1.45717;\n            mat(2,0) = 1.6039;\n            mat(2,1) = 0.673926;\n            mat(2,2) = 2.63817;\n            mat(2,3) = 1.0287;\n            mat(3,0) = 0.366503;\n            mat(3,1) = 3.02634;\n            mat(3,2) = 1.24104;\n            mat(3,3) = 3.62022;\n\n            MathUtils<double>::InvertMatrix(mat,inv, det);\n            \n            I = prod(inv, mat);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            i_dim = 5;\n            mat.resize(i_dim, i_dim, false);\n            \n            mat = ZeroMatrix(5, 5);\n            mat(0,0) =   1.0;\n            mat(1,1) =   1.0;\n            mat(2,2) =   1.0;\n            mat(3,3) =   1.0;\n            mat(2,3) = - 1.0;\n            mat(3,2) =   1.0;\n            mat(4,4) =   2.0;\n\n            MathUtils<double>::InvertMatrix(mat,inv, det);\n            \n            KRATOS_CHECK_NEAR(det, 4.0, tolerance);\n            \n            I = prod(inv, mat);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n        }\n        \n        /** Checks if it calculates correctly the inverse of a non square matrix\n         * Checks if it calculates correctly the inverse of a non square matrix\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsGeneralizedInvertMatrixTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n            \n            // We check the Left inverse\n\n            const unsigned int i_dim = 2;\n            const unsigned int j_dim = 3;\n            \n            Matrix mat = ZeroMatrix(i_dim, j_dim);\n            \n            mat(0,0) = 0.770724;\n            mat(1,0) = 0.573294;\n            mat(0,1) = 1.27699;\n            mat(1,1) = 1.57776;\n            mat(0,2) = 1.30216;\n            mat(1,2) = 2.66483;\n            \n            double det;\n            Matrix inv;\n            \n            MathUtils<double>::GeneralizedInvertMatrix(mat,inv, det);\n            \n            Matrix I = prod(mat, inv);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n            \n            // We check the Right inverse\n            mat.resize(j_dim, i_dim);\n            mat = ZeroMatrix(j_dim, i_dim);\n        \n            mat(0,0) = 0.786075;\n            mat(1,0) = 0.91272;\n            mat(2,0) = 0.745604;\n            mat(0,1) = 0.992728;\n            mat(1,1) = 1.82324;\n            mat(2,1) = 0.19581;\n            \n            MathUtils<double>::GeneralizedInvertMatrix(mat,inv, det);\n            \n            I = prod(inv, mat);\n            \n            for (unsigned int i = 0; i < i_dim; i++)\n            {\n                for (unsigned int j = 0; j < i_dim; j++)\n                {\n                    if (i == j) \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 1.0, tolerance);\n                    }\n                    else \n                    {\n                        KRATOS_CHECK_NEAR(I(i,j), 0.0, tolerance);\n                    }\n                }\n            }\n        }\n        \n        /** Checks if it calculates the sign function \n         * Checks if it calculates the sign function \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsSignTest, KratosCoreMathUtilsFastSuite) \n        {\n            int sign = MathUtils<double>::Sign(-1.0);\n            \n            KRATOS_CHECK_EQUAL(sign, -1);\n            \n            sign = MathUtils<double>::Sign(1.0);\n            \n            KRATOS_CHECK_EQUAL(sign, 1);\n        }\n        \n        /** Checks if it calculates the eigen decomposition of a 3x3 system\n         * Checks if it calculates the eigen decomposition of a 3x3 system\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsEigenTest, KratosCoreMathUtilsFastSuite) \n        {\n            constexpr double tolerance = 1e-6;\n            \n            boost::numeric::ublas::bounded_matrix<double, 3, 3> mat33;\n            boost::numeric::ublas::bounded_matrix<double, 3, 3> eigenmat33;\n            boost::numeric::ublas::bounded_matrix<double, 3, 3> vectormat33;\n            \n            mat33(0,0) = 0.678589;\n            mat33(0,1) = 0.386213;\n            mat33(0,2) = 0.371126;\n            mat33(1,0) = mat33(0,1);\n            mat33(1,1) = 0.403437;\n            mat33(1,2) = 1.03755;\n            mat33(2,0) = mat33(0,2);\n            mat33(2,1) = mat33(1,2);\n            mat33(2,2) = 0.972831;\n            \n            bool converged = MathUtils<double>::EigenSystem<3>(mat33, vectormat33, eigenmat33);\n\n            boost::numeric::ublas::bounded_matrix<double, 3, 3> auxmat33 = prod(trans(vectormat33), eigenmat33);\n            auxmat33 = prod(auxmat33, vectormat33);\n            \n            for (unsigned int i = 0; i < 3; i++)\n            {\n                for (unsigned int j = i; j < 3; j++)\n                {\n                    KRATOS_CHECK_NEAR(auxmat33(i,j), mat33(i,j), tolerance);\n                }\n            }\n            \n            KRATOS_CHECK_EQUAL(converged, true);\n        }\n        \n        /** Checks if it calculates the dot product \n         * Checks if it calculates the dot product \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsDotTest, KratosCoreMathUtilsFastSuite) \n        {\n            Vector a = ZeroVector(3);\n            a[1] = 1.0;\n            Vector b = ZeroVector(3);\n            b[0] = 1.0;\n\n            const double c = MathUtils<double>::Dot3(a, b);\n            const double d = MathUtils<double>::Dot(a, b);\n            \n            KRATOS_CHECK_EQUAL(c, 0.0);\n            KRATOS_CHECK_EQUAL(d, 0.0);\n        }\n        \n        /** Checks if it calculates the norm of a vector\n         * Checks if it calculates the norm of a vector\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsNormTest, KratosCoreMathUtilsFastSuite) \n        {\n            array_1d<double, 3> a = ZeroVector(3);\n            a[0] = 1.0;\n\n            const double b = MathUtils<double>::Norm3(a);\n            const double c = MathUtils<double>::Norm(a);\n            \n            KRATOS_CHECK_EQUAL(b, 1.0);\n            KRATOS_CHECK_EQUAL(c, 1.0);\n        }\n        \n        /** Checks if it calculates the cross product \n         * Checks if it calculates the cross product \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsCrossTest, KratosCoreMathUtilsFastSuite) \n        {\n            array_1d<double, 3> a = ZeroVector(3);\n            a[1] = 2.0;\n            array_1d<double, 3> b = ZeroVector(3);\n            b[0] = 1.0;\n\n            const array_1d<double, 3>  c = MathUtils<double>::CrossProduct(a, b);\n            const array_1d<double, 3>  d = MathUtils<double>::UnitCrossProduct(a, b);\n            \n            KRATOS_CHECK_EQUAL(c[2], 2.0);\n            KRATOS_CHECK_EQUAL(d[2], 1.0);\n        }\n        \n        /** Checks if it calculates the tensor product \n         * Checks if it calculates the tensor product \n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsTensorTest, KratosCoreMathUtilsFastSuite) \n        {\n            Vector a = ZeroVector(3);\n            a[1] = 2.0;\n            Vector b = ZeroVector(3);\n            b[0] = 1.0;\n\n            const Matrix c = MathUtils<double>::TensorProduct3(a, b);\n            \n            KRATOS_CHECK_EQUAL(c(0,0), 0.0);\n            KRATOS_CHECK_EQUAL(c(1,0), 2.0);\n            KRATOS_CHECK_EQUAL(c(0,1), 0.0);\n            KRATOS_CHECK_EQUAL(c(1,1), 0.0);\n        }\n        \n        /** Checks if it calculates the  matrix operations\n         * Checks if it calculates the  matrix operations\n         */\n        \n        KRATOS_TEST_CASE_IN_SUITE(MathUtilsMatrixOperationsTest, KratosCoreMathUtilsFastSuite) \n        {\n            Matrix a = IdentityMatrix(3);\n            Matrix b = IdentityMatrix(3);\n\n            MathUtils<double>::AddMatrix(a, b, 0 ,0);\n            \n            KRATOS_CHECK_EQUAL(a(0,0), 2.0);\n            KRATOS_CHECK_EQUAL(a(1,0), 0.0);\n            KRATOS_CHECK_EQUAL(a(0,1), 0.0);\n            KRATOS_CHECK_EQUAL(a(1,1), 2.0);\n            \n            MathUtils<double>::SubtractMatrix(a, b, 0 ,0);\n            \n            KRATOS_CHECK_EQUAL(a(0,0), 1.0);\n            KRATOS_CHECK_EQUAL(a(1,0), 0.0);\n            KRATOS_CHECK_EQUAL(a(0,1), 0.0);\n            KRATOS_CHECK_EQUAL(a(1,1), 1.0);\n            \n            MathUtils<double>::WriteMatrix(a, b, 0 ,0);\n            \n            KRATOS_CHECK_EQUAL(a(0,0), 1.0);\n            KRATOS_CHECK_EQUAL(a(1,0), 0.0);\n            KRATOS_CHECK_EQUAL(a(0,1), 0.0);\n            KRATOS_CHECK_EQUAL(a(1,1), 1.0);\n        }\n        \n    } // namespace Testing\n}  // namespace Kratos.\n\n",
  "patch": "@@ -595,8 +595,10 @@ namespace Kratos\n             array_1d<double, 3> b = ZeroVector(3);\n             b[0] = 1.0;\n \n-            const array_1d<double, 3>  c = MathUtils<double>::CrossProduct(a, b);\n-            const array_1d<double, 3>  d = MathUtils<double>::UnitCrossProduct(a, b);\n+            array_1d<double, 3>  c, d;\n+\n+            MathUtils<double>::CrossProduct(c, b, a);\n+            MathUtils<double>::UnitCrossProduct(d, b, a);\n             \n             KRATOS_CHECK_EQUAL(c[2], 2.0);\n             KRATOS_CHECK_EQUAL(d[2], 1.0);\n",
  "msg": "I assumed that for CrossProduct the values were inverted as well... Is that right?",
  "id": 90017,
  "y": 1
}

--- Sample 2 ---
{
  "oldf": "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# cython: profile=True\n\n\"\"\"Worker operations executor.\n\nFor internal use only; no backwards-compatibility guarantees.\n\"\"\"\n\nimport sys\nimport traceback\n\nimport six\n\nfrom apache_beam.internal import util\nfrom apache_beam.pvalue import TaggedOutput\nfrom apache_beam.transforms import DoFn\nfrom apache_beam.transforms import core\nfrom apache_beam.transforms.core import RestrictionProvider\nfrom apache_beam.transforms.window import GlobalWindow\nfrom apache_beam.transforms.window import TimestampedValue\nfrom apache_beam.transforms.window import WindowFn\nfrom apache_beam.utils.windowed_value import WindowedValue\n\n\nclass NameContext(object):\n  \"\"\"Holds the name information for a step.\"\"\"\n\n  def __init__(self, step_name):\n    \"\"\"Creates a new step NameContext.\n\n    Args:\n      step_name: The name of the step.\n    \"\"\"\n    self.step_name = step_name\n\n  def __eq__(self, other):\n    return self.step_name == other.step_name\n\n  def __ne__(self, other):\n    return not self == other\n\n  def __repr__(self):\n    return 'NameContext(%s)' % self.__dict__\n\n  def __hash__(self):\n    return hash(self.step_name)\n\n  def metrics_name(self):\n    \"\"\"Returns the step name used for metrics reporting.\"\"\"\n    return self.step_name\n\n  def logging_name(self):\n    \"\"\"Returns the step name used for logging.\"\"\"\n    return self.step_name\n\n\n# TODO(BEAM-4028): Move DataflowNameContext to Dataflow internal code.\nclass DataflowNameContext(NameContext):\n  \"\"\"Holds the name information for a step in Dataflow.\n\n  This includes a step_name (e.g. s2), a user_name (e.g. Foo/Bar/ParDo(Fab)),\n  and a system_name (e.g. s2-shuffle-read34).\"\"\"\n\n  def __init__(self, step_name, user_name, system_name):\n    \"\"\"Creates a new step NameContext.\n\n    Args:\n      step_name: The internal name of the step (e.g. s2).\n      user_name: The full user-given name of the step (e.g. Foo/Bar/ParDo(Far)).\n      system_name: The step name in the optimized graph (e.g. s2-1).\n    \"\"\"\n    super(DataflowNameContext, self).__init__(step_name)\n    self.user_name = user_name\n    self.system_name = system_name\n\n  def __eq__(self, other):\n    return (self.step_name == other.step_name and\n            self.user_name == other.user_name and\n            self.system_name == other.system_name)\n\n  def __ne__(self, other):\n    return not self == other\n\n  def __hash__(self):\n    return hash((self.step_name, self.user_name, self.system_name))\n\n  def __repr__(self):\n    return 'DataflowNameContext(%s)' % self.__dict__\n\n  def logging_name(self):\n    \"\"\"Stackdriver logging relies on user-given step names (e.g. Foo/Bar).\"\"\"\n    return self.user_name\n\n\nclass LoggingContext(object):\n  \"\"\"For internal use only; no backwards-compatibility guarantees.\"\"\"\n\n  def enter(self):\n    pass\n\n  def exit(self):\n    pass\n\n\nclass Receiver(object):\n  \"\"\"For internal use only; no backwards-compatibility guarantees.\n\n  An object that consumes a WindowedValue.\n\n  This class can be efficiently used to pass values between the\n  sdk and worker harnesses.\n  \"\"\"\n\n  def receive(self, windowed_value):\n    raise NotImplementedError\n\n\nclass MethodWrapper(object):\n  \"\"\"For internal use only; no backwards-compatibility guarantees.\n\n  Represents a method that can be invoked by `DoFnInvoker`.\"\"\"\n\n  def __init__(self, obj_to_invoke, method_name):\n    \"\"\"\n    Initiates a ``MethodWrapper``.\n\n    Args:\n      obj_to_invoke: the object that contains the method. Has to either be a\n                    `DoFn` object or a `RestrictionProvider` object.\n      method_name: name of the method as a string.\n    \"\"\"\n\n    if not isinstance(obj_to_invoke, (DoFn, RestrictionProvider)):\n      raise ValueError('\\'obj_to_invoke\\' has to be either a \\'DoFn\\' or '\n                       'a \\'RestrictionProvider\\'. Received %r instead.'\n                       % obj_to_invoke)\n\n    args, _, _, defaults = core.get_function_arguments(\n        obj_to_invoke, method_name)\n    defaults = defaults if defaults else []\n    method_value = getattr(obj_to_invoke, method_name)\n    self.method_value = method_value\n    self.args = args\n    self.defaults = defaults\n\n\nclass DoFnSignature(object):\n  \"\"\"Represents the signature of a given ``DoFn`` object.\n\n  Signature of a ``DoFn`` provides a view of the properties of a given ``DoFn``.\n  Among other things, this will give an extensible way for for (1) accessing the\n  structure of the ``DoFn`` including methods and method parameters\n  (2) identifying features that a given ``DoFn`` support, for example, whether\n  a given ``DoFn`` is a Splittable ``DoFn`` (\n  https://s.apache.org/splittable-do-fn) (3) validating a ``DoFn`` based on the\n  feature set offered by it.\n  \"\"\"\n\n  def __init__(self, do_fn):\n    # We add a property here for all methods defined by Beam DoFn features.\n\n    assert isinstance(do_fn, core.DoFn)\n    self.do_fn = do_fn\n\n    self.process_method = MethodWrapper(do_fn, 'process')\n    self.start_bundle_method = MethodWrapper(do_fn, 'start_bundle')\n    self.finish_bundle_method = MethodWrapper(do_fn, 'finish_bundle')\n\n    restriction_provider = self._get_restriction_provider(do_fn)\n    self.initial_restriction_method = (\n        MethodWrapper(restriction_provider, 'initial_restriction')\n        if restriction_provider else None)\n    self.restriction_coder_method = (\n        MethodWrapper(restriction_provider, 'restriction_coder')\n        if restriction_provider else None)\n    self.create_tracker_method = (\n        MethodWrapper(restriction_provider, 'create_tracker')\n        if restriction_provider else None)\n    self.split_method = (\n        MethodWrapper(restriction_provider, 'split')\n        if restriction_provider else None)\n\n    self._validate()\n\n  def _get_restriction_provider(self, do_fn):\n    result = _find_param_with_default(self.process_method,\n                                      default_as_type=RestrictionProvider)\n    return result[1] if result else None\n\n  def _validate(self):\n    self._validate_process()\n    self._validate_bundle_method(self.start_bundle_method)\n    self._validate_bundle_method(self.finish_bundle_method)\n\n  def _validate_process(self):\n    \"\"\"Validate that none of the DoFnParameters are repeated in the function\n    \"\"\"\n    for param in core.DoFn.DoFnParams:\n      assert self.process_method.defaults.count(param) <= 1\n\n  def _validate_bundle_method(self, method_wrapper):\n    \"\"\"Validate that none of the DoFnParameters are used in the function\n    \"\"\"\n    for param in core.DoFn.DoFnParams:\n      assert param not in method_wrapper.defaults\n\n  def is_splittable_dofn(self):\n    return any([isinstance(default, RestrictionProvider) for default in\n                self.process_method.defaults])\n\n\nclass DoFnInvoker(object):\n  \"\"\"An abstraction that can be used to execute DoFn methods.\n\n  A DoFnInvoker describes a particular way for invoking methods of a DoFn\n  represented by a given DoFnSignature.\"\"\"\n\n  def __init__(self, output_processor, signature):\n    self.output_processor = output_processor\n    self.signature = signature\n\n  @staticmethod\n  def create_invoker(\n      signature,\n      output_processor=None,\n      context=None, side_inputs=None, input_args=None, input_kwargs=None,\n      process_invocation=True):\n    \"\"\" Creates a new DoFnInvoker based on given arguments.\n\n    Args:\n        output_processor: an OutputProcessor for receiving elements produced by\n                          invoking functions of the DoFn.\n        signature: a DoFnSignature for the DoFn being invoked.\n        context: Context to be used when invoking the DoFn (deprecated).\n        side_inputs: side inputs to be used when invoking th process method.\n        input_args: arguments to be used when invoking the process method. Some\n                    of the arguments given here might be placeholders (for\n                    example for side inputs) that get filled before invoking the\n                    process method.\n        input_kwargs: keyword arguments to be used when invoking the process\n                      method. Some of the keyword arguments given here might be\n                      placeholders (for example for side inputs) that get filled\n                      before invoking the process method.\n        process_invocation: If True, this function may return an invoker that\n                            performs extra optimizations for invoking process()\n                            method efficiently.\n    \"\"\"\n    side_inputs = side_inputs or []\n    default_arg_values = signature.process_method.defaults\n    use_simple_invoker = not process_invocation or (\n        not side_inputs and not input_args and not input_kwargs and\n        not default_arg_values)\n    if use_simple_invoker:\n      return SimpleInvoker(output_processor, signature)\n    else:\n      return PerWindowInvoker(\n          output_processor,\n          signature, context, side_inputs, input_args, input_kwargs)\n\n  def invoke_process(self, windowed_value, restriction_tracker=None,\n                     output_processor=None,\n                     additional_args=None, additional_kwargs=None):\n    \"\"\"Invokes the DoFn.process() function.\n\n    Args:\n      windowed_value: a WindowedValue object that gives the element for which\n                      process() method should be invoked along with the window\n                      the element belongs to.\n      output_procesor: if provided given OutputProcessor will be used.\n      additional_args: additional arguments to be passed to the current\n                      `DoFn.process()` invocation, usually as side inputs.\n      additional_kwargs: additional keyword arguments to be passed to the\n                         current `DoFn.process()` invocation.\n    \"\"\"\n    raise NotImplementedError\n\n  def invoke_start_bundle(self):\n    \"\"\"Invokes the DoFn.start_bundle() method.\n    \"\"\"\n    self.output_processor.start_bundle_outputs(\n        self.signature.start_bundle_method.method_value())\n\n  def invoke_finish_bundle(self):\n    \"\"\"Invokes the DoFn.finish_bundle() method.\n    \"\"\"\n    self.output_processor.finish_bundle_outputs(\n        self.signature.finish_bundle_method.method_value())\n\n  def invoke_split(self, element, restriction):\n    return self.signature.split_method.method_value(element, restriction)\n\n  def invoke_initial_restriction(self, element):\n    return self.signature.initial_restriction_method.method_value(element)\n\n  def invoke_restriction_coder(self):\n    return self.signature.restriction_coder_method.method_value()\n\n  def invoke_create_tracker(self, restriction):\n    return self.signature.create_tracker_method.method_value(restriction)\n\n\ndef _find_param_with_default(\n    method, default_as_value=None, default_as_type=None):\n  if ((default_as_value and default_as_type) or\n      not (default_as_value or default_as_type)):\n    raise ValueError(\n        'Exactly one of \\'default_as_value\\' and \\'default_as_type\\' should be '\n        'provided. Received %r and %r.' % (default_as_value, default_as_type))\n\n  defaults = method.defaults\n  default_as_value = default_as_value\n  default_as_type = default_as_type\n  ret = None\n  for i, value in enumerate(defaults):\n    if default_as_value and value == default_as_value:\n      ret = (method.args[len(method.args) - len(defaults) + i], value)\n    elif default_as_type and isinstance(value, default_as_type):\n      index = len(method.args) - len(defaults) + i\n      ret = (method.args[index], value)\n\n  return ret\n\n\nclass SimpleInvoker(DoFnInvoker):\n  \"\"\"An invoker that processes elements ignoring windowing information.\"\"\"\n\n  def __init__(self, output_processor, signature):\n    super(SimpleInvoker, self).__init__(output_processor, signature)\n    self.process_method = signature.process_method.method_value\n\n  def invoke_process(self, windowed_value, restriction_tracker=None,\n                     output_processor=None,\n                     additional_args=None, additional_kwargs=None):\n    if not output_processor:\n      output_processor = self.output_processor\n    output_processor.process_outputs(\n        windowed_value, self.process_method(windowed_value.value))\n\n\nclass PerWindowInvoker(DoFnInvoker):\n  \"\"\"An invoker that processes elements considering windowing information.\"\"\"\n\n  def __init__(self, output_processor, signature, context,\n               side_inputs, input_args, input_kwargs):\n    super(PerWindowInvoker, self).__init__(output_processor, signature)\n    self.side_inputs = side_inputs\n    self.context = context\n    self.process_method = signature.process_method.method_value\n    default_arg_values = signature.process_method.defaults\n    self.has_windowed_inputs = (\n        not all(si.is_globally_windowed() for si in side_inputs) or\n        (core.DoFn.WindowParam in default_arg_values))\n\n    # Try to prepare all the arguments that can just be filled in\n    # without any additional work. in the process function.\n    # Also cache all the placeholders needed in the process function.\n\n    # Flag to cache additional arguments on the first element if all\n    # inputs are within the global window.\n    self.cache_globally_windowed_args = not self.has_windowed_inputs\n\n    input_args = input_args if input_args else []\n    input_kwargs = input_kwargs if input_kwargs else {}\n\n    arguments = signature.process_method.args\n    defaults = signature.process_method.defaults\n\n    # Create placeholder for element parameter of DoFn.process() method.\n    self_in_args = int(signature.do_fn.is_process_bounded())\n\n    class ArgPlaceholder(object):\n      def __init__(self, placeholder):\n        self.placeholder = placeholder\n\n    if core.DoFn.ElementParam not in default_arg_values:\n      args_to_pick = len(arguments) - len(default_arg_values) - 1 - self_in_args\n      args_with_placeholders = (\n          [ArgPlaceholder(core.DoFn.ElementParam)] + input_args[:args_to_pick])\n    else:\n      args_to_pick = len(arguments) - len(defaults) - self_in_args\n      args_with_placeholders = input_args[:args_to_pick]\n\n    # Fill the OtherPlaceholders for context, window or timestamp\n    remaining_args_iter = iter(input_args[args_to_pick:])\n    for a, d in zip(arguments[-len(defaults):], defaults):\n      if d == core.DoFn.ElementParam:\n        args_with_placeholders.append(ArgPlaceholder(d))\n      elif d == core.DoFn.WindowParam:\n        args_with_placeholders.append(ArgPlaceholder(d))\n      elif d == core.DoFn.TimestampParam:\n        args_with_placeholders.append(ArgPlaceholder(d))\n      elif d == core.DoFn.SideInputParam:\n        # If no more args are present then the value must be passed via kwarg\n        try:\n          args_with_placeholders.append(next(remaining_args_iter))\n        except StopIteration:\n          if a not in input_kwargs:\n            raise ValueError(\"Value for sideinput %s not provided\" % a)\n      else:\n        # If no more args are present then the value must be passed via kwarg\n        try:\n          args_with_placeholders.append(next(remaining_args_iter))\n        except StopIteration:\n          pass\n    args_with_placeholders.extend(list(remaining_args_iter))\n\n    # Stash the list of placeholder positions for performance\n    self.placeholders = [(i, x.placeholder) for (i, x) in enumerate(\n        args_with_placeholders)\n                         if isinstance(x, ArgPlaceholder)]\n\n    self.args_for_process = args_with_placeholders\n    self.kwargs_for_process = input_kwargs\n\n  def invoke_process(self, windowed_value, restriction_tracker=None,\n                     output_processor=None,\n                     additional_args=None, additional_kwargs=None):\n    if not additional_args:\n      additional_args = []\n    if not additional_kwargs:\n      additional_kwargs = {}\n\n    if not output_processor:\n      output_processor = self.output_processor\n    self.context.set_element(windowed_value)\n    # Call for the process function for each window if has windowed side inputs\n    # or if the process accesses the window parameter. We can just call it once\n    # otherwise as none of the arguments are changing\n\n    if restriction_tracker:\n      restriction_tracker_param = _find_param_with_default(\n          self.signature.process_method,\n          default_as_type=core.RestrictionProvider)[0]\n      if not restriction_tracker_param:\n        raise ValueError(\n            'A RestrictionTracker %r was provided but DoFn does not have a '\n            'RestrictionTrackerParam defined' % restriction_tracker)\n      additional_kwargs[restriction_tracker_param] = restriction_tracker\n    if self.has_windowed_inputs and len(windowed_value.windows) != 1:\n      for w in windowed_value.windows:\n        self._invoke_per_window(\n            WindowedValue(windowed_value.value, windowed_value.timestamp, (w,)),\n            additional_args, additional_kwargs, output_processor)\n    else:\n      self._invoke_per_window(\n          windowed_value, additional_args, additional_kwargs, output_processor)\n\n  def _invoke_per_window(\n      self, windowed_value, additional_args,\n      additional_kwargs, output_processor):\n    if self.has_windowed_inputs:\n      window, = windowed_value.windows\n      side_inputs = [si[window] for si in self.side_inputs]\n      side_inputs.extend(additional_args)\n      args_for_process, kwargs_for_process = util.insert_values_in_args(\n          self.args_for_process, self.kwargs_for_process,\n          side_inputs)\n    elif self.cache_globally_windowed_args:\n      # Attempt to cache additional args if all inputs are globally\n      # windowed inputs when processing the first element.\n      self.cache_globally_windowed_args = False\n\n      # Fill in sideInputs if they are globally windowed\n      global_window = GlobalWindow()\n      self.args_for_process, self.kwargs_for_process = (\n          util.insert_values_in_args(\n              self.args_for_process, self.kwargs_for_process,\n              [si[global_window] for si in self.side_inputs]))\n      args_for_process, kwargs_for_process = (\n          self.args_for_process, self.kwargs_for_process)\n    else:\n      args_for_process, kwargs_for_process = (\n          self.args_for_process, self.kwargs_for_process)\n    # TODO(sourabhbajaj): Investigate why we can't use `is` instead of ==\n    for i, p in self.placeholders:\n      if p == core.DoFn.ElementParam:\n        args_for_process[i] = windowed_value.value\n      elif p == core.DoFn.WindowParam:\n        args_for_process[i] = window\n      elif p == core.DoFn.TimestampParam:\n        args_for_process[i] = windowed_value.timestamp\n\n    if additional_kwargs:\n      if kwargs_for_process is None:\n        kwargs_for_process = additional_kwargs\n      else:\n        for key in additional_kwargs:\n          kwargs_for_process[key] = additional_kwargs[key]\n\n    if kwargs_for_process:\n      output_processor.process_outputs(\n          windowed_value,\n          self.process_method(*args_for_process, **kwargs_for_process))\n    else:\n      output_processor.process_outputs(\n          windowed_value, self.process_method(*args_for_process))\n\n\nclass DoFnRunner(Receiver):\n  \"\"\"For internal use only; no backwards-compatibility guarantees.\n\n  A helper class for executing ParDo operations.\n  \"\"\"\n\n  def __init__(self,\n               fn,\n               args,\n               kwargs,\n               side_inputs,\n               windowing,\n               tagged_receivers=None,\n               step_name=None,\n               logging_context=None,\n               state=None,\n               scoped_metrics_container=None):\n    \"\"\"Initializes a DoFnRunner.\n\n    Args:\n      fn: user DoFn to invoke\n      args: positional side input arguments (static and placeholder), if any\n      kwargs: keyword side input arguments (static and placeholder), if any\n      side_inputs: list of sideinput.SideInputMaps for deferred side inputs\n      windowing: windowing properties of the output PCollection(s)\n      tagged_receivers: a dict of tag name to Receiver objects\n      step_name: the name of this step\n      logging_context: a LoggingContext object\n      state: handle for accessing DoFn state\n      scoped_metrics_container: Context switcher for metrics container\n    \"\"\"\n    # Need to support multiple iterations.\n    side_inputs = list(side_inputs)\n\n    from apache_beam.metrics.execution import ScopedMetricsContainer\n\n    self.scoped_metrics_container = (\n        scoped_metrics_container or ScopedMetricsContainer())\n    self.step_name = step_name\n    self.logging_context = logging_context or LoggingContext()\n    self.context = DoFnContext(step_name, state=state)\n\n    do_fn_signature = DoFnSignature(fn)\n\n    # Optimize for the common case.\n    main_receivers = tagged_receivers[None]\n    output_processor = _OutputProcessor(\n        windowing.windowfn, main_receivers, tagged_receivers)\n\n    self.do_fn_invoker = DoFnInvoker.create_invoker(\n        do_fn_signature, output_processor, self.context, side_inputs, args,\n        kwargs)\n\n  def receive(self, windowed_value):\n    self.process(windowed_value)\n\n  def process(self, windowed_value):\n    try:\n      self.logging_context.enter()\n      self.scoped_metrics_container.enter()\n      self.do_fn_invoker.invoke_process(windowed_value)\n    except BaseException as exn:\n      self._reraise_augmented(exn)\n    finally:\n      self.scoped_metrics_container.exit()\n      self.logging_context.exit()\n\n  def _invoke_bundle_method(self, bundle_method):\n    try:\n      self.logging_context.enter()\n      self.scoped_metrics_container.enter()\n      self.context.set_element(None)\n      bundle_method()\n    except BaseException as exn:\n      self._reraise_augmented(exn)\n    finally:\n      self.scoped_metrics_container.exit()\n      self.logging_context.exit()\n\n  def start(self):\n    self._invoke_bundle_method(self.do_fn_invoker.invoke_start_bundle)\n\n  def finish(self):\n    self._invoke_bundle_method(self.do_fn_invoker.invoke_finish_bundle)\n\n  def _reraise_augmented(self, exn):\n    if getattr(exn, '_tagged_with_step', False) or not self.step_name:\n      raise\n    step_annotation = \" [while running '%s']\" % self.step_name\n    # To emulate exception chaining (not available in Python 2).\n    original_traceback = sys.exc_info()[2]\n    try:\n      # Attempt to construct the same kind of exception\n      # with an augmented message.\n      new_exn = type(exn)(exn.args[0] + step_annotation, *exn.args[1:])\n      new_exn._tagged_with_step = True  # Could raise attribute error.\n    except:  # pylint: disable=bare-except\n      # If anything goes wrong, construct a RuntimeError whose message\n      # records the original exception's type and message.\n      new_exn = RuntimeError(\n          traceback.format_exception_only(type(exn), exn)[-1].strip()\n          + step_annotation)\n      new_exn._tagged_with_step = True\n    six.reraise(type(new_exn), new_exn, original_traceback)\n\n\nclass OutputProcessor(object):\n\n  def process_outputs(self, windowed_input_element, results):\n    raise NotImplementedError\n\n\nclass _OutputProcessor(OutputProcessor):\n  \"\"\"Processes output produced by DoFn method invocations.\"\"\"\n\n  def __init__(self, window_fn, main_receivers, tagged_receivers):\n    \"\"\"Initializes ``_OutputProcessor``.\n\n    Args:\n      window_fn: a windowing function (WindowFn).\n      main_receivers: a dict of tag name to Receiver objects.\n      tagged_receivers: main receiver object.\n    \"\"\"\n    self.window_fn = window_fn\n    self.main_receivers = main_receivers\n    self.tagged_receivers = tagged_receivers\n\n  def process_outputs(self, windowed_input_element, results):\n    \"\"\"Dispatch the result of process computation to the appropriate receivers.\n\n    A value wrapped in a TaggedOutput object will be unwrapped and\n    then dispatched to the appropriate indexed output.\n    \"\"\"\n    if results is None:\n      return\n\n    for result in results:\n      tag = None\n      if isinstance(result, TaggedOutput):\n        tag = result.tag\n        if not isinstance(tag, six.string_types):\n          raise TypeError('In %s, tag %s is not a string' % (self, tag))\n        result = result.value\n      if isinstance(result, WindowedValue):\n        windowed_value = result\n        if (windowed_input_element is not None\n            and len(windowed_input_element.windows) != 1):\n          windowed_value.windows *= len(windowed_input_element.windows)\n      elif isinstance(result, TimestampedValue):\n        assign_context = WindowFn.AssignContext(result.timestamp, result.value)\n        windowed_value = WindowedValue(\n            result.value, result.timestamp,\n            self.window_fn.assign(assign_context))\n        if len(windowed_input_element.windows) != 1:\n          windowed_value.windows *= len(windowed_input_element.windows)\n      else:\n        windowed_value = windowed_input_element.with_value(result)\n      if tag is None:\n        self.main_receivers.receive(windowed_value)\n      else:\n        self.tagged_receivers[tag].receive(windowed_value)\n\n  def start_bundle_outputs(self, results):\n    \"\"\"Validate that start_bundle does not output any elements\"\"\"\n    if results is None:\n      return\n    raise RuntimeError(\n        'Start Bundle should not output any elements but got %s' % results)\n\n  def finish_bundle_outputs(self, results):\n    \"\"\"Dispatch the result of finish_bundle to the appropriate receivers.\n\n    A value wrapped in a TaggedOutput object will be unwrapped and\n    then dispatched to the appropriate indexed output.\n    \"\"\"\n    if results is None:\n      return\n\n    for result in results:\n      tag = None\n      if isinstance(result, TaggedOutput):\n        tag = result.tag\n        if not isinstance(tag, six.string_types):\n          raise TypeError('In %s, tag %s is not a string' % (self, tag))\n        result = result.value\n\n      if isinstance(result, WindowedValue):\n        windowed_value = result\n      else:\n        raise RuntimeError('Finish Bundle should only output WindowedValue ' +\\\n                           'type but got %s' % type(result))\n\n      if tag is None:\n        self.main_receivers.receive(windowed_value)\n      else:\n        self.tagged_receivers[tag].receive(windowed_value)\n\n\nclass _NoContext(WindowFn.AssignContext):\n  \"\"\"An uninspectable WindowFn.AssignContext.\"\"\"\n  NO_VALUE = object()\n\n  def __init__(self, value, timestamp=NO_VALUE):\n    self.value = value\n    self._timestamp = timestamp\n\n  @property\n  def timestamp(self):\n    if self._timestamp is self.NO_VALUE:\n      raise ValueError('No timestamp in this context.')\n    else:\n      return self._timestamp\n\n  @property\n  def existing_windows(self):\n    raise ValueError('No existing_windows in this context.')\n\n\nclass DoFnState(object):\n  \"\"\"For internal use only; no backwards-compatibility guarantees.\n\n  Keeps track of state that DoFns want, currently, user counters.\n  \"\"\"\n\n  def __init__(self, counter_factory):\n    self.step_name = ''\n    self._counter_factory = counter_factory\n\n  def counter_for(self, aggregator):\n    \"\"\"Looks up the counter for this aggregator, creating one if necessary.\"\"\"\n    return self._counter_factory.get_aggregator_counter(\n        self.step_name, aggregator)\n\n\n# TODO(robertwb): Replace core.DoFnContext with this.\nclass DoFnContext(object):\n  \"\"\"For internal use only; no backwards-compatibility guarantees.\"\"\"\n\n  def __init__(self, label, element=None, state=None):\n    self.label = label\n    self.state = state\n    if element is not None:\n      self.set_element(element)\n\n  def set_element(self, windowed_value):\n    self.windowed_value = windowed_value\n\n  @property\n  def element(self):\n    if self.windowed_value is None:\n      raise AttributeError('element not accessible in this context')\n    else:\n      return self.windowed_value.value\n\n  @property\n  def timestamp(self):\n    if self.windowed_value is None:\n      raise AttributeError('timestamp not accessible in this context')\n    else:\n      return self.windowed_value.timestamp\n\n  @property\n  def windows(self):\n    if self.windowed_value is None:\n      raise AttributeError('windows not accessible in this context')\n    else:\n      return self.windowed_value.windows\n",
  "patch": "@@ -22,8 +22,13 @@\n For internal use only; no backwards-compatibility guarantees.\n \"\"\"\n \n+from __future__ import absolute_import\n+\n import sys\n import traceback\n+from builtins import next\n+from builtins import object\n+from builtins import zip\n \n import six\n \n",
  "msg": "I think we should we avoid `import six` for consistency with the approach followed elsewhere. What do you think, @RobbeSneyders ? Looks like we are using `six.reraise` in a few places and `six.text_type` in apiclient.py.",
  "id": 144914,
  "y": 1
}

--- Sample 3 ---
{
  "oldf": "# frozen_string_literal: true\n\nrequire 'view/game/part/blocker'\nrequire 'view/game/part/borders'\nrequire 'view/game/part/cities'\nrequire 'view/game/part/label'\nrequire 'view/game/part/location_name'\nrequire 'view/game/part/revenue'\nrequire 'view/game/part/towns'\nrequire 'view/game/part/track'\nrequire 'view/game/part/upgrades'\n\nmodule View\n  module Game\n    class Tile < Snabberb::Component\n      needs :tile\n      needs :routes, default: [], store: true\n\n      # helper method to pass @tile and @region_use to every part\n      def render_tile_part(part_class, **kwargs)\n        h(part_class, region_use: @region_use, tile: @tile, **kwargs)\n      end\n\n      # if false, then the revenue is rendered by Part::Cities or Part::Towns\n      def should_render_revenue?\n        revenue = @tile.revenue_to_render\n\n        return false if revenue.empty?\n\n        return false if revenue.first.is_a?(Numeric) && (@tile.cities + @tile.towns).one?\n\n        return false if revenue.uniq.size > 1\n\n        return false if @tile.cities.sum(&:slots) < 3 && @tile.stops.size == 2\n\n        true\n      end\n\n      def render\n        # hash mapping the different regions to a number representing how much\n        # they've been used; it gets passed to the different tile parts and is\n        # modified before being passed on to the next one\n        @region_use = Hash.new(0)\n\n        children = []\n\n        render_revenue = should_render_revenue?\n        children << render_tile_part(Part::Track, routes: @routes) if @tile.exits.any?\n        children << render_tile_part(Part::Cities, show_revenue: !render_revenue) if @tile.cities.any?\n        children << render_tile_part(Part::Towns, routes: @routes) if @tile.towns.any?\n\n        # OO tiles have different rules...\n        rendered_loc_name = render_tile_part(Part::LocationName) if @tile.location_name && @tile.cities.size > 1\n\n        children << render_tile_part(Part::Revenue) if render_revenue\n        children << render_tile_part(Part::Label) if @tile.label\n\n        children << render_tile_part(Part::Upgrades) if @tile.upgrades.any?\n        children << render_tile_part(Part::Blocker)\n        rendered_loc_name = render_tile_part(Part::LocationName) if @tile.location_name && (@tile.cities.size <= 1)\n        @tile.reservations.each { |x| children << render_tile_part(Part::Reservation, reservation: x) }\n        children << render_tile_part(Part::Icons) if @tile.icons.any?\n\n        # borders should always be the top layer\n        children << h(Part::Borders, tile: @tile) if @tile.borders.any?\n\n        children << rendered_loc_name if rendered_loc_name\n\n        children.flatten!\n\n        h('g.tile', children)\n      end\n    end\n  end\nend\n",
  "patch": "@@ -25,13 +25,16 @@ module View\n       def should_render_revenue?\n         revenue = @tile.revenue_to_render\n \n+        # special case: city with multi-revenue - no choice but to draw separate revenue\n+        return true if revenue.any? { |r| !r.is_a?(Numeric) }\n+\n         return false if revenue.empty?\n \n         return false if revenue.first.is_a?(Numeric) && (@tile.cities + @tile.towns).one?\n \n         return false if revenue.uniq.size > 1\n \n-        return false if @tile.cities.sum(&:slots) < 3 && @tile.stops.size == 2\n+        return false if @tile.cities.sum(&:slots) < 3 && (@tile.cities + @tile.towns).size == 2\n \n         true\n       end\n",
  "msg": "we call cities + towns . size a lot, maybe make a helper method on tiles",
  "id": 12959,
  "y": 1
}

--- Sample 4 ---
{
  "oldf": "/*\n * (C) Copyright 2016-2021 Intel Corporation.\n *\n * SPDX-License-Identifier: BSD-2-Clause-Patent\n */\n/**\n * \\file\n *\n * ds_pool: Pool Service\n *\n * This file contains the server API methods and the RPC handlers that are both\n * related pool metadata.\n */\n\n#define D_LOGFAC DD_FAC(pool)\n\n#include <daos_srv/pool.h>\n\n#include <fcntl.h>\n#include <sys/stat.h>\n#include <daos_api.h> /* for daos_prop_alloc/_free() */\n#include <daos/pool_map.h>\n#include <daos/rpc.h>\n#include <daos/pool.h>\n#include <daos/rsvc.h>\n#include <daos_srv/container.h>\n#include <daos_srv/daos_mgmt_srv.h>\n#include <daos_srv/daos_engine.h>\n#include <daos_srv/rdb.h>\n#include <daos_srv/rebuild.h>\n#include <daos_srv/security.h>\n#include <cart/api.h>\n#include <cart/iv.h>\n#include \"rpc.h\"\n#include \"srv_internal.h\"\n#include \"srv_layout.h\"\n#include \"srv_pool_map.h\"\n\n/* Pool service */\nstruct pool_svc {\n\tstruct ds_rsvc\t\tps_rsvc;\n\tuuid_t\t\t\tps_uuid;\t/* pool UUID */\n\tstruct cont_svc\t       *ps_cont_svc;\t/* one combined svc for now */\n\tABT_rwlock\t\tps_lock;\t/* for DB data */\n\trdb_path_t\t\tps_root;\t/* root KVS */\n\trdb_path_t\t\tps_handles;\t/* pool handle KVS */\n\trdb_path_t\t\tps_user;\t/* pool user attributes KVS */\n\tstruct ds_pool\t       *ps_pool;\n};\n\nstatic bool pool_disable_evict = false;\nstatic int pool_prop_read(struct rdb_tx *tx, const struct pool_svc *svc,\n\t\t\t  uint64_t bits, daos_prop_t **prop_out);\nstatic int pool_space_query_bcast(crt_context_t ctx, struct pool_svc *svc,\n\t\t\t\t  uuid_t pool_hdl, struct daos_pool_space *ps);\n\nstatic struct pool_svc *\npool_svc_obj(struct ds_rsvc *rsvc)\n{\n\treturn container_of(rsvc, struct pool_svc, ps_rsvc);\n}\n\nstatic int\nwrite_map_buf(struct rdb_tx *tx, const rdb_path_t *kvs, struct pool_buf *buf,\n\t      uint32_t version)\n{\n\td_iov_t\tvalue;\n\tint\t\trc;\n\n\tD_DEBUG(DF_DSMS, \"version=%u ntargets=%u ndomains=%u\\n\", version,\n\t\tbuf->pb_target_nr, buf->pb_domain_nr);\n\n\t/* Write the version. */\n\td_iov_set(&value, &version, sizeof(version));\n\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_map_version, &value);\n\tif (rc != 0)\n\t\treturn rc;\n\n\t/* Write the buffer. */\n\td_iov_set(&value, buf, pool_buf_size(buf->pb_nr));\n\treturn rdb_tx_update(tx, kvs, &ds_pool_prop_map_buffer, &value);\n}\n\n/*\n * Retrieve the pool map buffer address in persistent memory and the pool map\n * version into \"map_buf\" and \"map_version\", respectively.\n */\nstatic int\nlocate_map_buf(struct rdb_tx *tx, const rdb_path_t *kvs, struct pool_buf **buf,\n\t       uint32_t *version)\n{\n\tuint32_t\tver;\n\td_iov_t\tvalue;\n\tint\t\trc;\n\n\t/* Read the version. */\n\td_iov_set(&value, &ver, sizeof(ver));\n\trc = rdb_tx_lookup(tx, kvs, &ds_pool_prop_map_version, &value);\n\tif (rc != 0)\n\t\treturn rc;\n\n\t/* Look up the buffer address. */\n\td_iov_set(&value, NULL /* buf */, 0 /* size */);\n\trc = rdb_tx_lookup(tx, kvs, &ds_pool_prop_map_buffer, &value);\n\tif (rc != 0)\n\t\treturn rc;\n\n\t*buf = value.iov_buf;\n\t*version = ver;\n\tD_DEBUG(DF_DSMS, \"version=%u ntargets=%u ndomains=%u\\n\", *version,\n\t\t(*buf)->pb_target_nr, (*buf)->pb_domain_nr);\n\treturn 0;\n}\n\n/* Callers are responsible for freeing buf with D_FREE. */\nstatic int\nread_map_buf(struct rdb_tx *tx, const rdb_path_t *kvs, struct pool_buf **buf,\n\t     uint32_t *version)\n{\n\tstruct pool_buf\t       *b;\n\tsize_t\t\t\tsize;\n\tint\t\t\trc;\n\n\trc = locate_map_buf(tx, kvs, &b, version);\n\tif (rc != 0)\n\t\treturn rc;\n\tsize = pool_buf_size(b->pb_nr);\n\tD_ALLOC(*buf, size);\n\tif (*buf == NULL)\n\t\treturn -DER_NOMEM;\n\tmemcpy(*buf, b, size);\n\treturn 0;\n}\n\n/* Callers are responsible for destroying the object via pool_map_decref(). */\nstatic int\nread_map(struct rdb_tx *tx, const rdb_path_t *kvs, struct pool_map **map)\n{\n\tstruct pool_buf\t       *buf;\n\tuint32_t\t\tversion;\n\tint\t\t\trc;\n\n\trc = locate_map_buf(tx, kvs, &buf, &version);\n\tif (rc != 0)\n\t\treturn rc;\n\n\treturn pool_map_create(buf, version, map);\n}\n\n/* Store uuid in file path. */\nstatic int\nuuid_store(const char *path, const uuid_t uuid)\n{\n\tint\tfd;\n\tint\trc;\n\n\t/* Create and open the UUID file. */\n\tfd = open(path, O_WRONLY | O_CREAT | O_EXCL, S_IRUSR | S_IWUSR);\n\tif (fd < 0) {\n\t\tD_ERROR(DF_UUID\": failed to create uuid file %s: %d\\n\",\n\t\t\tDP_UUID(uuid), path, errno);\n\t\trc = daos_errno2der(errno);\n\t\tgoto out;\n\t}\n\n\t/* Write the UUID. */\n\trc = write(fd, uuid, sizeof(uuid_t));\n\tif (rc != sizeof(uuid_t)) {\n\t\tif (rc != -1)\n\t\t\terrno = EIO;\n\t\tD_ERROR(DF_UUID\": failed to write uuid into %s: %d %d\\n\",\n\t\t\tDP_UUID(uuid), path, rc, errno);\n\t\trc = daos_errno2der(errno);\n\t\tgoto out_fd;\n\t}\n\n\t/* Persist the UUID. */\n\trc = fsync(fd);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to fsync %s: %d\\n\", DP_UUID(uuid),\n\t\t\tpath, errno);\n\t\trc = daos_errno2der(errno);\n\t}\n\n\t/* Free the resource and remove the file on errors. */\nout_fd:\n\tclose(fd);\n\tif (rc != 0)\n\t\tremove(path);\nout:\n\treturn rc;\n}\n\n/* Load uuid from file path. */\nstatic int\nuuid_load(const char *path, uuid_t uuid)\n{\n\tint\tfd;\n\tint\trc;\n\n\t/* Open the UUID file. */\n\tfd = open(path, O_RDONLY);\n\tif (fd < 0) {\n\t\tif (errno == ENOENT)\n\t\t\tD_DEBUG(DB_MD, \"failed to open uuid file %s: %d\\n\",\n\t\t\t\tpath, errno);\n\t\telse\n\t\t\tD_ERROR(\"failed to open uuid file %s: %d\\n\", path,\n\t\t\t\terrno);\n\t\trc = daos_errno2der(errno);\n\t\tgoto out;\n\t}\n\n\t/* Read the UUID. */\n\trc = read(fd, uuid, sizeof(uuid_t));\n\tif (rc == sizeof(uuid_t)) {\n\t\trc = 0;\n\t} else {\n\t\tif (rc != -1)\n\t\t\terrno = EIO;\n\t\tD_ERROR(\"failed to read %s: %d %d\\n\", path, rc, errno);\n\t\trc = daos_errno2der(errno);\n\t}\n\n\tclose(fd);\nout:\n\treturn rc;\n}\n\nstatic char *\npool_svc_rdb_path_common(const uuid_t pool_uuid, const char *suffix)\n{\n\tchar   *name;\n\tchar   *path;\n\tint\trc;\n\n\tD_ASPRINTF(name, RDB_FILE\"pool%s\", suffix);\n\tif (name == NULL)\n\t\treturn NULL;\n\trc = ds_mgmt_tgt_file(pool_uuid, name, NULL /* idx */, &path);\n\tD_FREE(name);\n\tif (rc != 0)\n\t\treturn NULL;\n\treturn path;\n}\n\n/* Return a pool service RDB path. */\nstatic char *\npool_svc_rdb_path(const uuid_t pool_uuid)\n{\n\treturn pool_svc_rdb_path_common(pool_uuid, \"\");\n}\n\n/* Return a pool service RDB UUID file path. This file stores the RDB UUID. */\nstatic char *\npool_svc_rdb_uuid_path(const uuid_t pool_uuid)\n{\n\treturn pool_svc_rdb_path_common(pool_uuid, \"-uuid\");\n}\n\n/*\n * Called by mgmt module on every storage node belonging to this pool.\n * \"path\" is the directory under which the VOS and metadata files shall be.\n * \"target_uuid\" returns the UUID generated for the target on this storage node.\n */\nint\nds_pool_create(const uuid_t pool_uuid, const char *path, uuid_t target_uuid)\n{\n\tchar   *fpath;\n\tint\trc;\n\n\tuuid_generate(target_uuid);\n\n\t/* Store target_uuid in DSM_META_FILE. */\n\tD_ASPRINTF(fpath, \"%s/%s\", path, DSM_META_FILE);\n\tif (fpath == NULL)\n\t\treturn -DER_NOMEM;\n\trc = uuid_store(fpath, target_uuid);\n\tD_FREE(fpath);\n\n\treturn rc;\n}\n\n/* copy \\a prop to \\a prop_def (duplicated default prop) */\nstatic int\npool_prop_default_copy(daos_prop_t *prop_def, daos_prop_t *prop)\n{\n\tstruct daos_prop_entry\t*entry;\n\tstruct daos_prop_entry\t*entry_def;\n\tint\t\t\t i;\n\n\tif (prop == NULL || prop->dpp_nr == 0 || prop->dpp_entries == NULL)\n\t\treturn 0;\n\n\tfor (i = 0; i < prop->dpp_nr; i++) {\n\t\tentry = &prop->dpp_entries[i];\n\t\tentry_def = daos_prop_entry_get(prop_def, entry->dpe_type);\n\t\tD_ASSERTF(entry_def != NULL, \"type %d not found in \"\n\t\t\t  \"default prop.\\n\", entry->dpe_type);\n\t\tswitch (entry->dpe_type) {\n\t\tcase DAOS_PROP_PO_LABEL:\n\t\t\tD_FREE(entry_def->dpe_str);\n\t\t\tD_STRNDUP(entry_def->dpe_str, entry->dpe_str,\n\t\t\t\t  DAOS_PROP_LABEL_MAX_LEN);\n\t\t\tif (entry_def->dpe_str == NULL)\n\t\t\t\treturn -DER_NOMEM;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_OWNER:\n\t\tcase DAOS_PROP_PO_OWNER_GROUP:\n\t\t\tD_FREE(entry_def->dpe_str);\n\t\t\tD_STRNDUP(entry_def->dpe_str, entry->dpe_str,\n\t\t\t\t  DAOS_ACL_MAX_PRINCIPAL_LEN);\n\t\t\tif (entry_def->dpe_str == NULL)\n\t\t\t\treturn -DER_NOMEM;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_SPACE_RB:\n\t\tcase DAOS_PROP_PO_SELF_HEAL:\n\t\tcase DAOS_PROP_PO_RECLAIM:\n\t\t\tentry_def->dpe_val = entry->dpe_val;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_ACL:\n\t\t\tif (entry->dpe_val_ptr != NULL) {\n\t\t\t\tstruct daos_acl *acl = entry->dpe_val_ptr;\n\n\t\t\t\tdaos_prop_entry_dup_ptr(entry_def, entry,\n\t\t\t\t\t\t\tdaos_acl_get_size(acl));\n\t\t\t\tif (entry_def->dpe_val_ptr == NULL)\n\t\t\t\t\treturn -DER_NOMEM;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tD_ERROR(\"ignore bad dpt_type %d.\\n\", entry->dpe_type);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int\npool_prop_write(struct rdb_tx *tx, const rdb_path_t *kvs, daos_prop_t *prop)\n{\n\tstruct daos_prop_entry\t*entry;\n\td_iov_t\t\t\t value;\n\tint\t\t\t i;\n\tint\t\t\t rc = 0;\n\n\tif (prop == NULL || prop->dpp_nr == 0 || prop->dpp_entries == NULL)\n\t\treturn 0;\n\n\tfor (i = 0; i < prop->dpp_nr; i++) {\n\t\tentry = &prop->dpp_entries[i];\n\t\tswitch (entry->dpe_type) {\n\t\tcase DAOS_PROP_PO_LABEL:\n\t\t\tif (entry->dpe_str == NULL ||\n\t\t\t    strlen(entry->dpe_str) == 0) {\n\t\t\t\tentry = daos_prop_entry_get(&pool_prop_default,\n\t\t\t\t\t\t\t    entry->dpe_type);\n\t\t\t\tD_ASSERT(entry != NULL);\n\t\t\t}\n\t\t\td_iov_set(&value, entry->dpe_str,\n\t\t\t\t     strlen(entry->dpe_str));\n\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_label,\n\t\t\t\t\t   &value);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_OWNER:\n\t\t\td_iov_set(&value, entry->dpe_str,\n\t\t\t\t     strlen(entry->dpe_str));\n\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_owner,\n\t\t\t\t\t   &value);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_OWNER_GROUP:\n\t\t\td_iov_set(&value, entry->dpe_str,\n\t\t\t\t     strlen(entry->dpe_str));\n\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_owner_group,\n\t\t\t\t\t   &value);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_ACL:\n\t\t\tif (entry->dpe_val_ptr != NULL) {\n\t\t\t\tstruct daos_acl *acl;\n\n\t\t\t\tacl = entry->dpe_val_ptr;\n\t\t\t\td_iov_set(&value, acl, daos_acl_get_size(acl));\n\t\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_acl,\n\t\t\t\t\t\t   &value);\n\t\t\t\tif (rc)\n\t\t\t\t\treturn rc;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_SPACE_RB:\n\t\t\td_iov_set(&value, &entry->dpe_val,\n\t\t\t\t     sizeof(entry->dpe_val));\n\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_space_rb,\n\t\t\t\t\t   &value);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_SELF_HEAL:\n\t\t\td_iov_set(&value, &entry->dpe_val,\n\t\t\t\t     sizeof(entry->dpe_val));\n\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_self_heal,\n\t\t\t\t\t   &value);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_RECLAIM:\n\t\t\td_iov_set(&value, &entry->dpe_val,\n\t\t\t\t     sizeof(entry->dpe_val));\n\t\t\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_reclaim,\n\t\t\t\t\t   &value);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t\tbreak;\n\t\tcase DAOS_PROP_PO_SVC_LIST:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tD_ERROR(\"bad dpe_type %d.\\n\", entry->dpe_type);\n\t\t\treturn -DER_INVAL;\n\t\t}\n\t}\n\n\treturn rc;\n}\n\nstatic int\ninit_pool_metadata(struct rdb_tx *tx, const rdb_path_t *kvs,\n\t\t   uint32_t nnodes, uuid_t target_uuids[], const char *group,\n\t\t   const d_rank_list_t *target_addrs, daos_prop_t *prop,\n\t\t   uint32_t ndomains, const int32_t *domains)\n{\n\tuint32_t\t\tversion = DS_POOL_MD_VERSION;\n\tstruct pool_buf\t       *map_buf;\n\tuint32_t\t\tmap_version = 1;\n\tuint32_t\t\tconnectable;\n\tuint32_t\t\tnhandles = 0;\n\tuuid_t\t\t       *uuids;\n\td_iov_t\t\t\tvalue;\n\tstruct rdb_kvs_attr\tattr;\n\tint\t\t\tntargets = nnodes * dss_tgt_nr;\n\tint\t\t\trc;\n\n\t/* Initialize the layout version. */\n\td_iov_set(&value, &version, sizeof(version));\n\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_version, &value);\n\tif (rc != 0)\n\t\tgoto out;\n\n\t/* Generate the pool buffer. */\n\trc = gen_pool_buf(NULL, &map_buf, map_version, ndomains, nnodes,\n\t\t\tntargets, domains, target_uuids, target_addrs, &uuids,\n\t\t\tdss_tgt_nr);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_buf, rc);\n\n\t/* Initialize the pool map properties. */\n\trc = write_map_buf(tx, kvs, map_buf, map_version);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\td_iov_set(&value, uuids, sizeof(uuid_t) * nnodes);\n\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_map_uuids, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\n\t/* Write the optional properties. */\n\trc = pool_prop_write(tx, kvs, prop);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\n\t/* Write connectable property */\n\tconnectable = 1;\n\td_iov_set(&value, &connectable, sizeof(connectable));\n\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_connectable, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\n\t/* Write the handle properties. */\n\td_iov_set(&value, &nhandles, sizeof(nhandles));\n\trc = rdb_tx_update(tx, kvs, &ds_pool_prop_nhandles, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\tattr.dsa_class = RDB_KVS_GENERIC;\n\tattr.dsa_order = 16;\n\trc = rdb_tx_create_kvs(tx, kvs, &ds_pool_prop_handles, &attr);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\n\t/* Create pool user attributes KVS */\n\trc = rdb_tx_create_kvs(tx, kvs, &ds_pool_attr_user, &attr);\n\tif (rc != 0)\n\t\tD_GOTO(out_uuids, rc);\n\nout_uuids:\n\tD_FREE(uuids);\nout_map_buf:\n\tpool_buf_free(map_buf);\nout:\n\treturn rc;\n}\n\n/*\n * nreplicas inputs how many replicas are wanted, while ranks->rl_nr\n * outputs how many replicas are actually selected, which may be less than\n * nreplicas. If successful, callers are responsible for calling\n * d_rank_list_free(*ranksp).\n */\nstatic int\nselect_svc_ranks(int nreplicas, const d_rank_list_t *target_addrs,\n\t\t int ndomains, const int *domains, d_rank_list_t **ranksp)\n{\n\tint\t\t\ti_rank_zero = -1;\n\tint\t\t\tselectable;\n\td_rank_list_t       *ranks;\n\tint\t\t\ti;\n\tint\t\t\tj;\n\n\tif (nreplicas <= 0)\n\t\treturn -DER_INVAL;\n\n\t/* Determine the number of selectable targets. */\n\tselectable = target_addrs->rl_nr;\n\tif (daos_rank_list_find((d_rank_list_t *)target_addrs, 0 /* rank */,\n\t\t\t\t&i_rank_zero)) {\n\t\t/*\n\t\t * Unless it is the only target available, we don't select rank\n\t\t * 0 for now to avoid losing orterun stdout.\n\t\t */\n\t\tif (selectable > 1)\n\t\t\tselectable -= 1 /* rank 0 */;\n\t}\n\n\tif (nreplicas > selectable)\n\t\tnreplicas = selectable;\n\tranks = daos_rank_list_alloc(nreplicas);\n\tif (ranks == NULL)\n\t\treturn -DER_NOMEM;\n\n\t/* TODO: Choose ranks according to failure domains. */\n\tj = 0;\n\tfor (i = 0; i < target_addrs->rl_nr; i++) {\n\t\tif (j == ranks->rl_nr)\n\t\t\tbreak;\n\t\tif (i == i_rank_zero && selectable > 1)\n\t\t\t/* This is rank 0 and it's not the only rank. */\n\t\t\tcontinue;\n\t\tD_DEBUG(DB_MD, \"ranks[%d]: %u\\n\", j, target_addrs->rl_ranks[i]);\n\t\tranks->rl_ranks[j] = target_addrs->rl_ranks[i];\n\t\tj++;\n\t}\n\tD_ASSERTF(j == ranks->rl_nr, \"%d == %u\\n\", j, ranks->rl_nr);\n\n\t*ranksp = ranks;\n\treturn 0;\n}\n\n/**\n * Create a (combined) pool(/container) service. This method shall be called on\n * a single storage node in the pool. \"target_uuids\" shall be an array of the\n * target UUIDs returned by the ds_pool_create() calls.\n *\n * \\param[in]\t\tpool_uuid\tpool UUID\n * \\param[in]\t\tntargets\tnumber of targets in the pool\n * \\param[in]\t\ttarget_uuids\tarray of \\a ntargets target UUIDs\n * \\param[in]\t\tgroup\t\tcrt group ID (unused now)\n * \\param[in]\t\ttarget_addrs\tlist of \\a ntargets target ranks\n * \\param[in]\t\tndomains\tnumber of domains the pool spans over\n * \\param[in]\t\tdomains\t\tserialized domain tree\n * \\param[in]\t\tprop\t\tpool properties\n * \\param[in,out]\tsvc_addrs\t\\a svc_addrs.rl_nr inputs how many\n *\t\t\t\t\treplicas shall be created; returns the\n *\t\t\t\t\tlist of pool service replica ranks\n */\nint\nds_pool_svc_create(const uuid_t pool_uuid, int ntargets, uuid_t target_uuids[],\n\t\t   const char *group, const d_rank_list_t *target_addrs,\n\t\t   int ndomains, const int *domains, daos_prop_t *prop,\n\t\t   d_rank_list_t *svc_addrs)\n{\n\td_rank_list_t\t       *ranks;\n\tuuid_t\t\t\trdb_uuid;\n\td_iov_t\t\tpsid;\n\tstruct rsvc_client\tclient;\n\tstruct dss_module_info *info = dss_get_module_info();\n\tcrt_endpoint_t\t\tep;\n\tcrt_rpc_t\t       *rpc;\n\tstruct pool_create_in  *in;\n\tstruct pool_create_out *out;\n\tint\t\t\trc;\n\n\tD_ASSERTF(ntargets == target_addrs->rl_nr, \"ntargets=%u num=%u\\n\",\n\t\t  ntargets, target_addrs->rl_nr);\n\n\trc = select_svc_ranks(svc_addrs->rl_nr, target_addrs, ndomains,\n\t\t\t      domains, &ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\tuuid_generate(rdb_uuid);\n\td_iov_set(&psid, (void *)pool_uuid, sizeof(uuid_t));\n\trc = ds_rsvc_dist_start(DS_RSVC_CLASS_POOL, &psid, rdb_uuid, ranks,\n\t\t\t\ttrue /* create */, true /* bootstrap */,\n\t\t\t\tds_rsvc_get_md_cap());\n\tif (rc != 0)\n\t\tD_GOTO(out_ranks, rc);\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out_creation, rc);\n\nrechoose:\n\t/* Create a POOL_CREATE request. */\n\tep.ep_grp = NULL;\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_CREATE, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create POOL_CREATE RPC: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out_client, rc);\n\t}\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pri_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->pri_op.pi_hdl);\n\tin->pri_ntgts = ntargets;\n\tin->pri_tgt_uuids.ca_count = ntargets;\n\tin->pri_tgt_uuids.ca_arrays = target_uuids;\n\tin->pri_tgt_ranks = (d_rank_list_t *)target_addrs;\n\tin->pri_prop = prop;\n\tin->pri_ndomains = ndomains;\n\tin->pri_domains.ca_count = ndomains;\n\tin->pri_domains.ca_arrays = (int *)domains;\n\n\t/* Send the POOL_CREATE request. */\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      rc == 0 ? out->pro_op.po_rc : -DER_IO,\n\t\t\t\t      rc == 0 ? &out->pro_op.po_hint : NULL);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\trc = out->pro_op.po_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out_rpc, rc);\n\t}\n\n\trc = daos_rank_list_copy(svc_addrs, ranks);\n\tD_ASSERTF(rc == 0, \"daos_rank_list_copy: \"DF_RC\"\\n\", DP_RC(rc));\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout_creation:\n\tif (rc != 0)\n\t\tds_rsvc_dist_stop(DS_RSVC_CLASS_POOL, &psid, ranks,\n\t\t\t\t  NULL, true /* destroy */);\nout_ranks:\n\td_rank_list_free(ranks);\nout:\n\treturn rc;\n}\n\nint\nds_pool_svc_destroy(const uuid_t pool_uuid, d_rank_list_t *svc_ranks)\n{\n\td_iov_t\t\tpsid;\n\tint\t\trc;\n\n\td_iov_set(&psid, (void *)pool_uuid, sizeof(uuid_t));\n\trc = ds_rsvc_dist_stop(DS_RSVC_CLASS_POOL, &psid, svc_ranks,\n\t\t\t       NULL /* excluded */, true /* destroy */);\n\tif (rc != 0)\n\t\tD_ERROR(DF_UUID\": failed to destroy pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\n\treturn rc;\n}\n\nstatic int\npool_svc_name_cb(d_iov_t *id, char **name)\n{\n\tchar *s;\n\n\tif (id->iov_len != sizeof(uuid_t))\n\t\treturn -DER_INVAL;\n\tD_ALLOC(s, DAOS_UUID_STR_SIZE);\n\tif (s == NULL)\n\t\treturn -DER_NOMEM;\n\tuuid_unparse_lower(id->iov_buf, s);\n\ts[8] = '\\0'; /* strlen(DF_UUID) */\n\t*name = s;\n\treturn 0;\n}\n\nstatic int\npool_svc_load_uuid_cb(d_iov_t *id, uuid_t db_uuid)\n{\n\tchar   *path;\n\tint\trc;\n\n\tif (id->iov_len != sizeof(uuid_t))\n\t\treturn -DER_INVAL;\n\tpath = pool_svc_rdb_uuid_path(id->iov_buf);\n\tif (path == NULL)\n\t\treturn -DER_NOMEM;\n\trc = uuid_load(path, db_uuid);\n\tD_FREE(path);\n\treturn rc;\n}\n\nstatic int\npool_svc_store_uuid_cb(d_iov_t *id, uuid_t db_uuid)\n{\n\tchar   *path;\n\tint\trc;\n\n\tif (id->iov_len != sizeof(uuid_t))\n\t\treturn -DER_INVAL;\n\tpath = pool_svc_rdb_uuid_path(id->iov_buf);\n\tif (path == NULL)\n\t\treturn -DER_NOMEM;\n\trc = uuid_store(path, db_uuid);\n\tD_FREE(path);\n\treturn rc;\n}\n\nstatic int\npool_svc_delete_uuid_cb(d_iov_t *id)\n{\n\tchar   *path;\n\tint\trc;\n\n\tif (id->iov_len != sizeof(uuid_t))\n\t\treturn -DER_INVAL;\n\tpath = pool_svc_rdb_uuid_path(id->iov_buf);\n\tif (path == NULL)\n\t\treturn -DER_NOMEM;\n\trc = remove(path);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to remove %s: %d\\n\",\n\t\t\tDP_UUID(id->iov_buf), path, errno);\n\t\trc = daos_errno2der(errno);\n\t}\n\tD_FREE(path);\n\treturn rc;\n}\n\nstatic int\npool_svc_locate_cb(d_iov_t *id, char **path)\n{\n\tchar *s;\n\n\tif (id->iov_len != sizeof(uuid_t))\n\t\treturn -DER_INVAL;\n\ts = pool_svc_rdb_path(id->iov_buf);\n\tif (s == NULL)\n\t\treturn -DER_NOMEM;\n\t*path = s;\n\treturn 0;\n}\n\nstatic int\npool_svc_alloc_cb(d_iov_t *id, struct ds_rsvc **rsvc)\n{\n\tstruct pool_svc\t       *svc;\n\tint\t\t\trc;\n\n\tif (id->iov_len != sizeof(uuid_t)) {\n\t\trc = -DER_INVAL;\n\t\tgoto err;\n\t}\n\n\tD_ALLOC_PTR(svc);\n\tif (svc == NULL) {\n\t\trc = -DER_NOMEM;\n\t\tgoto err;\n\t}\n\n\td_iov_set(&svc->ps_rsvc.s_id, svc->ps_uuid, sizeof(uuid_t));\n\n\tuuid_copy(svc->ps_uuid, id->iov_buf);\n\n\trc = ABT_rwlock_create(&svc->ps_lock);\n\tif (rc != ABT_SUCCESS) {\n\t\tD_ERROR(\"failed to create ps_lock: %d\\n\", rc);\n\t\trc = dss_abterr2der(rc);\n\t\tgoto err_svc;\n\t}\n\n\trc = rdb_path_init(&svc->ps_root);\n\tif (rc != 0)\n\t\tgoto err_lock;\n\trc = rdb_path_push(&svc->ps_root, &rdb_path_root_key);\n\tif (rc != 0)\n\t\tgoto err_root;\n\n\trc = rdb_path_clone(&svc->ps_root, &svc->ps_handles);\n\tif (rc != 0)\n\t\tgoto err_root;\n\trc = rdb_path_push(&svc->ps_handles, &ds_pool_prop_handles);\n\tif (rc != 0)\n\t\tgoto err_handles;\n\n\trc = rdb_path_clone(&svc->ps_root, &svc->ps_user);\n\tif (rc != 0)\n\t\tgoto err_handles;\n\trc = rdb_path_push(&svc->ps_user, &ds_pool_attr_user);\n\tif (rc != 0)\n\t\tgoto err_user;\n\n\trc = ds_cont_svc_init(&svc->ps_cont_svc, svc->ps_uuid, 0 /* id */,\n\t\t\t      &svc->ps_rsvc);\n\tif (rc != 0)\n\t\tgoto err_user;\n\n\t*rsvc = &svc->ps_rsvc;\n\treturn 0;\n\nerr_user:\n\trdb_path_fini(&svc->ps_user);\nerr_handles:\n\trdb_path_fini(&svc->ps_handles);\nerr_root:\n\trdb_path_fini(&svc->ps_root);\nerr_lock:\n\tABT_rwlock_free(&svc->ps_lock);\nerr_svc:\n\tD_FREE(svc);\nerr:\n\treturn rc;\n}\n\nstatic void\npool_svc_get(struct pool_svc *svc)\n{\n\tds_rsvc_get(&svc->ps_rsvc);\n}\n\nstatic void\npool_svc_put(struct pool_svc *svc)\n{\n\tds_rsvc_put(&svc->ps_rsvc);\n}\n\nstruct ds_pool_evict_arg {\n\tstruct pool_svc *svc;\n\td_rank_t\trank;\n};\n\nstatic void\npool_evict_rank_ult(void *data)\n{\n\tstruct ds_pool_evict_arg *arg = data;\n\tint\t\t\t rc;\n\n\trc = ds_pool_evict_rank(arg->svc->ps_uuid, arg->rank);\n\n\tD_DEBUG(DB_MGMT, DF_UUID\" evict rank %u : rc %d\\n\",\n\t\tDP_UUID(arg->svc->ps_uuid), arg->rank, rc);\n\n\tpool_svc_put(arg->svc);\n\tD_FREE_PTR(arg);\n}\n\n/* Disable all pools eviction */\nvoid\nds_pool_disable_evict(void)\n{\n\tpool_disable_evict = true;\n}\n\nvoid\nds_pool_enable_evict(void)\n{\n\tpool_disable_evict = false;\n}\n\nstatic int\npool_evict_rank(struct pool_svc *svc, d_rank_t rank)\n{\n\tstruct ds_pool_evict_arg\t*ult_arg;\n\tint\t\t\t\trc;\n\n\tD_ALLOC_PTR(ult_arg);\n\tif (ult_arg == NULL)\n\t\tD_GOTO(out, rc = -DER_NOMEM);\n\n\tpool_svc_get(svc);\n\tult_arg->svc = svc;\n\tult_arg->rank = rank;\n\trc = dss_ult_create(pool_evict_rank_ult, ult_arg, DSS_XS_SELF,\n\t\t\t    0, 0, NULL);\n\tif (rc) {\n\t\tpool_svc_put(svc);\n\t\tD_FREE_PTR(ult_arg);\n\t}\nout:\n\tif (rc)\n\t\tD_ERROR(\"evict ult failed: rc %d\\n\", rc);\n\treturn rc;\n}\n\nstatic void\nds_pool_crt_event_cb(d_rank_t rank, enum crt_event_source src,\n\t\t     enum crt_event_type type, void *arg)\n{\n\tdaos_prop_t\t\tprop = { 0 };\n\tstruct daos_prop_entry\t*entry;\n\tstruct pool_svc\t\t*svc = arg;\n\tint\t\t\trc = 0;\n\n\t/* Only used for evict the rank for the moment */\n\tif (src != CRT_EVS_SWIM || type != CRT_EVT_DEAD || pool_disable_evict) {\n\t\tD_DEBUG(DB_MGMT, \"ignore src/type/evict %u/%u/%d\\n\",\n\t\t\tsrc, type, pool_disable_evict);\n\t\treturn;\n\t}\n\n\trc = ds_pool_iv_prop_fetch(svc->ps_pool, &prop);\n\tif (rc)\n\t\tD_GOTO(out, rc);\n\n\tentry = daos_prop_entry_get(&prop, DAOS_PROP_PO_SELF_HEAL);\n\tD_ASSERT(entry != NULL);\n\tif (!(entry->dpe_val & DAOS_SELF_HEAL_AUTO_EXCLUDE)) {\n\t\tD_DEBUG(DB_MGMT, \"self healing is disabled\\n\");\n\t\tD_GOTO(out, rc);\n\t}\n\n\trc = pool_evict_rank(svc, rank);\nout:\n\tif (rc)\n\t\tD_ERROR(\"pool \"DF_UUID\" event %d failed: rc %d\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), src, rc);\n\tdaos_prop_fini(&prop);\n}\n\nstatic void\npool_svc_free_cb(struct ds_rsvc *rsvc)\n{\n\tstruct pool_svc *svc = pool_svc_obj(rsvc);\n\n\tds_cont_svc_fini(&svc->ps_cont_svc);\n\trdb_path_fini(&svc->ps_user);\n\trdb_path_fini(&svc->ps_handles);\n\trdb_path_fini(&svc->ps_root);\n\tABT_rwlock_free(&svc->ps_lock);\n\tD_FREE(svc);\n}\n\n/*\n * Initialize and update svc->ps_pool with map_buf and map_version. This\n * ensures that svc->ps_pool matches the latest pool map.\n */\nstatic int\ninit_svc_pool(struct pool_svc *svc, struct pool_buf *map_buf,\n\t      uint32_t map_version)\n{\n\tstruct ds_pool *pool;\n\tint\t\trc;\n\n\tpool = ds_pool_lookup(svc->ps_uuid);\n\tif (pool == NULL) {\n\t\tD_ERROR(DF_UUID\": failed to get ds_pool\\n\",\n\t\t\tDP_UUID(svc->ps_uuid));\n\t\treturn -DER_NONEXIST;\n\t}\n\trc = ds_pool_tgt_map_update(pool, map_buf, map_version);\n\tif (rc != 0) {\n\t\tds_pool_put(pool);\n\t\treturn rc;\n\t}\n\tds_pool_iv_ns_update(pool, dss_self_rank());\n\n\tD_ASSERT(svc->ps_pool == NULL);\n\tsvc->ps_pool = pool;\n\treturn 0;\n}\n\n/* Finalize svc->ps_pool. */\nstatic void\nfini_svc_pool(struct pool_svc *svc)\n{\n\tD_ASSERT(svc->ps_pool != NULL);\n\tds_pool_iv_ns_update(svc->ps_pool, -1 /* master_rank */);\n\tds_pool_put(svc->ps_pool);\n\tsvc->ps_pool = NULL;\n}\n\n/*\n * Read the DB for map_buf, map_version, and prop. Callers are responsible for\n * freeing *map_buf and *prop.\n */\nstatic int\nread_db_for_stepping_up(struct pool_svc *svc, struct pool_buf **map_buf,\n\t\t\tuint32_t *map_version, daos_prop_t **prop)\n{\n\tstruct rdb_tx\ttx;\n\td_iov_t\t\tvalue;\n\tbool\t\tversion_exists = false;\n\tuint32_t\tversion;\n\tint\t\trc;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tgoto out;\n\tABT_rwlock_rdlock(svc->ps_lock);\n\n\t/* Check the layout version. */\n\td_iov_set(&value, &version, sizeof(version));\n\trc = rdb_tx_lookup(&tx, &svc->ps_root, &ds_pool_prop_version, &value);\n\tif (rc == -DER_NONEXIST) {\n\t\t/*\n\t\t * This DB may be new or incompatible. Check the existence of\n\t\t * the pool map to find out which is the case. (See the\n\t\t * references to version_exists below.)\n\t\t */\n\t\tD_DEBUG(DB_MD, DF_UUID\": no layout version\\n\",\n\t\t\tDP_UUID(svc->ps_uuid));\n\t\tgoto check_map;\n\t} else if (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to look up layout version: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tgoto out_lock;\n\t}\n\tversion_exists = true;\n\tif (version < DS_POOL_MD_VERSION_LOW || version > DS_POOL_MD_VERSION) {\n\t\tds_notify_ras_eventf(RAS_POOL_DF_INCOMPAT, RAS_TYPE_INFO,\n\t\t\t\t     RAS_SEV_ERROR, NULL /* hwid */,\n\t\t\t\t     NULL /* rank */, NULL /* jobid */,\n\t\t\t\t     &svc->ps_uuid, NULL /* cont */,\n\t\t\t\t     NULL /* objid */, NULL /* ctlop */,\n\t\t\t\t     NULL /* data */,\n\t\t\t\t     \"incompatible layout version: %u not in \"\n\t\t\t\t     \"[%u, %u]\", version,\n\t\t\t\t     DS_POOL_MD_VERSION_LOW,\n\t\t\t\t     DS_POOL_MD_VERSION);\n\t\trc = -DER_DF_INCOMPT;\n\t\tgoto out_lock;\n\t}\n\ncheck_map:\n\trc = read_map_buf(&tx, &svc->ps_root, map_buf, map_version);\n\tif (rc != 0) {\n\t\tif (rc == -DER_NONEXIST && !version_exists) {\n\t\t\t/*\n\t\t\t * This DB is new. Note that if the layout version\n\t\t\t * exists, then the pool map must also exist;\n\t\t\t * otherwise, it is an error.\n\t\t\t */\n\t\t\tD_DEBUG(DB_MD, DF_UUID\": new db\\n\",\n\t\t\t\tDP_UUID(svc->ps_uuid));\n\t\t\trc = +DER_UNINIT;\n\t\t} else {\n\t\t\tD_ERROR(DF_UUID\": failed to read pool map buffer: \"DF_RC\n\t\t\t\t\"\\n\", DP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\t}\n\t\tgoto out_lock;\n\t}\n\tif (!version_exists) {\n\t\t/* This DB is not new and uses a layout that lacks a version. */\n\t\tds_notify_ras_eventf(RAS_POOL_DF_INCOMPAT, RAS_TYPE_INFO,\n\t\t\t\t     RAS_SEV_ERROR, NULL /* hwid */,\n\t\t\t\t     NULL /* rank */, NULL /* jobid */,\n\t\t\t\t     &svc->ps_uuid, NULL /* cont */,\n\t\t\t\t     NULL /* objid */, NULL /* ctlop */,\n\t\t\t\t     NULL /* data */,\n\t\t\t\t     \"incompatible layout version\");\n\t\trc = -DER_DF_INCOMPT;\n\t\tgoto out_lock;\n\t}\n\n\trc = pool_prop_read(&tx, svc, DAOS_PO_QUERY_PROP_ALL, prop);\n\tif (rc != 0)\n\t\tD_ERROR(DF_UUID\": cannot get access data for pool: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout:\n\treturn rc;\n}\n\n/*\n * There might be some swim status inconsistency, let's check and\n * fix it.\n */\nstatic int\npool_svc_check_node_status(struct pool_svc *svc)\n{\n\tstruct pool_domain\t*doms;\n\tint\t\t\tdoms_cnt;\n\tint\t\t\ti;\n\tint\t\t\trc = 0;\n\n\tif (pool_disable_evict) {\n\t\tD_DEBUG(DB_REBUILD, DF_UUID\" disable swim evict.\\n\",\n\t\t\tDP_UUID(svc->ps_uuid));\n\t\treturn 0;\n\t}\n\n\tdoms_cnt = pool_map_find_nodes(svc->ps_pool->sp_map, PO_COMP_ID_ALL,\n\t\t\t\t       &doms);\n\tD_ASSERT(doms_cnt >= 0);\n\tfor (i = 0; i < doms_cnt; i++) {\n\t\tstruct swim_member_state state;\n\n\t\t/* Only check if UPIN server becomes DEAD for now */\n\t\tif (!(doms[i].do_comp.co_status & PO_COMP_ST_UPIN))\n\t\t\tcontinue;\n\n\t\trc = crt_rank_state_get(crt_group_lookup(NULL),\n\t\t\t\t   doms[i].do_comp.co_rank, &state);\n\t\tif (rc != 0) {\n\t\t\tD_ERROR(\"failed to get swim for rank %u: %d\\n\",\n\t\t\t\tdoms[i].do_comp.co_rank, rc);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Since there is a big chance the INACTIVE node will become\n\t\t * ACTIVE soon, let's only evict the DEAD node rank for the\n\t\t * moment.\n\t\t */\n\t\tD_DEBUG(DB_REBUILD, \"rank/state %d/%d\\n\",\n\t\t\tdoms[i].do_comp.co_rank, state.sms_status);\n\t\tif (state.sms_status == SWIM_MEMBER_DEAD) {\n\t\t\trc = pool_evict_rank(svc, doms[i].do_comp.co_rank);\n\t\t\tif (rc) {\n\t\t\t\tD_ERROR(\"failed to evict rank %u: %d\\n\",\n\t\t\t\t\tdoms[i].do_comp.co_rank, rc);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn rc;\n}\n\nstatic int\npool_svc_step_up_cb(struct ds_rsvc *rsvc)\n{\n\tstruct pool_svc\t       *svc = pool_svc_obj(rsvc);\n\tstruct pool_buf\t       *map_buf = NULL;\n\tuint32_t\t\tmap_version;\n\tuuid_t\t\t\tpool_hdl_uuid;\n\tuuid_t\t\t\tcont_hdl_uuid;\n\tdaos_prop_t\t       *prop = NULL;\n\tbool\t\t\tcont_svc_up = false;\n\tbool\t\t\tevent_cb_registered = false;\n\td_rank_t\t\trank;\n\tint\t\t\trc;\n\n\trc = read_db_for_stepping_up(svc, &map_buf, &map_version, &prop);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = init_svc_pool(svc, map_buf, map_version);\n\tif (rc != 0)\n\t\tgoto out;\n\n\t/*\n\t * Just in case the previous leader didn't complete distributing the\n\t * latest pool map. This doesn't need to be undone if we encounter an\n\t * error below.\n\t */\n\tds_rsvc_request_map_dist(&svc->ps_rsvc);\n\n\trc = ds_cont_svc_step_up(svc->ps_cont_svc);\n\tif (rc != 0)\n\t\tgoto out;\n\tcont_svc_up = true;\n\n\trc = crt_register_event_cb(ds_pool_crt_event_cb, svc);\n\tif (rc)\n\t\tgoto out;\n\tevent_cb_registered = true;\n\n\trc = ds_pool_iv_prop_update(svc->ps_pool, prop);\n\tif (rc) {\n\t\tD_ERROR(\"ds_pool_iv_prop_update failed %d.\\n\", rc);\n\t\tD_GOTO(out, rc);\n\t}\n\n\trc = pool_svc_check_node_status(svc);\n\tif (rc)\n\t\tD_GOTO(out, rc);\n\n\tif (!uuid_is_null(svc->ps_pool->sp_srv_cont_hdl)) {\n\t\tuuid_copy(pool_hdl_uuid, svc->ps_pool->sp_srv_pool_hdl);\n\t\tuuid_copy(cont_hdl_uuid, svc->ps_pool->sp_srv_cont_hdl);\n\t} else {\n\t\tuuid_generate(pool_hdl_uuid);\n\t\tuuid_generate(cont_hdl_uuid);\n\t}\n\n\trc = ds_pool_iv_srv_hdl_update(svc->ps_pool, pool_hdl_uuid,\n\t\t\t\t       cont_hdl_uuid);\n\tif (rc) {\n\t\tD_ERROR(\"ds_pool_iv_srv_hdl_update failed %d.\\n\", rc);\n\t\tD_GOTO(out, rc);\n\t}\n\n\tD_PRINT(DF_UUID\": pool/cont hdl uuid \"DF_UUID\"/\"DF_UUID\"\\n\",\n\t\tDP_UUID(svc->ps_uuid), DP_UUID(pool_hdl_uuid),\n\t\tDP_UUID(cont_hdl_uuid));\n\n\trc = ds_rebuild_regenerate_task(svc->ps_pool);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = crt_group_rank(NULL, &rank);\n\tD_ASSERTF(rc == 0, \"\"DF_RC\"\\n\", DP_RC(rc));\n\tD_PRINT(DF_UUID\": rank %u became pool service leader \"DF_U64\"\\n\",\n\t\tDP_UUID(svc->ps_uuid), rank, svc->ps_rsvc.s_term);\nout:\n\tif (rc != 0) {\n\t\tif (event_cb_registered)\n\t\t\tcrt_unregister_event_cb(ds_pool_crt_event_cb, svc);\n\t\tif (cont_svc_up)\n\t\t\tds_cont_svc_step_down(svc->ps_cont_svc);\n\t\tif (svc->ps_pool != NULL)\n\t\t\tfini_svc_pool(svc);\n\t}\n\tif (map_buf != NULL)\n\t\tD_FREE(map_buf);\n\tif (prop != NULL)\n\t\tdaos_prop_free(prop);\n\treturn rc;\n}\n\nstatic void\npool_svc_step_down_cb(struct ds_rsvc *rsvc)\n{\n\tstruct pool_svc\t       *svc = pool_svc_obj(rsvc);\n\td_rank_t\t\trank;\n\tint\t\t\trc;\n\n\tcrt_unregister_event_cb(ds_pool_crt_event_cb, svc);\n\n\tds_pool_iv_srv_hdl_invalidate(svc->ps_pool);\n\tds_cont_svc_step_down(svc->ps_cont_svc);\n\tfini_svc_pool(svc);\n\n\trc = crt_group_rank(NULL, &rank);\n\tD_ASSERTF(rc == 0, \"\"DF_RC\"\\n\", DP_RC(rc));\n\tD_PRINT(DF_UUID\": rank %u no longer pool service leader \"DF_U64\"\\n\",\n\t\tDP_UUID(svc->ps_uuid), rank, svc->ps_rsvc.s_term);\n}\n\nstatic void\npool_svc_drain_cb(struct ds_rsvc *rsvc)\n{\n}\n\nstatic int\npool_svc_map_dist_cb(struct ds_rsvc *rsvc)\n{\n\tstruct pool_svc\t       *svc = pool_svc_obj(rsvc);\n\tstruct rdb_tx\t\ttx;\n\tstruct pool_buf\t       *map_buf = NULL;\n\tuint32_t\t\tmap_version;\n\tint\t\t\trc;\n\n\t/* Read the pool map into map_buf and map_version. */\n\trc = rdb_tx_begin(rsvc->s_db, rsvc->s_term, &tx);\n\tif (rc != 0)\n\t\tgoto out;\n\tABT_rwlock_rdlock(svc->ps_lock);\n\trc = read_map_buf(&tx, &svc->ps_root, &map_buf, &map_version);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to read pool map buffer: %d\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), rc);\n\t\tgoto out;\n\t}\n\n\trc = ds_pool_iv_map_update(svc->ps_pool, map_buf, map_version);\n\tif (rc != 0)\n\t\tD_ERROR(DF_UUID\": failed to distribute pool map %u: %d\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), map_version, rc);\n\nout:\n\tif (map_buf != NULL)\n\t\tD_FREE(map_buf);\n\treturn rc;\n}\n\nstatic struct ds_rsvc_class pool_svc_rsvc_class = {\n\t.sc_name\t= pool_svc_name_cb,\n\t.sc_load_uuid\t= pool_svc_load_uuid_cb,\n\t.sc_store_uuid\t= pool_svc_store_uuid_cb,\n\t.sc_delete_uuid\t= pool_svc_delete_uuid_cb,\n\t.sc_locate\t= pool_svc_locate_cb,\n\t.sc_alloc\t= pool_svc_alloc_cb,\n\t.sc_free\t= pool_svc_free_cb,\n\t.sc_step_up\t= pool_svc_step_up_cb,\n\t.sc_step_down\t= pool_svc_step_down_cb,\n\t.sc_drain\t= pool_svc_drain_cb,\n\t.sc_map_dist\t= pool_svc_map_dist_cb\n};\n\nvoid\nds_pool_rsvc_class_register(void)\n{\n\tds_rsvc_class_register(DS_RSVC_CLASS_POOL, &pool_svc_rsvc_class);\n}\n\nvoid\nds_pool_rsvc_class_unregister(void)\n{\n\tds_rsvc_class_unregister(DS_RSVC_CLASS_POOL);\n}\n\nstatic int\npool_svc_lookup(uuid_t uuid, struct pool_svc **svcp)\n{\n\tstruct ds_rsvc *rsvc;\n\td_iov_t\tid;\n\tint\t\trc;\n\n\td_iov_set(&id, uuid, sizeof(uuid_t));\n\trc = ds_rsvc_lookup(DS_RSVC_CLASS_POOL, &id, &rsvc);\n\tif (rc != 0)\n\t\treturn rc;\n\t*svcp = pool_svc_obj(rsvc);\n\treturn 0;\n}\n\nstatic int\npool_svc_lookup_leader(uuid_t uuid, struct pool_svc **svcp,\n\t\t       struct rsvc_hint *hint)\n{\n\tstruct ds_rsvc *rsvc;\n\td_iov_t\tid;\n\tint\t\trc;\n\n\td_iov_set(&id, uuid, sizeof(uuid_t));\n\trc = ds_rsvc_lookup_leader(DS_RSVC_CLASS_POOL, &id, &rsvc, hint);\n\tif (rc != 0)\n\t\treturn rc;\n\t*svcp = pool_svc_obj(rsvc);\n\treturn 0;\n}\n\nstatic void\npool_svc_put_leader(struct pool_svc *svc)\n{\n\tds_rsvc_put_leader(&svc->ps_rsvc);\n}\n\n/** Look up container service \\a pool_uuid. */\nint\nds_pool_cont_svc_lookup_leader(uuid_t pool_uuid, struct cont_svc **svcp,\n\t\t\t       struct rsvc_hint *hint)\n{\n\tstruct pool_svc\t       *pool_svc;\n\tint\t\t\trc;\n\n\trc = pool_svc_lookup_leader(pool_uuid, &pool_svc, hint);\n\tif (rc != 0)\n\t\treturn rc;\n\t*svcp = pool_svc->ps_cont_svc;\n\treturn 0;\n}\n\n/*\n * Try to start the pool. If a pool service RDB exists, start it. Continue the\n * iteration upon errors as other pools may still be able to work.\n */\nstatic int\nstart_one(uuid_t uuid, void *varg)\n{\n\tchar\t       *path;\n\td_iov_t\t\tid;\n\tstruct stat\tst;\n\tint\t\trc;\n\n\tD_DEBUG(DB_MD, DF_UUID\": starting pool\\n\", DP_UUID(uuid));\n\n\trc = ds_pool_start(uuid);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to start pool: %d\\n\", DP_UUID(uuid),\n\t\t\trc);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Check if an RDB file exists, to avoid unnecessary error messages\n\t * from the ds_rsvc_start() call.\n\t */\n\tpath = pool_svc_rdb_path(uuid);\n\tif (path == NULL) {\n\t\tD_ERROR(DF_UUID\": failed to allocate rdb path\\n\",\n\t\t\tDP_UUID(uuid));\n\t\treturn 0;\n\t}\n\trc = stat(path, &st);\n\tD_FREE(path);\n\tif (rc != 0) {\n\t\tif (errno != ENOENT)\n\t\t\tD_ERROR(DF_UUID\": failed to check rdb existence: %d\\n\",\n\t\t\t\tDP_UUID(uuid), errno);\n\t\treturn 0;\n\t}\n\n\td_iov_set(&id, uuid, sizeof(uuid_t));\n\tds_rsvc_start(DS_RSVC_CLASS_POOL, &id, NULL /* db_uuid */,\n\t\t      false /* create */, 0 /* size */, NULL /* replicas */,\n\t\t      NULL /* arg */);\n\treturn 0;\n}\n\nstatic void\npool_start_all(void *arg)\n{\n\tint rc;\n\n\t/* Scan the storage and start all pool services. */\n\trc = ds_mgmt_tgt_pool_iterate(start_one, NULL /* arg */);\n\tif (rc != 0)\n\t\tD_ERROR(\"failed to scan all pool services: \"DF_RC\"\\n\",\n\t\t\tDP_RC(rc));\n}\n\n/* Note that this function is currently called from the main xstream. */\nint\nds_pool_start_all(void)\n{\n\tABT_thread\tthread;\n\tint\t\trc;\n\n\t/* Create a ULT to call ds_rsvc_start() in xstream 0. */\n\trc = dss_ult_create(pool_start_all, NULL /* arg */, DSS_XS_SYS,\n\t\t\t    0 /* tgt_idx */, 0 /* stack_size */, &thread);\n\tif (rc != 0) {\n\t\tD_ERROR(\"failed to create pool start ULT: \"DF_RC\"\\n\",\n\t\t\tDP_RC(rc));\n\t\treturn rc;\n\t}\n\tABT_thread_join(thread);\n\tABT_thread_free(&thread);\n\treturn 0;\n}\n\nstatic int\nstop_one(uuid_t uuid, void *varg)\n{\n\tD_DEBUG(DB_MD, DF_UUID\": stopping pool\\n\", DP_UUID(uuid));\n\tds_pool_stop(uuid);\n\treturn 0;\n}\n\nstatic void\npool_stop_all(void *arg)\n{\n\tint\trc;\n\n\trc = ds_mgmt_tgt_pool_iterate(stop_one, NULL /* arg */);\n\tif (rc != 0)\n\t\tD_ERROR(\"failed to stop all pools: \"DF_RC\"\\n\", DP_RC(rc));\n}\n\n/*\n * Note that this function is currently called from the main xstream to save\n * one ULT creation.\n */\nint\nds_pool_stop_all(void)\n{\n\tABT_thread\tthread;\n\tint\t\trc;\n\n\trc = ds_rsvc_stop_all(DS_RSVC_CLASS_POOL);\n\tif (rc)\n\t\tD_ERROR(\"failed to stop all pool svcs: \"DF_RC\"\\n\", DP_RC(rc));\n\n\t/* Create a ULT to stop pools, since it requires TLS */\n\trc = dss_ult_create(pool_stop_all, NULL /* arg */, DSS_XS_SYS,\n\t\t\t    0 /* tgt_idx */, 0 /* stack_size */, &thread);\n\tif (rc != 0) {\n\t\tD_ERROR(\"failed to create pool stop ULT: \"DF_RC\"\\n\",\n\t\t\tDP_RC(rc));\n\t\treturn rc;\n\t}\n\tABT_thread_join(thread);\n\tABT_thread_free(&thread);\n\n\treturn 0;\n}\n\nstatic int\nbcast_create(crt_context_t ctx, struct pool_svc *svc, crt_opcode_t opcode,\n\t     crt_bulk_t bulk_hdl, crt_rpc_t **rpc)\n{\n\treturn ds_pool_bcast_create(ctx, svc->ps_pool, DAOS_POOL_MODULE, opcode,\n\t\t\t\t    DAOS_POOL_VERSION, rpc, bulk_hdl, NULL);\n}\n\n/**\n * Retrieve the latest leader hint from \\a db and fill it into \\a hint.\n *\n * \\param[in]\tdb\tdatabase\n * \\param[out]\thint\trsvc hint\n */\nvoid\nds_pool_set_hint(struct rdb *db, struct rsvc_hint *hint)\n{\n\tint rc;\n\n\trc = rdb_get_leader(db, &hint->sh_term, &hint->sh_rank);\n\tif (rc != 0)\n\t\treturn;\n\thint->sh_flags |= RSVC_HINT_VALID;\n}\n\nstatic int\npool_prop_read(struct rdb_tx *tx, const struct pool_svc *svc, uint64_t bits,\n\t       daos_prop_t **prop_out)\n{\n\tdaos_prop_t\t*prop;\n\td_iov_t\t value;\n\tuint64_t\t val;\n\tuint32_t\t idx = 0, nr = 0;\n\tint\t\t rc;\n\n\tif (bits & DAOS_PO_QUERY_PROP_LABEL)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_SPACE_RB)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_SELF_HEAL)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_RECLAIM)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_ACL)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_OWNER)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_OWNER_GROUP)\n\t\tnr++;\n\tif (bits & DAOS_PO_QUERY_PROP_SVC_LIST)\n\t\tnr++;\n\tif (nr == 0)\n\t\treturn 0;\n\n\tprop = daos_prop_alloc(nr);\n\tif (prop == NULL)\n\t\treturn -DER_NOMEM;\n\t*prop_out = prop;\n\tif (bits & DAOS_PO_QUERY_PROP_LABEL) {\n\t\td_iov_set(&value, NULL, 0);\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_label,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tif (value.iov_len > DAOS_PROP_LABEL_MAX_LEN) {\n\t\t\tD_ERROR(\"bad label length %zu (> %d).\\n\", value.iov_len,\n\t\t\t\tDAOS_PROP_LABEL_MAX_LEN);\n\t\t\treturn -DER_IO;\n\t\t}\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_LABEL;\n\t\tD_STRNDUP(prop->dpp_entries[idx].dpe_str, value.iov_buf,\n\t\t\t  value.iov_len);\n\t\tif (prop->dpp_entries[idx].dpe_str == NULL)\n\t\t\treturn -DER_NOMEM;\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_SPACE_RB) {\n\t\td_iov_set(&value, &val, sizeof(val));\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_space_rb,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_SPACE_RB;\n\t\tprop->dpp_entries[idx].dpe_val = val;\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_SELF_HEAL) {\n\t\td_iov_set(&value, &val, sizeof(val));\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_self_heal,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_SELF_HEAL;\n\t\tprop->dpp_entries[idx].dpe_val = val;\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_RECLAIM) {\n\t\td_iov_set(&value, &val, sizeof(val));\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_reclaim,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_RECLAIM;\n\t\tprop->dpp_entries[idx].dpe_val = val;\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_ACL) {\n\t\td_iov_set(&value, NULL, 0);\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_acl,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_ACL;\n\t\tD_ALLOC(prop->dpp_entries[idx].dpe_val_ptr, value.iov_buf_len);\n\t\tif (prop->dpp_entries[idx].dpe_val_ptr == NULL)\n\t\t\treturn -DER_NOMEM;\n\t\tmemcpy(prop->dpp_entries[idx].dpe_val_ptr, value.iov_buf,\n\t\t       value.iov_buf_len);\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_OWNER) {\n\t\td_iov_set(&value, NULL, 0);\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_owner,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tif (value.iov_len > DAOS_ACL_MAX_PRINCIPAL_LEN) {\n\t\t\tD_ERROR(\"bad owner length %zu (> %d).\\n\", value.iov_len,\n\t\t\t\tDAOS_ACL_MAX_PRINCIPAL_LEN);\n\t\t\treturn -DER_IO;\n\t\t}\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_OWNER;\n\t\tD_STRNDUP(prop->dpp_entries[idx].dpe_str, value.iov_buf,\n\t\t\t  value.iov_len);\n\t\tif (prop->dpp_entries[idx].dpe_str == NULL)\n\t\t\treturn -DER_NOMEM;\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_OWNER_GROUP) {\n\t\td_iov_set(&value, NULL, 0);\n\t\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_owner_group,\n\t\t\t\t   &value);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t\tif (value.iov_len > DAOS_ACL_MAX_PRINCIPAL_LEN) {\n\t\t\tD_ERROR(\"bad owner group length %zu (> %d).\\n\",\n\t\t\t\tvalue.iov_len,\n\t\t\t\tDAOS_ACL_MAX_PRINCIPAL_LEN);\n\t\t\treturn -DER_IO;\n\t\t}\n\t\tD_ASSERT(idx < nr);\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_OWNER_GROUP;\n\t\tD_STRNDUP(prop->dpp_entries[idx].dpe_str, value.iov_buf,\n\t\t\t  value.iov_len);\n\t\tif (prop->dpp_entries[idx].dpe_str == NULL)\n\t\t\treturn -DER_NOMEM;\n\t\tidx++;\n\t}\n\tif (bits & DAOS_PO_QUERY_PROP_SVC_LIST) {\n\t\td_rank_list_t\t*svc_list = NULL;\n\n\t\td_iov_set(&value, NULL, 0);\n\t\trc = rdb_get_ranks(svc->ps_rsvc.s_db, &svc_list);\n\t\tif (rc) {\n\t\t\tD_ERROR(\"get svc list failed: rc %d\\n\", rc);\n\t\t\treturn rc;\n\t\t}\n\t\tprop->dpp_entries[idx].dpe_type = DAOS_PROP_PO_SVC_LIST;\n\t\tprop->dpp_entries[idx].dpe_val_ptr = svc_list;\n\t\tidx++;\n\t}\n\n\treturn 0;\n}\n\n/*\n * We use this RPC to not only create the pool metadata but also initialize the\n * pool/container service DB.\n */\nvoid\nds_pool_create_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_create_in  *in = crt_req_get(rpc);\n\tstruct pool_create_out *out = crt_reply_get(rpc);\n\tstruct pool_svc\t       *svc;\n\tstruct rdb_tx\t\ttx;\n\td_iov_t\t\t\tvalue;\n\tstruct rdb_kvs_attr\tattr;\n\tdaos_prop_t\t       *prop_dup = NULL;\n\tint\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->pri_op.pi_uuid), rpc);\n\n\tif (in->pri_ntgts != in->pri_tgt_uuids.ca_count ||\n\t    in->pri_ntgts != in->pri_tgt_ranks->rl_nr)\n\t\tD_GOTO(out, rc = -DER_PROTO);\n\tif (in->pri_ndomains != in->pri_domains.ca_count)\n\t\tD_GOTO(out, rc = -DER_PROTO);\n\n\t/* This RPC doesn't care about whether the service is up. */\n\trc = pool_svc_lookup(in->pri_op.pi_uuid, &svc);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/*\n\t * Simply serialize this whole RPC with rsvc_step_{up,down}_cb() and\n\t * ds_rsvc_stop().\n\t */\n\tABT_mutex_lock(svc->ps_rsvc.s_mutex);\n\n\tif (svc->ps_rsvc.s_stop) {\n\t\tD_DEBUG(DB_MD, DF_UUID\": pool service already stopping\\n\",\n\t\t\tDP_UUID(svc->ps_uuid));\n\t\tD_GOTO(out_mutex, rc = -DER_CANCELED);\n\t}\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, RDB_NIL_TERM, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_mutex, rc);\n\tABT_rwlock_wrlock(svc->ps_lock);\n\tds_cont_wrlock_metadata(svc->ps_cont_svc);\n\n\t/* See if the DB has already been initialized. */\n\td_iov_set(&value, NULL /* buf */, 0 /* size */);\n\trc = rdb_tx_lookup(&tx, &svc->ps_root, &ds_pool_prop_map_buffer,\n\t\t\t   &value);\n\tif (rc != -DER_NONEXIST) {\n\t\tif (rc == 0)\n\t\t\tD_DEBUG(DF_DSMS, DF_UUID\": db already initialized\\n\",\n\t\t\t\tDP_UUID(svc->ps_uuid));\n\t\telse\n\t\t\tD_ERROR(DF_UUID\": failed to look up pool map: \"\n\t\t\t\tDF_RC\"\\n\", DP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tD_GOTO(out_tx, rc);\n\t}\n\n\t/* duplicate the default properties, overwrite it with pool create\n\t * parameter and then write to pool meta data.\n\t */\n\tprop_dup = daos_prop_dup(&pool_prop_default, true);\n\tif (prop_dup == NULL) {\n\t\tD_ERROR(\"daos_prop_dup failed.\\n\");\n\t\tD_GOTO(out_tx, rc = -DER_NOMEM);\n\t}\n\trc = pool_prop_default_copy(prop_dup, in->pri_prop);\n\tif (rc) {\n\t\tD_ERROR(\"daos_prop_default_copy failed.\\n\");\n\t\tD_GOTO(out_tx, rc);\n\t}\n\n\t/* Initialize the DB and the metadata for this pool. */\n\tattr.dsa_class = RDB_KVS_GENERIC;\n\tattr.dsa_order = 8;\n\trc = rdb_tx_create_root(&tx, &attr);\n\tif (rc != 0)\n\t\tD_GOTO(out_tx, rc);\n\trc = init_pool_metadata(&tx, &svc->ps_root, in->pri_tgt_uuids.ca_count,\n\t\t\t\tin->pri_tgt_uuids.ca_arrays, NULL /* group */,\n\t\t\t\tin->pri_tgt_ranks, prop_dup,\n\t\t\t\tin->pri_ndomains, in->pri_domains.ca_arrays);\n\tif (rc != 0)\n\t\tD_GOTO(out_tx, rc);\n\trc = ds_cont_init_metadata(&tx, &svc->ps_root, in->pri_op.pi_uuid);\n\tif (rc != 0)\n\t\tD_GOTO(out_tx, rc);\n\n\trc = rdb_tx_commit(&tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_tx, rc);\n\nout_tx:\n\tdaos_prop_free(prop_dup);\n\tds_cont_unlock_metadata(svc->ps_cont_svc);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tif (svc->ps_rsvc.s_state == DS_RSVC_UP_EMPTY) {\n\t\t/*\n\t\t * The DB is no longer empty. Since the previous\n\t\t * pool_svc_step_up_cb() call didn't finish stepping up due to\n\t\t * an empty DB, and there hasn't been a pool_svc_step_down_cb()\n\t\t * call yet, we should call pool_svc_step_up() to finish\n\t\t * stepping up.\n\t\t */\n\t\tD_DEBUG(DF_DSMS, DF_UUID\": trying to finish stepping up\\n\",\n\t\t\tDP_UUID(in->pri_op.pi_uuid));\n\t\trc = pool_svc_step_up_cb(&svc->ps_rsvc);\n\t\tif (rc != 0) {\n\t\t\tD_ASSERT(rc != DER_UNINIT);\n\t\t\t/* TODO: Ask rdb to step down. */\n\t\t\tD_GOTO(out_svc, rc);\n\t\t}\n\t\tsvc->ps_rsvc.s_state = DS_RSVC_UP;\n\t\tABT_cond_broadcast(svc->ps_rsvc.s_state_cv);\n\t}\n\nout_mutex:\n\tABT_mutex_unlock(svc->ps_rsvc.s_mutex);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pro_op.po_hint);\n\tpool_svc_put(svc);\nout:\n\tout->pro_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pri_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nstatic int\npool_connect_iv_dist(struct pool_svc *svc, uuid_t pool_hdl,\n\t\t     uint64_t flags, uint64_t sec_capas, d_iov_t *cred)\n{\n\td_rank_t rank;\n\tint\t rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": bcasting\\n\", DP_UUID(svc->ps_uuid));\n\n\trc = crt_group_rank(svc->ps_pool->sp_group, &rank);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = ds_pool_iv_conn_hdl_update(svc->ps_pool, pool_hdl, flags,\n\t\t\t\t\tsec_capas, cred);\n\tif (rc)\n\t\tD_GOTO(out, rc);\nout:\n\tD_DEBUG(DF_DSMS, DF_UUID\": bcasted: \"DF_RC\"\\n\", DP_UUID(svc->ps_uuid),\n\t\tDP_RC(rc));\n\treturn rc;\n}\n\nstatic int\nbulk_cb(const struct crt_bulk_cb_info *cb_info)\n{\n\tABT_eventual *eventual = cb_info->bci_arg;\n\n\tABT_eventual_set(*eventual, (void *)&cb_info->bci_rc,\n\t\t\t sizeof(cb_info->bci_rc));\n\treturn 0;\n}\n\n/*\n * Transfer the pool map to \"remote_bulk\". If the remote bulk buffer is too\n * small, then return -DER_TRUNC and set \"required_buf_size\" to the local pool\n * map buffer size.\n * If the map_buf_bulk is non-NULL, then the created local bulk handle for\n * pool_buf will be returned and caller needs to do crt_bulk_free later.\n * If the map_buf_bulk is NULL then the internally created local bulk handle\n * will be freed within this function.\n */\nstatic int\ntransfer_map_buf(struct pool_buf *map_buf, uint32_t map_version,\n\t\t struct pool_svc *svc, crt_rpc_t *rpc,\n\t\t crt_bulk_t remote_bulk, uint32_t *required_buf_size)\n{\n\tsize_t\t\t\tmap_buf_size;\n\tdaos_size_t\t\tremote_bulk_size;\n\td_iov_t\t\tmap_iov;\n\td_sg_list_t\t\tmap_sgl;\n\tcrt_bulk_t\t\tbulk = CRT_BULK_NULL;\n\tstruct crt_bulk_desc\tmap_desc;\n\tcrt_bulk_opid_t\t\tmap_opid;\n\tABT_eventual\t\teventual;\n\tint\t\t       *status;\n\tint\t\t\trc;\n\n\tif (map_version != pool_map_get_version(svc->ps_pool->sp_map)) {\n\t\tD_ERROR(DF_UUID\": found different cached and persistent pool \"\n\t\t\t\"map versions: cached=%u persistent=%u\\n\",\n\t\t\tDP_UUID(svc->ps_uuid),\n\t\t\tpool_map_get_version(svc->ps_pool->sp_map),\n\t\t\tmap_version);\n\t\tD_GOTO(out, rc = -DER_IO);\n\t}\n\n\tmap_buf_size = pool_buf_size(map_buf->pb_nr);\n\n\t/* Check if the client bulk buffer is large enough. */\n\trc = crt_bulk_get_len(remote_bulk, &remote_bulk_size);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\tif (remote_bulk_size < map_buf_size) {\n\t\tD_ERROR(DF_UUID\": remote pool map buffer (\"DF_U64\") < required \"\n\t\t\t\"(%lu)\\n\", DP_UUID(svc->ps_uuid), remote_bulk_size,\n\t\t\tmap_buf_size);\n\t\t*required_buf_size = map_buf_size;\n\t\tD_GOTO(out, rc = -DER_TRUNC);\n\t}\n\n\td_iov_set(&map_iov, map_buf, map_buf_size);\n\tmap_sgl.sg_nr = 1;\n\tmap_sgl.sg_nr_out = 0;\n\tmap_sgl.sg_iovs = &map_iov;\n\n\trc = crt_bulk_create(rpc->cr_ctx, &map_sgl, CRT_BULK_RO, &bulk);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/* Prepare \"map_desc\" for crt_bulk_transfer(). */\n\tmap_desc.bd_rpc = rpc;\n\tmap_desc.bd_bulk_op = CRT_BULK_PUT;\n\tmap_desc.bd_remote_hdl = remote_bulk;\n\tmap_desc.bd_remote_off = 0;\n\tmap_desc.bd_local_hdl = bulk;\n\tmap_desc.bd_local_off = 0;\n\tmap_desc.bd_len = map_iov.iov_len;\n\n\trc = ABT_eventual_create(sizeof(*status), &eventual);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_bulk, rc = dss_abterr2der(rc));\n\n\trc = crt_bulk_transfer(&map_desc, bulk_cb, &eventual, &map_opid);\n\tif (rc != 0)\n\t\tD_GOTO(out_eventual, rc);\n\n\trc = ABT_eventual_wait(eventual, (void **)&status);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_eventual, rc = dss_abterr2der(rc));\n\n\tif (*status != 0)\n\t\tD_GOTO(out_eventual, rc = *status);\n\nout_eventual:\n\tABT_eventual_free(&eventual);\nout_bulk:\n\tif (bulk != CRT_BULK_NULL)\n\t\tcrt_bulk_free(bulk);\nout:\n\treturn rc;\n}\n\nvoid\nds_pool_connect_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_connect_in\t       *in = crt_req_get(rpc);\n\tstruct pool_connect_out\t       *out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t       *svc;\n\tstruct pool_buf\t\t       *map_buf = NULL;\n\tuint32_t\t\t\tmap_version;\n\tuint32_t\t\t\tconnectable;\n\tstruct rdb_tx\t\t\ttx;\n\td_iov_t\t\t\t\tkey;\n\td_iov_t\t\t\t\tvalue;\n\tstruct pool_hdl\t\t\thdl;\n\tuint32_t\t\t\tnhandles;\n\tint\t\t\t\tskip_update = 0;\n\tint\t\t\t\trc;\n\tdaos_prop_t\t\t       *prop = NULL;\n\tuint64_t\t\t\tprop_bits;\n\tstruct daos_prop_entry\t       *acl_entry;\n\tstruct ownership\t\towner;\n\tstruct daos_prop_entry\t       *owner_entry;\n\tstruct daos_prop_entry\t       *owner_grp_entry;\n\tuint64_t\t\t\tsec_capas = 0;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->pci_op.pi_uuid), rpc, DP_UUID(in->pci_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->pci_op.pi_uuid, &svc,\n\t\t\t\t    &out->pco_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\tif (in->pci_query_bits & DAOS_PO_QUERY_REBUILD_STATUS) {\n\t\trc = ds_rebuild_query(in->pci_op.pi_uuid, &out->pco_rebuild_st);\n\t\tif (rc != 0)\n\t\t\tD_GOTO(out_svc, rc);\n\t}\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\t/* Check if pool is being destroyed and not accepting connections */\n\td_iov_set(&value, &connectable, sizeof(connectable));\n\trc = rdb_tx_lookup(&tx, &svc->ps_root,\n\t\t\t   &ds_pool_prop_connectable, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\tD_DEBUG(DF_DSMS, DF_UUID\": connectable=%u\\n\",\n\t\tDP_UUID(in->pci_op.pi_uuid), connectable);\n\tif (!connectable) {\n\t\tD_ERROR(DF_UUID\": being destroyed, not accepting connections\\n\",\n\t\t\tDP_UUID(in->pci_op.pi_uuid));\n\t\tD_GOTO(out_lock, rc = -DER_BUSY);\n\t}\n\n\t/* Check existing pool handles. */\n\td_iov_set(&key, in->pci_op.pi_hdl, sizeof(uuid_t));\n\td_iov_set(&value, &hdl, sizeof(hdl));\n\trc = rdb_tx_lookup(&tx, &svc->ps_handles, &key, &value);\n\tif (rc == 0) {\n\t\tif (hdl.ph_flags == in->pci_flags) {\n\t\t\t/*\n\t\t\t * The handle already exists; only do the pool map\n\t\t\t * transfer.\n\t\t\t */\n\t\t\tskip_update = 1;\n\t\t} else {\n\t\t\t/* The existing one does not match the new one. */\n\t\t\tD_ERROR(DF_UUID\": found conflicting pool handle\\n\",\n\t\t\t\tDP_UUID(in->pci_op.pi_uuid));\n\t\t\tD_GOTO(out_lock, rc = -DER_EXIST);\n\t\t}\n\t} else if (rc != -DER_NONEXIST) {\n\t\tD_GOTO(out_lock, rc);\n\t}\n\n\t/* Fetch properties, the  ACL and ownership info for access check,\n\t * all properties will update to IV.\n\t */\n\tprop_bits = DAOS_PO_QUERY_PROP_ALL;\n\trc = pool_prop_read(&tx, svc, prop_bits, &prop);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot get access data for pool, \"\n\t\t\t\"rc=\"DF_RC\"\\n\", DP_UUID(in->pci_op.pi_uuid), DP_RC(rc));\n\t\tD_GOTO(out_map_version, rc);\n\t}\n\tD_ASSERT(prop != NULL);\n\n\tacl_entry = daos_prop_entry_get(prop, DAOS_PROP_PO_ACL);\n\tD_ASSERT(acl_entry != NULL);\n\tD_ASSERT(acl_entry->dpe_val_ptr != NULL);\n\n\towner_entry = daos_prop_entry_get(prop, DAOS_PROP_PO_OWNER);\n\tD_ASSERT(owner_entry != NULL);\n\tD_ASSERT(owner_entry->dpe_str != NULL);\n\n\towner_grp_entry = daos_prop_entry_get(prop, DAOS_PROP_PO_OWNER_GROUP);\n\tD_ASSERT(owner_grp_entry != NULL);\n\tD_ASSERT(owner_grp_entry->dpe_str != NULL);\n\n\towner.user = owner_entry->dpe_str;\n\towner.group = owner_grp_entry->dpe_str;\n\n\t/*\n\t * Security capabilities determine the access control policy on this\n\t * pool handle.\n\t */\n\trc = ds_sec_pool_get_capabilities(in->pci_flags, &in->pci_cred, &owner,\n\t\t\t\t\t  acl_entry->dpe_val_ptr,\n\t\t\t\t\t  &sec_capas);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": refusing connect attempt for \"\n\t\t\tDF_X64\" error: \"DF_RC\"\\n\", DP_UUID(in->pci_op.pi_uuid),\n\t\t\tin->pci_flags, DP_RC(rc));\n\t\tD_GOTO(out_map_version, rc);\n\t}\n\n\tif (!ds_sec_pool_can_connect(sec_capas)) {\n\t\tD_ERROR(DF_UUID\": permission denied for connect attempt for \"\n\t\t\tDF_X64\"\\n\", DP_UUID(in->pci_op.pi_uuid),\n\t\t\tin->pci_flags);\n\t\tD_GOTO(out_map_version, rc = -DER_NO_PERM);\n\t}\n\n\t/*\n\t * Transfer the pool map to the client before adding the pool handle,\n\t * so that we don't need to worry about rolling back the transaction\n\t * when the transfer fails. The client has already been authenticated\n\t * and authorized at this point. If an error occurs after the transfer\n\t * completes, then we simply return the error and the client will throw\n\t * its pool_buf away.\n\t */\n\trc = read_map_buf(&tx, &svc->ps_root, &map_buf, &map_version);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to read pool map: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tD_GOTO(out_map_version, rc);\n\t}\n\trc = transfer_map_buf(map_buf, map_version, svc, rpc, in->pci_map_bulk,\n\t\t\t      &out->pco_map_buf_size);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_version, rc);\n\n\tif (skip_update)\n\t\tD_GOTO(out_map_version, rc = 0);\n\n\td_iov_set(&value, &nhandles, sizeof(nhandles));\n\trc = rdb_tx_lookup(&tx, &svc->ps_root, &ds_pool_prop_nhandles, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_version, rc);\n\n\t/* Take care of exclusive handles. */\n\tif (nhandles != 0) {\n\t\tif (in->pci_flags & DAOS_PC_EX) {\n\t\t\tD_DEBUG(DF_DSMS, DF_UUID\": others already connected\\n\",\n\t\t\t\tDP_UUID(in->pci_op.pi_uuid));\n\t\t\tD_GOTO(out_map_version, rc = -DER_BUSY);\n\t\t} else {\n\t\t\t/*\n\t\t\t * If there is a non-exclusive handle, then all handles\n\t\t\t * are non-exclusive.\n\t\t\t */\n\t\t\td_iov_set(&value, &hdl, sizeof(hdl));\n\t\t\trc = rdb_tx_fetch(&tx, &svc->ps_handles,\n\t\t\t\t\t  RDB_PROBE_FIRST, NULL /* key_in */,\n\t\t\t\t\t  NULL /* key_out */, &value);\n\t\t\tif (rc != 0)\n\t\t\t\tD_GOTO(out_map_version, rc);\n\t\t\tif (hdl.ph_flags & DAOS_PC_EX)\n\t\t\t\tD_GOTO(out_map_version, rc = -DER_BUSY);\n\t\t}\n\t}\n\n\trc = pool_connect_iv_dist(svc, in->pci_op.pi_hdl, in->pci_flags,\n\t\t\t\t  sec_capas, &in->pci_cred);\n\tif (rc == 0 && DAOS_FAIL_CHECK(DAOS_POOL_CONNECT_FAIL_CORPC))\n\t\trc = -DER_TIMEDOUT;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to connect to targets: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(in->pci_op.pi_uuid), DP_RC(rc));\n\t\tD_GOTO(out_map_version, rc);\n\t}\n\n\thdl.ph_flags = in->pci_flags;\n\thdl.ph_sec_capas = sec_capas;\n\tnhandles++;\n\td_iov_set(&key, in->pci_op.pi_hdl, sizeof(uuid_t));\n\td_iov_set(&value, &hdl, sizeof(hdl));\n\trc = rdb_tx_update(&tx, &svc->ps_handles, &key, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_version, rc);\n\n\td_iov_set(&value, &nhandles, sizeof(nhandles));\n\trc = rdb_tx_update(&tx, &svc->ps_root, &ds_pool_prop_nhandles, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_version, rc);\n\n\trc = rdb_tx_commit(&tx);\n\tif (rc)\n\t\tD_GOTO(out_map_version, rc);\n\n\tif (in->pci_query_bits & DAOS_PO_QUERY_SPACE)\n\t\trc = pool_space_query_bcast(rpc->cr_ctx, svc, in->pci_op.pi_hdl,\n\t\t\t\t\t    &out->pco_space);\nout_map_version:\n\tout->pco_op.po_map_version = pool_map_get_version(svc->ps_pool->sp_map);\n\tif (map_buf)\n\t\tD_FREE(map_buf);\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\tif (prop)\n\t\tdaos_prop_free(prop);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pco_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pco_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pci_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nstatic int\npool_disconnect_bcast(crt_context_t ctx, struct pool_svc *svc,\n\t\t      uuid_t *pool_hdls, int n_pool_hdls)\n{\n\tstruct pool_tgt_disconnect_in  *in;\n\tstruct pool_tgt_disconnect_out *out;\n\tcrt_rpc_t\t\t       *rpc;\n\tint\t\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": bcasting\\n\", DP_UUID(svc->ps_uuid));\n\n\trc = bcast_create(ctx, svc, POOL_TGT_DISCONNECT, NULL, &rpc);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->tdi_uuid, svc->ps_uuid);\n\tin->tdi_hdls.ca_arrays = pool_hdls;\n\tin->tdi_hdls.ca_count = n_pool_hdls;\n\trc = dss_rpc_send(rpc);\n\tif (rc == 0 && DAOS_FAIL_CHECK(DAOS_POOL_DISCONNECT_FAIL_CORPC))\n\t\trc = -DER_TIMEDOUT;\n\tif (rc != 0)\n\t\tD_GOTO(out_rpc, rc);\n\n\tout = crt_reply_get(rpc);\n\trc = out->tdo_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to disconnect from \"DF_RC\" targets\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\trc = -DER_IO;\n\t}\n\nout_rpc:\n\tcrt_req_decref(rpc);\nout:\n\tD_DEBUG(DF_DSMS, DF_UUID\": bcasted: \"DF_RC\"\\n\", DP_UUID(svc->ps_uuid),\n\t\tDP_RC(rc));\n\treturn rc;\n}\n\nstatic int\npool_disconnect_hdls(struct rdb_tx *tx, struct pool_svc *svc, uuid_t *hdl_uuids,\n\t\t     int n_hdl_uuids, crt_context_t ctx)\n{\n\td_iov_t\tvalue;\n\tuint32_t\tnhandles;\n\tint\t\ti;\n\tint\t\trc;\n\n\tD_ASSERTF(n_hdl_uuids > 0, \"%d\\n\", n_hdl_uuids);\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": disconnecting %d hdls: hdl_uuids[0]=\"DF_UUID\n\t\t\"\\n\", DP_UUID(svc->ps_uuid), n_hdl_uuids,\n\t\tDP_UUID(hdl_uuids[0]));\n\n\t/*\n\t * TODO: Send POOL_TGT_CLOSE_CONTS and somehow retry until every\n\t * container service has responded (through ds_pool).\n\t */\n\trc = ds_cont_close_by_pool_hdls(svc->ps_uuid, hdl_uuids, n_hdl_uuids,\n\t\t\t\t\tctx);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = pool_disconnect_bcast(ctx, svc, hdl_uuids, n_hdl_uuids);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\td_iov_set(&value, &nhandles, sizeof(nhandles));\n\trc = rdb_tx_lookup(tx, &svc->ps_root, &ds_pool_prop_nhandles, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\tnhandles -= n_hdl_uuids;\n\n\tfor (i = 0; i < n_hdl_uuids; i++) {\n\t\td_iov_t key;\n\n\t\td_iov_set(&key, hdl_uuids[i], sizeof(uuid_t));\n\t\trc = rdb_tx_delete(tx, &svc->ps_handles, &key);\n\t\tif (rc != 0)\n\t\t\tD_GOTO(out, rc);\n\t}\n\n\td_iov_set(&value, &nhandles, sizeof(nhandles));\n\trc = rdb_tx_update(tx, &svc->ps_root, &ds_pool_prop_nhandles, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nout:\n\tD_DEBUG(DF_DSMS, DF_UUID\": leaving: \"DF_RC\"\\n\", DP_UUID(svc->ps_uuid),\n\t\tDP_RC(rc));\n\treturn rc;\n}\n\nvoid\nds_pool_disconnect_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_disconnect_in      *pdi = crt_req_get(rpc);\n\tstruct pool_disconnect_out     *pdo = crt_reply_get(rpc);\n\tstruct pool_svc\t\t       *svc;\n\tstruct rdb_tx\t\t\ttx;\n\td_iov_t\t\t\tkey;\n\td_iov_t\t\t\tvalue;\n\tstruct pool_hdl\t\t\thdl;\n\tint\t\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(pdi->pdi_op.pi_uuid), rpc, DP_UUID(pdi->pdi_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(pdi->pdi_op.pi_uuid, &svc,\n\t\t\t\t    &pdo->pdo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\td_iov_set(&key, pdi->pdi_op.pi_hdl, sizeof(uuid_t));\n\td_iov_set(&value, &hdl, sizeof(hdl));\n\trc = rdb_tx_lookup(&tx, &svc->ps_handles, &key, &value);\n\tif (rc != 0) {\n\t\tif (rc == -DER_NONEXIST)\n\t\t\trc = 0;\n\t\tD_GOTO(out_lock, rc);\n\t}\n\n\trc = pool_disconnect_hdls(&tx, svc, &pdi->pdi_op.pi_hdl,\n\t\t\t\t  1 /* n_hdl_uuids */, rpc->cr_ctx);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\n\trc = rdb_tx_commit(&tx);\n\t/* No need to set pdo->pdo_op.po_map_version. */\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &pdo->pdo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tpdo->pdo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(pdi->pdi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nstatic int\npool_space_query_bcast(crt_context_t ctx, struct pool_svc *svc, uuid_t pool_hdl,\n\t\t       struct daos_pool_space *ps)\n{\n\tstruct pool_tgt_query_in\t*in;\n\tstruct pool_tgt_query_out\t*out;\n\tcrt_rpc_t\t\t\t*rpc;\n\tint\t\t\t\t rc;\n\n\tD_DEBUG(DB_MD, DF_UUID\": bcasting\\n\", DP_UUID(svc->ps_uuid));\n\n\trc = bcast_create(ctx, svc, POOL_TGT_QUERY, NULL, &rpc);\n\tif (rc != 0)\n\t\tgoto out;\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->tqi_op.pi_uuid, svc->ps_uuid);\n\tuuid_copy(in->tqi_op.pi_hdl, pool_hdl);\n\trc = dss_rpc_send(rpc);\n\tif (rc == 0 && DAOS_FAIL_CHECK(DAOS_POOL_QUERY_FAIL_CORPC))\n\t\trc = -DER_TIMEDOUT;\n\tif (rc != 0)\n\t\tgoto out_rpc;\n\n\tout = crt_reply_get(rpc);\n\trc = out->tqo_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to query from \"DF_RC\" targets\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\trc = -DER_IO;\n\t} else {\n\t\tD_ASSERT(ps != NULL);\n\t\t*ps = out->tqo_space;\n\t}\n\nout_rpc:\n\tcrt_req_decref(rpc);\nout:\n\tD_DEBUG(DB_MD, DF_UUID\": bcasted: \"DF_RC\"\\n\", DP_UUID(svc->ps_uuid),\n\t\tDP_RC(rc));\n\treturn rc;\n}\n\n/*\n * Transfer list of containers to \"remote_bulk\". If the remote bulk buffer\n * is too small, then return -DER_TRUNC. RPC response will contain the number\n * of containers in the pool that the client can use to resize its buffer\n * for another RPC request.\n */\nstatic int\ntransfer_cont_buf(struct daos_pool_cont_info *cont_buf, size_t ncont,\n\t\t  struct pool_svc *svc, crt_rpc_t *rpc, crt_bulk_t remote_bulk)\n{\n\tsize_t\t\t\t\t cont_buf_size;\n\tdaos_size_t\t\t\t remote_bulk_size;\n\td_iov_t\t\t\t\t cont_iov;\n\td_sg_list_t\t\t\t cont_sgl;\n\tcrt_bulk_t\t\t\t bulk = CRT_BULK_NULL;\n\tstruct crt_bulk_desc\t\t bulk_desc;\n\tcrt_bulk_opid_t\t\t\t bulk_opid;\n\tABT_eventual\t\t\t eventual;\n\tint\t\t\t\t*status;\n\tint\t\t\t\t rc;\n\n\tD_ASSERT(ncont > 0);\n\tcont_buf_size = ncont * sizeof(struct daos_pool_cont_info);\n\n\t/* Check if the client bulk buffer is large enough. */\n\trc = crt_bulk_get_len(remote_bulk, &remote_bulk_size);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\tif (remote_bulk_size < cont_buf_size) {\n\t\tD_ERROR(DF_UUID\": remote container buffer(\"DF_U64\")\"\n\t\t\t\" < required (%lu)\\n\", DP_UUID(svc->ps_uuid),\n\t\t\tremote_bulk_size, cont_buf_size);\n\t\tD_GOTO(out, rc = -DER_TRUNC);\n\t}\n\n\td_iov_set(&cont_iov, cont_buf, cont_buf_size);\n\tcont_sgl.sg_nr = 1;\n\tcont_sgl.sg_nr_out = 0;\n\tcont_sgl.sg_iovs = &cont_iov;\n\n\trc = crt_bulk_create(rpc->cr_ctx, &cont_sgl, CRT_BULK_RO, &bulk);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/* Prepare for crt_bulk_transfer(). */\n\tbulk_desc.bd_rpc = rpc;\n\tbulk_desc.bd_bulk_op = CRT_BULK_PUT;\n\tbulk_desc.bd_remote_hdl = remote_bulk;\n\tbulk_desc.bd_remote_off = 0;\n\tbulk_desc.bd_local_hdl = bulk;\n\tbulk_desc.bd_local_off = 0;\n\tbulk_desc.bd_len = cont_iov.iov_len;\n\n\trc = ABT_eventual_create(sizeof(*status), &eventual);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_bulk, rc = dss_abterr2der(rc));\n\n\trc = crt_bulk_transfer(&bulk_desc, bulk_cb, &eventual, &bulk_opid);\n\tif (rc != 0)\n\t\tD_GOTO(out_eventual, rc);\n\n\trc = ABT_eventual_wait(eventual, (void **)&status);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_eventual, rc = dss_abterr2der(rc));\n\n\tif (*status != 0)\n\t\tD_GOTO(out_eventual, rc = *status);\n\nout_eventual:\n\tABT_eventual_free(&eventual);\nout_bulk:\n\tif (bulk != CRT_BULK_NULL)\n\t\tcrt_bulk_free(bulk);\nout:\n\treturn rc;\n}\n\n/**\n * Send CaRT RPC to pool svc to get container list.\n *\n * \\param[in]\tuuid\t\tUUID of the pool\n * \\param[in]\tranks\t\tPool service replicas\n * \\param[out]\tcontainers\tArray of container information (allocated)\n * \\param[out]\tncontainers\tNumber of items in containers\n *\n * return\t0\t\tSuccess\n *\n */\nint\nds_pool_svc_list_cont(uuid_t uuid, d_rank_list_t *ranks,\n\t\t      struct daos_pool_cont_info **containers,\n\t\t      uint64_t *ncontainers)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_list_cont_in\t*in;\n\tstruct pool_list_cont_out\t*out;\n\tuint64_t\t\t\tresp_ncont = 1024;\n\tstruct daos_pool_cont_info\t*resp_cont = NULL;\n\n\tD_DEBUG(DB_MGMT, DF_UUID\": Getting container list\\n\", DP_UUID(uuid));\n\n\t*containers = NULL;\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\nrealloc_resp:\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_LIST_CONT, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool list cont rpc: %d\\n\",\n\t\t\tDP_UUID(uuid), rc);\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\t/* Allocate response buffer */\n\tD_ALLOC_ARRAY(resp_cont, resp_ncont);\n\tif (resp_cont == NULL)\n\t\tD_GOTO(out_rpc, rc = -DER_NOMEM);\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->plci_op.pi_uuid, uuid);\n\tuuid_clear(in->plci_op.pi_hdl);\n\tin->plci_ncont = resp_ncont;\n\trc = list_cont_bulk_create(info->dmi_ctx, &in->plci_cont_bulk,\n\t\t\t\t   resp_cont, in->plci_ncont);\n\tif (rc != 0)\n\t\tD_GOTO(out_resp_buf, rc);\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->plco_op.po_rc,\n\t\t\t\t      &out->plco_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\t/* To simplify logic, destroy bulk hdl and buffer each time */\n\t\tlist_cont_bulk_destroy(in->plci_cont_bulk);\n\t\tD_FREE(resp_cont);\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->plco_op.po_rc;\n\tif (rc == -DER_TRUNC) {\n\t\t/* resp_ncont too small - realloc with server-provided ncont */\n\t\tresp_ncont = out->plco_ncont;\n\t\tlist_cont_bulk_destroy(in->plci_cont_bulk);\n\t\tD_FREE(resp_cont);\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(realloc_resp, rc);\n\t} else if (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to get container list for pool: %d\\n\",\n\t\t\tDP_UUID(uuid), rc);\n\t} else {\n\t\t*containers = resp_cont;\n\t}\n\n\tlist_cont_bulk_destroy(in->plci_cont_bulk);\nout_resp_buf:\n\tif (rc != 0)\n\t\tD_FREE(resp_cont);\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\n/* CaRT RPC handler for pool container listing\n * Requires a pool handle (except for rebuild).\n */\nvoid\nds_pool_list_cont_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_list_cont_in\t*in = crt_req_get(rpc);\n\tstruct pool_list_cont_out\t*out = crt_reply_get(rpc);\n\tstruct daos_pool_cont_info\t*cont_buf = NULL;\n\tuint64_t\t\t\t ncont = 0;\n\tstruct pool_svc\t\t\t*svc;\n\tstruct rdb_tx\t\t\t tx;\n\td_iov_t\t\t\t\t key;\n\td_iov_t\t\t\t\t value;\n\tstruct pool_hdl\t\t\t hdl;\n\tint\t\t\t\t rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->plci_op.pi_uuid), rpc, DP_UUID(in->plci_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->plci_op.pi_uuid, &svc,\n\t\t\t\t    &out->plco_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/* Verify pool handle only if RPC initiated by a client\n\t * (not for mgmt svc to pool svc RPCs that do not have a handle).\n\t */\n\tif (daos_rpc_from_client(rpc)) {\n\t\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\t\tif (rc != 0)\n\t\t\tD_GOTO(out_svc, rc);\n\n\t\tABT_rwlock_rdlock(svc->ps_lock);\n\n\t\t/* Verify the pool handle. Note: since rebuild will not\n\t\t * connect the pool, so we only verify the non-rebuild\n\t\t * pool.\n\t\t */\n\t\tif (!is_pool_from_srv(in->plci_op.pi_uuid,\n\t\t\t\t      in->plci_op.pi_hdl)) {\n\t\t\td_iov_set(&key, in->plci_op.pi_hdl, sizeof(uuid_t));\n\t\t\td_iov_set(&value, &hdl, sizeof(hdl));\n\t\t\trc = rdb_tx_lookup(&tx, &svc->ps_handles, &key, &value);\n\t\t\tif (rc == -DER_NONEXIST)\n\t\t\t\trc = -DER_NO_HDL;\n\t\t\t\t/* defer goto out_svc until unlock/tx_end */\n\t\t}\n\n\t\tABT_rwlock_unlock(svc->ps_lock);\n\t\trdb_tx_end(&tx);\n\t\tif (rc != 0)\n\t\t\tD_GOTO(out_svc, rc);\n\t}\n\n\t/* Call container service to get the list */\n\trc = ds_cont_list(in->plci_op.pi_uuid, &cont_buf, &ncont);\n\tif (rc != 0) {\n\t\tD_GOTO(out_svc, rc);\n\t} else if ((in->plci_ncont > 0) && (ncont > in->plci_ncont)) {\n\t\t/* Got a list, but client buffer not supplied or too small */\n\t\tD_DEBUG(DF_DSMS, DF_UUID\": hdl=\"DF_UUID\": has %\"PRIu64\n\t\t\t\t \" containers (more than client: %\"PRIu64\")\\n\",\n\t\t\t\t DP_UUID(in->plci_op.pi_uuid),\n\t\t\t\t DP_UUID(in->plci_op.pi_hdl),\n\t\t\t\t ncont, in->plci_ncont);\n\t\tD_GOTO(out_free_cont_buf, rc = -DER_TRUNC);\n\t} else {\n\t\tD_DEBUG(DF_DSMS, DF_UUID\": hdl=\"DF_UUID\": has %\"PRIu64\n\t\t\t\t \" containers\\n\", DP_UUID(in->plci_op.pi_uuid),\n\t\t\t\t DP_UUID(in->plci_op.pi_hdl), ncont);\n\n\t\t/* Send any results only if client provided a handle */\n\t\tif (cont_buf && (in->plci_ncont > 0) &&\n\t\t    (in->plci_cont_bulk != CRT_BULK_NULL)) {\n\t\t\trc = transfer_cont_buf(cont_buf, ncont, svc, rpc,\n\t\t\t\t\t       in->plci_cont_bulk);\n\t\t}\n\t}\n\nout_free_cont_buf:\n\tif (cont_buf) {\n\t\tD_FREE(cont_buf);\n\t\tcont_buf = NULL;\n\t}\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->plco_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->plco_op.po_rc = rc;\n\tout->plco_ncont = ncont;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: %d\\n\",\n\t\tDP_UUID(in->plci_op.pi_uuid), rpc, rc);\n\tcrt_reply_send(rpc);\n}\n\nvoid\nds_pool_query_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_query_in   *in = crt_req_get(rpc);\n\tstruct pool_query_out  *out = crt_reply_get(rpc);\n\tdaos_prop_t\t       *prop = NULL;\n\tstruct pool_buf\t\t*map_buf = NULL;\n\tuint32_t\t\tmap_version;\n\tstruct pool_svc\t       *svc;\n\tstruct rdb_tx\t\ttx;\n\td_iov_t\t\t\tkey;\n\td_iov_t\t\t\tvalue;\n\tstruct pool_hdl\t\thdl;\n\tint\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->pqi_op.pi_uuid), rpc, DP_UUID(in->pqi_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->pqi_op.pi_uuid, &svc,\n\t\t\t\t    &out->pqo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\tif (in->pqi_query_bits & DAOS_PO_QUERY_REBUILD_STATUS) {\n\t\trc = ds_rebuild_query(in->pqi_op.pi_uuid, &out->pqo_rebuild_st);\n\t\tif (rc != 0)\n\t\t\tD_GOTO(out_svc, rc);\n\t}\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_rdlock(svc->ps_lock);\n\n\t/* Verify the pool handle for client calls.\n\t * Note: since rebuild will not connect the pool, so we only verify\n\t * the non-rebuild pool. Server-to-server calls also don't have a\n\t * handle.\n\t */\n\tif (daos_rpc_from_client(rpc) &&\n\t    !is_pool_from_srv(in->pqi_op.pi_uuid, in->pqi_op.pi_hdl)) {\n\t\td_iov_set(&key, in->pqi_op.pi_hdl, sizeof(uuid_t));\n\t\td_iov_set(&value, &hdl, sizeof(hdl));\n\t\trc = rdb_tx_lookup(&tx, &svc->ps_handles, &key, &value);\n\t\tif (rc != 0) {\n\t\t\tif (rc == -DER_NONEXIST)\n\t\t\t\trc = -DER_NO_HDL;\n\t\t\tD_GOTO(out_lock, rc);\n\t\t}\n\t}\n\n\t/* read optional properties */\n\trc = pool_prop_read(&tx, svc, in->pqi_query_bits, &prop);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_version, rc);\n\tout->pqo_prop = prop;\n\n\tif (DAOS_FAIL_CHECK(DAOS_FORCE_PROP_VERIFY) && prop != NULL) {\n\t\tdaos_prop_t\t\t*iv_prop = NULL;\n\t\tstruct daos_prop_entry\t*entry, *iv_entry;\n\t\tint\t\t\ti;\n\n\t\tD_ALLOC_PTR(iv_prop);\n\t\tif (iv_prop == NULL)\n\t\t\tD_GOTO(out_map_version, rc = -DER_NOMEM);\n\n\t\trc = ds_pool_iv_prop_fetch(svc->ps_pool, iv_prop);\n\t\tif (rc) {\n\t\t\tD_ERROR(\"ds_pool_iv_prop_fetch failed \"DF_RC\"\\n\",\n\t\t\t\tDP_RC(rc));\n\t\t\tdaos_prop_free(iv_prop);\n\t\t\tD_GOTO(out_map_version, rc);\n\t\t}\n\n\t\tfor (i = 0; i < prop->dpp_nr; i++) {\n\t\t\tentry = &prop->dpp_entries[i];\n\t\t\tiv_entry = daos_prop_entry_get(iv_prop,\n\t\t\t\t\t\t       entry->dpe_type);\n\t\t\tD_ASSERT(iv_entry != NULL);\n\t\t\tswitch (entry->dpe_type) {\n\t\t\tcase DAOS_PROP_PO_LABEL:\n\t\t\t\tD_ASSERT(strlen(entry->dpe_str) <=\n\t\t\t\t\t DAOS_PROP_LABEL_MAX_LEN);\n\t\t\t\tif (strncmp(entry->dpe_str, iv_entry->dpe_str,\n\t\t\t\t\t    DAOS_PROP_LABEL_MAX_LEN) != 0) {\n\t\t\t\t\tD_ERROR(\"mismatch %s - %s.\\n\",\n\t\t\t\t\t\tentry->dpe_str,\n\t\t\t\t\t\tiv_entry->dpe_str);\n\t\t\t\t\trc = -DER_IO;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase DAOS_PROP_PO_OWNER:\n\t\t\tcase DAOS_PROP_PO_OWNER_GROUP:\n\t\t\t\tD_ASSERT(strlen(entry->dpe_str) <=\n\t\t\t\t\t DAOS_ACL_MAX_PRINCIPAL_LEN);\n\t\t\t\tif (strncmp(entry->dpe_str, iv_entry->dpe_str,\n\t\t\t\t\t    DAOS_ACL_MAX_PRINCIPAL_BUF_LEN)\n\t\t\t\t    != 0) {\n\t\t\t\t\tD_ERROR(\"mismatch %s - %s.\\n\",\n\t\t\t\t\t\tentry->dpe_str,\n\t\t\t\t\t\tiv_entry->dpe_str);\n\t\t\t\t\trc = -DER_IO;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase DAOS_PROP_PO_SPACE_RB:\n\t\t\tcase DAOS_PROP_PO_SELF_HEAL:\n\t\t\tcase DAOS_PROP_PO_RECLAIM:\n\t\t\t\tif (entry->dpe_val != iv_entry->dpe_val) {\n\t\t\t\t\tD_ERROR(\"type %d mismatch \"DF_U64\" - \"\n\t\t\t\t\t\tDF_U64\".\\n\", entry->dpe_type,\n\t\t\t\t\t\tentry->dpe_val,\n\t\t\t\t\t\tiv_entry->dpe_val);\n\t\t\t\t\trc = -DER_IO;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase DAOS_PROP_PO_ACL:\n\t\t\t\tif (daos_prop_entry_cmp_acl(entry,\n\t\t\t\t\t\t\t    iv_entry) != 0)\n\t\t\t\t\trc = -DER_IO;\n\t\t\t\tbreak;\n\t\t\tcase DAOS_PROP_PO_SVC_LIST:\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tD_ASSERTF(0, \"bad dpe_type %d\\n\",\n\t\t\t\t\t  entry->dpe_type);\n\t\t\t\tbreak;\n\t\t\t};\n\t\t}\n\t\tdaos_prop_free(iv_prop);\n\t\tif (rc) {\n\t\t\tD_ERROR(\"iv_prop verify failed \"DF_RC\"\\n\", DP_RC(rc));\n\t\t\tD_GOTO(out_map_version, rc);\n\t\t}\n\t}\n\n\trc = read_map_buf(&tx, &svc->ps_root, &map_buf, &map_version);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to read pool map: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tD_GOTO(out_map_version, rc);\n\t}\n\n\trc = transfer_map_buf(map_buf, map_version, svc, rpc, in->pqi_map_bulk,\n\t\t\t      &out->pqo_map_buf_size);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_version, rc);\n\nout_map_version:\n\tout->pqo_op.po_map_version = pool_map_get_version(svc->ps_pool->sp_map);\n\tif (map_buf)\n\t\tD_FREE(map_buf);\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pqo_op.po_hint);\n\t/* See comment above, rebuild doesn't connect the pool */\n\tif (rc == 0 && (in->pqi_query_bits & DAOS_PO_QUERY_SPACE) &&\n\t    !is_pool_from_srv(in->pqi_op.pi_uuid, in->pqi_op.pi_hdl))\n\t\trc = pool_space_query_bcast(rpc->cr_ctx, svc, in->pqi_op.pi_hdl,\n\t\t\t\t\t    &out->pqo_space);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pqo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pqi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\n/* Convert pool_comp_state_t to daos_target_state_t */\nstatic daos_target_state_t\nenum_pool_comp_state_to_tgt_state(int tgt_state)\n{\n\n\tswitch (tgt_state) {\n\tcase PO_COMP_ST_UNKNOWN: return DAOS_TS_UNKNOWN;\n\tcase PO_COMP_ST_NEW: return DAOS_TS_NEW;\n\tcase PO_COMP_ST_UP: return DAOS_TS_UP;\n\tcase PO_COMP_ST_UPIN: return DAOS_TS_UP_IN;\n\tcase PO_COMP_ST_DOWN: return  DAOS_TS_DOWN;\n\tcase PO_COMP_ST_DOWNOUT: return DAOS_TS_DOWN_OUT;\n\tcase PO_COMP_ST_DRAIN: return DAOS_TS_DRAIN;\n\t}\n\n\treturn DAOS_TS_UNKNOWN;\n}\n\nvoid\nds_pool_query_info_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_query_info_in\t*in = crt_req_get(rpc);\n\tstruct pool_query_info_out\t*out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t\t*svc;\n\tstruct pool_target\t\t*target = NULL;\n\tint\t\t\t\t tgt_state;\n\tint\t\t\t\t rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->pqii_op.pi_uuid), rpc, DP_UUID(in->pqii_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->pqii_op.pi_uuid, &svc,\n\t\t\t\t    &out->pqio_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/* get the target state from pool map */\n\tABT_rwlock_rdlock(svc->ps_pool->sp_lock);\n\trc = pool_map_find_target_by_rank_idx(svc->ps_pool->sp_map,\n\t\t\t\t\t      in->pqii_rank,\n\t\t\t\t\t      in->pqii_tgt,\n\t\t\t\t\t      &target);\n\tif (rc != 1) {\n\t\tD_ERROR(DF_UUID\": Failed to get rank:%u, idx:%d\\n, rc:%d\",\n\t\t\tDP_UUID(in->pqii_op.pi_uuid), in->pqii_rank,\n\t\t\tin->pqii_tgt, rc);\n\t\tD_GOTO(out, rc = -DER_NONEXIST);\n\t } else {\n\t\trc = 0;\n\t}\n\n\tD_ASSERT(target != NULL);\n\n\ttgt_state = target->ta_comp.co_status;\n\tout->pqio_state = enum_pool_comp_state_to_tgt_state(tgt_state);\n\n\t/**\n\t * TODO (DAOS-3625): Send pool tgt query RPC (server->server) to\n\t * return pool target space info (including fragmentation).\n\t */\n\n\tABT_rwlock_unlock(svc->ps_pool->sp_lock);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pqio_op.po_rc = rc;\n\tout->pqio_rank = in->pqii_rank;\n\tout->pqio_tgt = in->pqii_tgt;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pqii_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nstatic int\nprocess_query_result(daos_pool_info_t *info, uuid_t pool_uuid,\n\t\t     uint32_t map_version, uint32_t leader_rank,\n\t\t     struct daos_pool_space *ps,\n\t\t     struct daos_rebuild_status *rs,\n\t\t     struct pool_buf *map_buf)\n{\n\tstruct pool_map\t       *map;\n\tint\t\t\trc;\n\tunsigned int\t\tnum_disabled = 0;\n\n\trc = pool_map_create(map_buf, map_version, &map);\n\tif (rc != 0) {\n\t\tD_ERROR(\"failed to create local pool map: %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\trc = pool_map_find_failed_tgts(map, NULL, &num_disabled);\n\tif (rc != 0) {\n\t\tD_ERROR(\"failed to get num disabled tgts, rc=%d\\n\", rc);\n\t\tD_GOTO(out, rc);\n\t}\n\n\tinfo->pi_ndisabled = num_disabled;\n\n\tpool_query_reply_to_info(pool_uuid, map_buf, map_version, leader_rank,\n\t\t\t\t ps, rs, info);\n\nout:\n\tpool_map_decref(map);\n\treturn rc;\n}\n\n/**\n * Query the pool without holding a pool handle.\n *\n * \\param[in]\tpool_uuid\tUUID of the pool\n * \\param[in]\tranks\t\tRanks of pool svc replicas\n * \\param[out]\tpool_info\tResults of the pool query\n *\n * \\return\t0\t\tSuccess\n *\t\t-DER_INVAL\tInvalid input\n *\t\tNegative value\tError\n */\nint\nds_pool_svc_query(uuid_t pool_uuid, d_rank_list_t *ranks,\n\t\t  daos_pool_info_t *pool_info)\n{\n\tint\t\t\trc;\n\tstruct rsvc_client\tclient;\n\tcrt_endpoint_t\t\tep;\n\tstruct dss_module_info\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t*rpc;\n\tstruct pool_query_in\t*in;\n\tstruct pool_query_out\t*out;\n\tstruct pool_buf\t\t*map_buf;\n\tuint32_t\t\tmap_size = 0;\n\n\tif (ranks == NULL || pool_info == NULL)\n\t\tD_GOTO(out, rc = -DER_INVAL);\n\n\tD_DEBUG(DB_MGMT, DF_UUID\": Querying pool\\n\", DP_UUID(pool_uuid));\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\nrealloc:\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_QUERY, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool query rpc: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pqi_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->pqi_op.pi_hdl);\n\tin->pqi_query_bits = pool_query_bits(pool_info, NULL);\n\n\trc = map_bulk_create(info->dmi_ctx, &in->pqi_map_bulk, &map_buf,\n\t\t\t     map_size);\n\tif (rc != 0)\n\t\tD_GOTO(out_rpc, rc);\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->pqo_op.po_rc,\n\t\t\t\t      &out->pqo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tmap_bulk_destroy(in->pqi_map_bulk, map_buf);\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->pqo_op.po_rc;\n\tif (rc == -DER_TRUNC) {\n\t\tmap_size = out->pqo_map_buf_size;\n\t\tmap_bulk_destroy(in->pqi_map_bulk, map_buf);\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(realloc, rc);\n\t} else if (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to query pool: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_bulk, rc);\n\t}\n\n\tD_DEBUG(DB_MGMT, \"Successfully queried pool\\n\");\n\n\trc = process_query_result(pool_info, pool_uuid,\n\t\t\t\t\t out->pqo_op.po_map_version,\n\t\t\t\t\t out->pqo_op.po_hint.sh_rank,\n\t\t\t\t\t &out->pqo_space,\n\t\t\t\t\t &out->pqo_rebuild_st,\n\t\t\t\t\t map_buf);\n\tif (rc != 0)\n\t\tD_ERROR(\"Failed to process pool query results, rc=%d\\n\", rc);\n\nout_bulk:\n\tmap_bulk_destroy(in->pqi_map_bulk, map_buf);\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\n/**\n * Query a pool's properties without having a handle for the pool\n */\nvoid\nds_pool_prop_get_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_prop_get_in\t\t*in = crt_req_get(rpc);\n\tstruct pool_prop_get_out\t*out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t\t*svc;\n\tstruct rdb_tx\t\t\ttx;\n\tint\t\t\t\trc;\n\tdaos_prop_t\t\t\t*prop = NULL;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->pgi_op.pi_uuid), rpc);\n\n\trc = pool_svc_lookup_leader(in->pgi_op.pi_uuid, &svc,\n\t\t\t\t    &out->pgo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_rdlock(svc->ps_lock);\n\n\trc = pool_prop_read(&tx, svc, in->pgi_query_bits, &prop);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\tout->pgo_prop = prop;\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pgo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pgo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pgi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n\tdaos_prop_free(prop);\n}\n\n/**\n * Send a CaRT message to the pool svc to get the ACL pool property.\n *\n * \\param[in]\t\tpool_uuid\tUUID of the pool\n * \\param[in]\t\tranks\t\tPool service replicas\n * \\param[in][out]\tprop\t\tProp with requested properties, to be\n *\t\t\t\t\tfilled out and returned.\n *\n * \\return\t0\t\tSuccess\n *\n */\nint\nds_pool_svc_get_prop(uuid_t pool_uuid, d_rank_list_t *ranks,\n\t\t     daos_prop_t *prop)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_prop_get_in\t\t*in;\n\tstruct pool_prop_get_out\t*out;\n\n\tD_DEBUG(DB_MGMT, DF_UUID\": Getting prop\\n\", DP_UUID(pool_uuid));\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_PROP_GET, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool get prop rpc: \"\n\t\t\t\"\"DF_RC\"\\n\", DP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pgi_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->pgi_op.pi_hdl);\n\tin->pgi_query_bits = pool_query_bits(NULL, prop);\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->pgo_op.po_rc,\n\t\t\t\t      &out->pgo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->pgo_op.po_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to get prop for pool: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out_rpc, rc);\n\t}\n\n\trc = daos_prop_copy(prop, out->pgo_prop);\n\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\nint\nds_pool_extend(uuid_t pool_uuid, int ntargets, uuid_t target_uuids[],\n\t       const d_rank_list_t *rank_list, int ndomains,\n\t       const int *domains, d_rank_list_t *svc_ranks)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_extend_in\t\t*in;\n\tstruct pool_extend_out\t\t*out;\n\n\trc = rsvc_client_init(&client, svc_ranks);\n\tif (rc != 0)\n\t\treturn rc;\n\nrechoose:\n\n\tep.ep_grp = NULL; /* primary group */\n\trsvc_client_choose(&client, &ep);\n\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_EXTEND, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool extend rpc: \"\n\t\t\t\"\"DF_RC\"\\n\", DP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pei_op.pi_uuid, pool_uuid);\n\tin->pei_ntgts = ntargets;\n\tin->pei_ndomains = ndomains;\n\tin->pei_tgt_uuids.ca_count = rank_list->rl_nr;\n\tin->pei_tgt_uuids.ca_arrays = target_uuids;\n\tin->pei_tgt_ranks = (d_rank_list_t *)rank_list;\n\tin->pei_domains.ca_count = ndomains;\n\tin->pei_domains.ca_arrays = (int *)domains;\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\tout->peo_op.po_rc, &out->peo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->peo_op.po_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": Failed to set targets to UP state for \"\n\t\t\t\t\"reintegration: \"DF_RC\"\\n\", DP_UUID(pool_uuid),\n\t\t\t\tDP_RC(rc));\n\t\tD_GOTO(out_rpc, rc);\n\t}\n\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\n\treturn rc;\n}\n\nint\nds_pool_target_update_state(uuid_t pool_uuid, d_rank_list_t *ranks,\n\t\tuint32_t rank, struct pool_target_id_list *target_list,\n\t\tpool_comp_state_t state)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_target_addr_list\tlist;\n\tstruct pool_add_in\t\t*in;\n\tstruct pool_add_out\t\t*out;\n\tcrt_opcode_t\t\t\topcode;\n\tint i = 0;\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\treturn rc;\n\nrechoose:\n\n\tep.ep_grp = NULL; /* primary group */\n\trsvc_client_choose(&client, &ep);\n\n\tswitch (state) {\n\tcase PO_COMP_ST_DOWN:\n\t\topcode = POOL_EXCLUDE;\n\t\tbreak;\n\tcase PO_COMP_ST_UP:\n\t\topcode = POOL_REINT;\n\t\tbreak;\n\tcase PO_COMP_ST_DRAIN:\n\t\topcode = POOL_DRAIN;\n\t\tbreak;\n\tdefault:\n\t\tD_GOTO(out_client, rc = -DER_INVAL);\n\t}\n\n\trc = pool_req_create(info->dmi_ctx, &ep, opcode, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool req: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pti_op.pi_uuid, pool_uuid);\n\n\trc = pool_target_addr_list_alloc(target_list->pti_number, &list);\n\tif (rc) {\n\t\tD_ERROR(DF_UUID\": pool_target_addr_list_alloc failed, rc %d.\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_rpc, rc);\n\t}\n\n\t/* pool_update rpc requires an addr list. */\n\tfor (i = 0; i < target_list->pti_number; i++) {\n\t\tlist.pta_addrs[i].pta_target = target_list->pti_ids[i].pti_id;\n\t\tlist.pta_addrs[i].pta_rank = rank;\n\t}\n\n\tin->pti_addr_list.ca_arrays = list.pta_addrs;\n\tin->pti_addr_list.ca_count = (size_t)list.pta_number;\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\tout->pto_op.po_rc, &out->pto_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->pto_op.po_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": Failed to set targets to %s state: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid),\n\t\t\tstate == PO_COMP_ST_DOWN ? \"DOWN\" :\n\t\t\tstate == PO_COMP_ST_UP ? \"UP\" : \"UNKNOWN\",\n\t\t\tDP_RC(rc));\n\t\tD_GOTO(out_rpc, rc);\n\t}\n\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\n\treturn rc;\n}\n\n/**\n * Set a pool's properties without having a handle for the pool\n */\nvoid\nds_pool_prop_set_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_prop_set_in\t\t*in = crt_req_get(rpc);\n\tstruct pool_prop_set_out\t*out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t\t*svc;\n\tstruct rdb_tx\t\t\ttx;\n\tdaos_prop_t\t\t\t*prop = NULL;\n\tint\t\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->psi_op.pi_uuid), rpc);\n\n\trc = pool_svc_lookup_leader(in->psi_op.pi_uuid, &svc,\n\t\t\t\t    &out->pso_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\trc = pool_prop_write(&tx, &svc->ps_root, in->psi_prop);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to write prop for pool: %d\\n\",\n\t\t\tDP_UUID(in->psi_op.pi_uuid), rc);\n\t\tD_GOTO(out_lock, rc);\n\t}\n\n\trc = rdb_tx_commit(&tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\n\t/* Read all props & update prop IV */\n\trc = pool_prop_read(&tx, svc, DAOS_PO_QUERY_PROP_ALL, &prop);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to read prop for pool, rc=%d\\n\",\n\t\t\tDP_UUID(in->psi_op.pi_uuid), rc);\n\t\tD_GOTO(out_lock, rc);\n\t}\n\tD_ASSERT(prop != NULL);\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\t/*\n\t * TODO: Introduce prop version to avoid inconsistent prop over targets\n\t *\t caused by the out of order IV sync.\n\t */\n\tif (!rc && prop != NULL) {\n\t\trc = ds_pool_iv_prop_update(svc->ps_pool, prop);\n\t\tif (rc)\n\t\t\tD_ERROR(DF_UUID\": failed to update prop IV for pool, \"\n\t\t\t\t\"%d.\\n\", DP_UUID(in->psi_op.pi_uuid), rc);\n\t}\n\tdaos_prop_free(prop);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pso_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pso_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: %d\\n\",\n\t\tDP_UUID(in->psi_op.pi_uuid), rpc, rc);\n\tcrt_reply_send(rpc);\n}\n\n/**\n * Send a CaRT message to the pool svc to set the requested pool properties.\n *\n * \\param[in]\tpool_uuid\tUUID of the pool\n * \\param[in]\tranks\t\tPool service replicas\n * \\param[in]\tprop\t\tPool prop\n *\n * \\return\t0\t\tSuccess\n *\n */\nint\nds_pool_svc_set_prop(uuid_t pool_uuid, d_rank_list_t *ranks, daos_prop_t *prop)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_prop_set_in\t\t*in;\n\tstruct pool_prop_set_out\t*out;\n\n\tD_DEBUG(DB_MGMT, DF_UUID\": Setting pool prop\\n\", DP_UUID(pool_uuid));\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to init rsvc client: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tD_GOTO(out, rc);\n\t}\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_PROP_SET, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool set prop rpc: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->psi_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->psi_op.pi_hdl);\n\tin->psi_prop = prop;\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->pso_op.po_rc,\n\t\t\t\t      &out->pso_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->pso_op.po_rc;\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to set prop for pool: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_rpc, rc);\n\t}\n\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\n/*\n * Adds the contents of new_acl to the original ACL. If an entry is added for\n * a principal already in the ACL, the old entry will be replaced.\n * *acl may be reallocated in the process.\n */\nstatic int\nmerge_acl(struct daos_acl **acl, struct daos_acl *new_acl)\n{\n\tstruct daos_ace\t*new_ace;\n\tint\t\trc = 0;\n\n\tnew_ace = daos_acl_get_next_ace(new_acl, NULL);\n\twhile (new_ace != NULL) {\n\t\trc = daos_acl_add_ace(acl, new_ace);\n\t\tif (rc != 0)\n\t\t\tbreak;\n\t\tnew_ace = daos_acl_get_next_ace(new_acl, new_ace);\n\t}\n\n\treturn rc;\n}\n\n/**\n * Update entries in a pool's ACL without having a handle for the pool\n */\nvoid\nds_pool_acl_update_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_acl_update_in\t*in = crt_req_get(rpc);\n\tstruct pool_acl_update_out\t*out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t\t*svc;\n\tstruct rdb_tx\t\t\ttx;\n\tint\t\t\t\trc;\n\tdaos_prop_t\t\t\t*prop = NULL;\n\tstruct daos_prop_entry\t\t*entry = NULL;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->pui_op.pi_uuid), rpc);\n\n\trc = pool_svc_lookup_leader(in->pui_op.pi_uuid, &svc,\n\t\t\t\t    &out->puo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\t/*\n\t * We need to read the old ACL, modify, and rewrite it\n\t */\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\trc = pool_prop_read(&tx, svc, DAOS_PO_QUERY_PROP_ACL, &prop);\n\tif (rc != 0)\n\t\t/* Prop might be allocated and returned even if rc != 0 */\n\t\tD_GOTO(out_prop, rc);\n\n\tentry = daos_prop_entry_get(prop, DAOS_PROP_PO_ACL);\n\tif (entry == NULL) {\n\t\tD_ERROR(DF_UUID\": No ACL prop entry for pool\\n\",\n\t\t\tDP_UUID(in->pui_op.pi_uuid));\n\t\tD_GOTO(out_prop, rc);\n\t}\n\n\trc = merge_acl((struct daos_acl **)&entry->dpe_val_ptr, in->pui_acl);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": Unable to update pool with new ACL, rc=%d\\n\",\n\t\t\tDP_UUID(in->pui_op.pi_uuid), rc);\n\t\tD_GOTO(out_prop, rc);\n\t}\n\n\trc = pool_prop_write(&tx, &svc->ps_root, prop);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to write updated ACL for pool: %d\\n\",\n\t\t\tDP_UUID(in->pui_op.pi_uuid), rc);\n\t\tD_GOTO(out_prop, rc);\n\t}\n\n\trc = rdb_tx_commit(&tx);\n\nout_prop:\n\tif (prop != NULL)\n\t\tdaos_prop_free(prop);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->puo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->puo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: %d\\n\",\n\t\tDP_UUID(in->pui_op.pi_uuid), rpc, rc);\n\tcrt_reply_send(rpc);\n}\n\n/**\n * Send a CaRT message to the pool svc to update the pool ACL by adding and\n * updating entries.\n *\n * \\param[in]\tpool_uuid\tUUID of the pool\n * \\param[in]\tranks\t\tPool service replicas\n * \\param[in]\tacl\t\tACL to merge with the current pool ACL\n *\n * \\return\t0\t\tSuccess\n *\n */\nint\nds_pool_svc_update_acl(uuid_t pool_uuid, d_rank_list_t *ranks,\n\t\t       struct daos_acl *acl)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_acl_update_in\t*in;\n\tstruct pool_acl_update_out\t*out;\n\n\tD_DEBUG(DB_MGMT, DF_UUID\": Updating pool ACL\\n\", DP_UUID(pool_uuid));\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_ACL_UPDATE, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool update ACL rpc: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pui_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->pui_op.pi_hdl);\n\tin->pui_acl = acl;\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->puo_op.po_rc,\n\t\t\t\t      &out->puo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->puo_op.po_rc;\n\tif (rc != 0)\n\t\tD_ERROR(DF_UUID\": failed to update ACL for pool: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\n/**\n * Delete entries in a pool's ACL without having a handle for the pool\n */\nvoid\nds_pool_acl_delete_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_acl_delete_in\t*in = crt_req_get(rpc);\n\tstruct pool_acl_delete_out\t*out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t\t*svc;\n\tstruct rdb_tx\t\t\ttx;\n\tint\t\t\t\trc;\n\tdaos_prop_t\t\t\t*prop = NULL;\n\tstruct daos_prop_entry\t\t*entry;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->pdi_op.pi_uuid), rpc);\n\n\trc = pool_svc_lookup_leader(in->pdi_op.pi_uuid, &svc,\n\t\t\t\t    &out->pdo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\t/*\n\t * We need to read the old ACL, modify, and rewrite it\n\t */\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\trc = pool_prop_read(&tx, svc, DAOS_PO_QUERY_PROP_ACL, &prop);\n\tif (rc != 0)\n\t\t/* Prop might be allocated and returned even if rc != 0 */\n\t\tD_GOTO(out_prop, rc);\n\n\tentry = daos_prop_entry_get(prop, DAOS_PROP_PO_ACL);\n\tif (entry == NULL) {\n\t\tD_ERROR(DF_UUID\": No ACL prop entry for pool\\n\",\n\t\t\tDP_UUID(in->pdi_op.pi_uuid));\n\t\tD_GOTO(out_prop, rc);\n\t}\n\n\trc = daos_acl_remove_ace((struct daos_acl **)&entry->dpe_val_ptr,\n\t\t\t\t in->pdi_type, in->pdi_principal);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": Failed to remove requested principal, \"\n\t\t\t\"rc=%d\\n\", DP_UUID(in->pdi_op.pi_uuid), rc);\n\t\tD_GOTO(out_prop, rc);\n\t}\n\n\trc = pool_prop_write(&tx, &svc->ps_root, prop);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to write updated ACL for pool: %d\\n\",\n\t\t\tDP_UUID(in->pdi_op.pi_uuid), rc);\n\t\tD_GOTO(out_prop, rc);\n\t}\n\n\trc = rdb_tx_commit(&tx);\n\nout_prop:\n\tif (prop != NULL)\n\t\tdaos_prop_free(prop);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pdo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pdo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: %d\\n\",\n\t\tDP_UUID(in->pdi_op.pi_uuid), rpc, rc);\n\tcrt_reply_send(rpc);\n}\n\n/**\n * Send a CaRT message to the pool svc to remove an entry by principal from the\n * pool's ACL.\n *\n * \\param[in]\tpool_uuid\tUUID of the pool\n * \\param[in]\tranks\t\tPool service replicas\n * \\param[in]\tprincipal_type\tType of the principal to be removed\n * \\param[in]\tprincipal_name\tName of the principal to be removed\n *\n * \\return\t0\t\tSuccess\n *\n */\nint\nds_pool_svc_delete_acl(uuid_t pool_uuid, d_rank_list_t *ranks,\n\t\t       enum daos_acl_principal_type principal_type,\n\t\t       const char *principal_name)\n{\n\tint\t\t\t\trc;\n\tstruct rsvc_client\t\tclient;\n\tcrt_endpoint_t\t\t\tep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_acl_delete_in\t*in;\n\tstruct pool_acl_delete_out\t*out;\n\tchar\t\t\t\t*name_buf = NULL;\n\tsize_t\t\t\t\tname_buf_len;\n\n\tD_DEBUG(DB_MGMT, DF_UUID\": Deleting entry from pool ACL\\n\",\n\t\tDP_UUID(pool_uuid));\n\n\tif (principal_name != NULL) {\n\t\t/* Need to sanitize the incoming string */\n\t\tname_buf_len = DAOS_ACL_MAX_PRINCIPAL_BUF_LEN;\n\t\tD_ALLOC_ARRAY(name_buf, name_buf_len);\n\t\tif (name_buf == NULL)\n\t\t\tD_GOTO(out, rc = -DER_NOMEM);\n\t\t/* force null terminator in copy */\n\t\tstrncpy(name_buf, principal_name, name_buf_len - 1);\n\t}\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_ACL_DELETE, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool delete ACL rpc: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pdi_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->pdi_op.pi_hdl);\n\tin->pdi_type = (uint8_t)principal_type;\n\tin->pdi_principal = name_buf;\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->pdo_op.po_rc,\n\t\t\t\t      &out->pdo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->pdo_op.po_rc;\n\tif (rc != 0)\n\t\tD_ERROR(DF_UUID\": failed to delete ACL entry for pool: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\tD_FREE(name_buf);\n\treturn rc;\n}\n\nstatic int\nreplace_failed_replicas(struct pool_svc *svc, struct pool_map *map)\n{\n\td_rank_list_t\t*old, *new, *current, failed, replacement;\n\tint              rc;\n\n\trc = rdb_get_ranks(svc->ps_rsvc.s_db, &current);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = daos_rank_list_dup(&old, current);\n\tif (rc != 0)\n\t\tgoto out_cur;\n\n\trc = ds_pool_check_failed_replicas(map, current, &failed, &replacement);\n\tif (rc != 0) {\n\t\tD_DEBUG(DB_MD, DF_UUID\": cannot replace failed replicas: \"\n\t\t\t\"\"DF_RC\"\\n\", DP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tgoto out_old;\n\t}\n\n\tif (failed.rl_nr < 1)\n\t\tgoto out_old;\n\tif (replacement.rl_nr > 0)\n\t\tds_rsvc_add_replicas_s(&svc->ps_rsvc, &replacement,\n\t\t\t\t       ds_rsvc_get_md_cap());\n\tds_rsvc_remove_replicas_s(&svc->ps_rsvc, &failed);\n\t/** `replacement.rl_ranks` is not allocated and shouldn't be freed **/\n\tD_FREE(failed.rl_ranks);\n\n\tif (rdb_get_ranks(svc->ps_rsvc.s_db, &new) == 0) {\n\t\tdaos_rank_list_sort(current);\n\t\tdaos_rank_list_sort(old);\n\t\tdaos_rank_list_sort(new);\n\n\t\tif (!daos_rank_list_identical(current, new)) {\n\t\t\tD_DEBUG(DB_MD, DF_UUID\": failed to update replicas\\n\",\n\t\t\t\tDP_UUID(svc->ps_uuid));\n\t\t} else if (!daos_rank_list_identical(new, old)) {\n\t\t\t/*\n\t\t\t * Send RAS event to control-plane over dRPC to indicate\n\t\t\t * change in pool service replicas.\n\t\t\t */\n\t\t\trc = ds_notify_pool_svc_update(&svc->ps_uuid, new);\n\t\t\tif (rc != 0)\n\t\t\t\tD_DEBUG(DB_MD, DF_UUID\": replica update notify \"\n\t\t\t\t\t\"failure: \"DF_RC\"\\n\",\n\t\t\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\t}\n\n\t\td_rank_list_free(new);\n\t}\n\nout_old:\n\td_rank_list_free(old);\nout_cur:\n\td_rank_list_free(current);\nout:\n\treturn rc;\n}\n\n/* Callers are responsible for d_rank_list_free(*replicasp). */\nstatic int\nds_pool_update_internal(uuid_t pool_uuid, struct pool_target_id_list *tgts,\n\t\t\tunsigned int opc, struct rsvc_hint *hint,\n\t\t\tbool *p_updated, bool evict_rank,\n\t\t\tuint32_t *map_version_p, uint32_t *tgt_map_ver)\n{\n\tstruct pool_svc\t       *svc;\n\tstruct rdb_tx\t\ttx;\n\tstruct pool_map\t       *map = NULL;\n\tuint32_t\t\tmap_version_before;\n\tuint32_t\t\tmap_version = 0;\n\tstruct pool_buf\t       *map_buf = NULL;\n\tbool\t\t\tupdated = false;\n\tint\t\t\trc;\n\n\trc = pool_svc_lookup_leader(pool_uuid, &svc, hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\t/* Create a temporary pool map based on the last committed version. */\n\trc = read_map(&tx, &svc->ps_root, &map);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\n\t/*\n\t * Attempt to modify the temporary pool map and save its versions\n\t * before and after. If the version hasn't changed, we are done.\n\t */\n\tmap_version_before = pool_map_get_version(map);\n\trc = ds_pool_map_tgts_update(map, tgts, opc, evict_rank, tgt_map_ver,\n\t\t\t\t     true);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\n\tmap_version = pool_map_get_version(map);\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": version=%u->%u\\n\",\n\t\tDP_UUID(svc->ps_uuid), map_version_before, map_version);\n\tif (map_version == map_version_before)\n\t\tD_GOTO(out_map, rc = 0);\n\n\t/* Write the new pool map. */\n\trc = pool_buf_extract(map, &map_buf);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\trc = write_map_buf(&tx, &svc->ps_root, map_buf, map_version);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\n\trc = rdb_tx_commit(&tx);\n\tif (rc != 0) {\n\t\tD_DEBUG(DB_MD, DF_UUID\": failed to commit: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tD_GOTO(out_map, rc);\n\t}\n\n\tupdated = true;\n\n\t/* Update svc->ps_pool to match the new pool map. */\n\trc = ds_pool_tgt_map_update(svc->ps_pool, map_buf, map_version);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to update pool map cache: %d\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), rc);\n\t\t/*\n\t\t * We must resign to avoid handling future requests with a\n\t\t * stale pool map cache.\n\t\t */\n\t\trdb_resign(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term);\n\t\trc = 0;\n\t\tgoto out_map;\n\t}\n\n\tds_rsvc_request_map_dist(&svc->ps_rsvc);\n\n\treplace_failed_replicas(svc, map);\n\n\tif (opc == POOL_ADD_IN) {\n\t\t/*\n\t\t * If we are setting ranks from UP -> UPIN, schedule a reclaim\n\t\t * operation to garbage collect any unreachable data moved\n\t\t * during reintegration/addition\n\t\t */\n\t\trc = ds_rebuild_schedule(svc->ps_pool, map_version, tgts,\n\t\t\t\t\t RB_OP_RECLAIM);\n\t\tif (rc != 0) {\n\t\t\tD_ERROR(\"failed to schedule reclaim rc: \"DF_RC\"\\n\",\n\t\t\t\tDP_RC(rc));\n\t\t\tD_GOTO(out, rc);\n\t\t}\n\t}\n\nout_map:\n\tif (map_version_p != NULL)\n\t\t*map_version_p = pool_map_get_version((map == NULL || rc != 0) ?\n\t\t\t\t\t\t      svc->ps_pool->sp_map :\n\t\t\t\t\t\t      map);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\tif (map)\n\t\tpool_map_decref(map);\n\n\tif (map_buf != NULL)\n\t\tpool_buf_free(map_buf);\nout_svc:\n\tif (hint != NULL)\n\t\tds_rsvc_set_hint(&svc->ps_rsvc, hint);\n\tpool_svc_put_leader(svc);\nout:\n\tif (p_updated)\n\t\t*p_updated = updated;\n\treturn rc;\n}\n\nstatic int\npool_find_all_targets_by_addr(uuid_t pool_uuid,\n\t\t\t      struct pool_target_addr_list *list,\n\t\t\t      struct pool_target_id_list *tgt_list,\n\t\t\t      struct pool_target_addr_list *out_list,\n\t\t\t      struct rsvc_hint *hint)\n{\n\tstruct pool_svc\t*svc;\n\tstruct rdb_tx\ttx;\n\tstruct pool_map *map = NULL;\n\tint\t\ti;\n\tint\t\trc;\n\n\trc = pool_svc_lookup_leader(pool_uuid, &svc, hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\tABT_rwlock_rdlock(svc->ps_lock);\n\n\t/* Create a temporary pool map based on the last committed version. */\n\trc = read_map(&tx, &svc->ps_root, &map);\n\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tfor (i = 0; i < list->pta_number; i++) {\n\t\tstruct pool_target *tgt;\n\t\tint tgt_nr;\n\t\tint j;\n\t\tint ret;\n\n\t\ttgt_nr = pool_map_find_target_by_rank_idx(map,\n\t\t\t\tlist->pta_addrs[i].pta_rank,\n\t\t\t\tlist->pta_addrs[i].pta_target, &tgt);\n\t\tif (tgt_nr <= 0) {\n\t\t\t/* Can not locate the target in pool map, let's\n\t\t\t * add it to the output list\n\t\t\t */\n\t\t\tD_WARN(\"Can not find %u/%d , add to out_list\\n\",\n\t\t\t\tlist->pta_addrs[i].pta_rank,\n\t\t\t\t(int)list->pta_addrs[i].pta_target);\n\t\t\tret = pool_target_addr_list_append(out_list,\n\t\t\t\t\t\t&list->pta_addrs[i]);\n\t\t\tif (ret) {\n\t\t\t\trc = ret;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tfor (j = 0; j < tgt_nr; j++) {\n\t\t\tstruct pool_target_id tid;\n\n\t\t\ttid.pti_id = tgt[j].ta_comp.co_id;\n\t\t\tret = pool_target_id_list_append(tgt_list, &tid);\n\t\t\tif (ret) {\n\t\t\t\trc = ret;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\nout_svc:\n\tpool_svc_put_leader(svc);\nout:\n\tif (map != NULL)\n\t\tpool_map_decref(map);\n\treturn rc;\n}\n\nstruct redist_open_hdls_arg {\n\t/**\n\t * Pointer to pointer containing flattened array of output handles\n\t * Note that these are variable size, so can't be indexed as an array\n\t */\n\tstruct pool_iv_conn **hdls;\n\t/** Pointer to the next write location within hdls */\n\tstruct pool_iv_conn *next;\n\t/** Total current size of the hdls buffer, in bytes */\n\tsize_t hdls_size;\n\t/** Total used space in hdls buffer, in bytes */\n\tsize_t hdls_used;\n};\n\nstatic int\nget_open_handles_cb(daos_handle_t ih, d_iov_t *key, d_iov_t *val, void *varg)\n{\n\tstruct redist_open_hdls_arg *arg = varg;\n\tuuid_t *uuid = key->iov_buf;\n\tstruct pool_hdl *hdl = val->iov_buf;\n\tstruct ds_pool_hdl *lookup_hdl;\n\tsize_t size_needed;\n\tint rc = DER_SUCCESS;\n\n\tif (key->iov_len != sizeof(uuid_t) ||\n\t    val->iov_len != sizeof(struct pool_hdl)) {\n\t\tD_ERROR(\"invalid key/value size: key=\"DF_U64\" value=\"DF_U64\"\\n\",\n\t\t\tkey->iov_len, val->iov_len);\n\t\treturn -DER_IO;\n\t}\n\n\t/* Look up the handle in the local pool to obtain the creds, which are\n\t * not stored in RDB\n\t */\n\tlookup_hdl = ds_pool_hdl_lookup(*uuid);\n\tif (lookup_hdl == NULL) {\n\t\tD_ERROR(\"Pool open handle \"DF_UUID\" is in RDB but not the pool\",\n\t\t\tDP_UUID(*uuid));\n\t\treturn -DER_NONEXIST;\n\t}\n\n\t/* Check if there's enough room is the preallocated array, and expand\n\t * if not\n\t */\n\tsize_needed = arg->hdls_used + lookup_hdl->sph_cred.iov_buf_len +\n\t\tsizeof(struct pool_iv_conn);\n\tif (size_needed > arg->hdls_size) {\n\t\tvoid *newbuf = NULL;\n\n\t\tD_REALLOC(newbuf, *arg->hdls, size_needed);\n\t\tif (newbuf == NULL)\n\t\t\tD_GOTO(out_hdl, rc = -DER_NOMEM);\n\n\t\t/* Since this probably changed the hdls pointer, adjust the\n\t\t * next pointer correspondingly\n\t\t */\n\t\t*(arg->hdls) = newbuf;\n\t\targ->next = (struct pool_iv_conn *)\n\t\t\t(((char *)*arg->hdls) + arg->hdls_used);\n\t\targ->hdls_size = size_needed;\n\t}\n\n\t/* Copy the data */\n\tuuid_copy(arg->next->pic_hdl, *uuid);\n\targ->next->pic_flags = hdl->ph_flags;\n\targ->next->pic_capas = hdl->ph_sec_capas;\n\targ->next->pic_cred_size = lookup_hdl->sph_cred.iov_buf_len;\n\tmemcpy(arg->next->pic_creds, lookup_hdl->sph_cred.iov_buf,\n\t       lookup_hdl->sph_cred.iov_buf_len);\n\n\t/* Adjust the pointers for the next iteration */\n\targ->hdls_used = size_needed;\n\targ->next = (struct pool_iv_conn *)\n\t\t(((char *)*arg->hdls) + arg->hdls_used);\n\nout_hdl:\n\tds_pool_hdl_put(lookup_hdl);\n\n\treturn DER_SUCCESS;\n}\n\n/**\n * Retrieves a flat buffer containing all currently open handles\n *\n * \\param pool_uuid [IN]  The pool to get handles for\n * \\param hdls      [OUT] A flat-packed buffer of all open handles\n *                        (struct pool_iv_conn). Caller must free hdls->iov_buf.\n *                        Note that these are variable size, and can not be\n *                        indexed like an array\n * \\param out_size  [OUT] The size of the buffer pointed to by hdls\n *\n * \\return If no handles are currently open this will return DER_SUCCESS with\n *         hdls = NULL and out_size = 0.\n *         Otherwise returns DER_SUCCESS or an -error code\n */\nint\nds_pool_get_open_handles(uuid_t pool_uuid, d_iov_t *hdls)\n{\n\tstruct pool_svc\t\t\t*svc;\n\tstruct redist_open_hdls_arg\t arg;\n\tuint32_t\t\t\t connectable;\n\tstruct rdb_tx\t\t\t tx;\n\td_iov_t\t\t\t\t value;\n\tuint32_t\t\t\t nhandles;\n\tint\t\t\t\t rc;\n\n\td_iov_set(hdls, NULL, 0);\n\n\trc = pool_svc_lookup_leader(pool_uuid, &svc, NULL /* hint */);\n\tif (rc != 0)\n\t\treturn rc;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_rdlock(svc->ps_lock);\n\n\t/* Check if pool is being destroyed and not accepting connections */\n\td_iov_set(&value, &connectable, sizeof(connectable));\n\trc = rdb_tx_lookup(&tx, &svc->ps_root,\n\t\t\t   &ds_pool_prop_connectable, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\tif (!connectable) {\n\t\tD_ERROR(DF_UUID\": being destroyed, not accepting connections\\n\",\n\t\t\tDP_UUID(pool_uuid));\n\t\tD_GOTO(out_lock, rc = -DER_BUSY);\n\t}\n\n\t/* Check how many handles are currently open */\n\td_iov_set(&value, &nhandles, sizeof(nhandles));\n\trc = rdb_tx_lookup(&tx, &svc->ps_root, &ds_pool_prop_nhandles, &value);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\n\t/* Abort early if there are no open handles */\n\tif (nhandles == 0)\n\t\tD_GOTO(out_lock, rc);\n\n\t/* Preallocate an approximate amount of space for the handles and the\n\t * variable-size creds field which is not accounted for in the size of\n\t * the base structure. The goal here isn't to be exactly right - can't\n\t * know at this point how much space is actually needed. Ballparking\n\t * close enough just reduces the number of reallocations needed during\n\t * iteration\n\t */\n\tD_ALLOC(hdls->iov_buf, nhandles * (sizeof(struct pool_iv_conn) + 160));\n\tif (hdls->iov_buf == NULL)\n\t\tD_GOTO(out_lock, rc = -DER_NOMEM);\n\n\t/* Pass in the preallocated array and handles as pointers\n\t * This allows the iterator to reallocate the array if an element\n\t * was added between when we retrieved the size and iteration completes\n\t */\n\targ.hdls = (struct pool_iv_conn **)&hdls->iov_buf;\n\targ.next = *arg.hdls;\n\targ.hdls_size = nhandles * (sizeof(struct pool_iv_conn) + 128);\n\targ.hdls_used = 0;\n\n\t/* Iterate the open handles and accumulate their UUIDs */\n\trc = rdb_tx_iterate(&tx, &svc->ps_handles, false /* backward */,\n\t\t\t    get_open_handles_cb, &arg);\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\n\thdls->iov_buf_len = hdls->iov_len = arg.hdls_used;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": packed %u handles into %zu bytes\\n\",\n\t\tDP_UUID(pool_uuid), nhandles, arg.hdls_used);\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\n\nout_svc:\n\tpool_svc_put_leader(svc);\n\treturn rc;\n}\n\nint\nds_pool_tgt_exclude_out(uuid_t pool_uuid, struct pool_target_id_list *list)\n{\n\treturn ds_pool_update_internal(pool_uuid, list, POOL_EXCLUDE_OUT,\n\t\t\t\t       NULL, NULL, false, NULL, NULL);\n}\n\nint\nds_pool_tgt_exclude(uuid_t pool_uuid, struct pool_target_id_list *list)\n{\n\treturn ds_pool_update_internal(pool_uuid, list, POOL_EXCLUDE,\n\t\t\t\t       NULL, NULL, false, NULL, NULL);\n}\n\nint\nds_pool_tgt_add_in(uuid_t pool_uuid, struct pool_target_id_list *list)\n{\n\treturn ds_pool_update_internal(pool_uuid, list, POOL_ADD_IN,\n\t\t\t\t       NULL, NULL, false, NULL, NULL);\n}\n\n/*\n * Perform a pool map update indicated by opc. If successful, the new pool map\n * version is reported via map_version. Upon -DER_NOTLEADER, a pool service\n * leader hint, if available, is reported via hint (if not NULL).\n */\nstatic int\nds_pool_update(uuid_t pool_uuid, crt_opcode_t opc,\n\t       struct pool_target_addr_list *list,\n\t       struct pool_target_addr_list *out_list,\n\t       uint32_t *map_version, struct rsvc_hint *hint, bool evict_rank)\n{\n\tdaos_rebuild_opc_t\t\top;\n\tstruct pool_target_id_list\ttarget_list = { 0 };\n\tstruct ds_pool\t\t\t*pool = NULL;\n\tdaos_prop_t\t\t\tprop = { 0 };\n\tuint32_t\t\t\ttgt_map_ver = 0;\n\tstruct daos_prop_entry\t\t*entry;\n\tbool\t\t\t\tupdated;\n\tint\t\t\t\trc;\n\tchar\t\t\t\t*env;\n\n\trc = pool_find_all_targets_by_addr(pool_uuid, list, &target_list,\n\t\t\t\t\t   out_list, hint);\n\tif (rc)\n\t\tD_GOTO(out, rc);\n\n\t/* Update target by target id */\n\trc = ds_pool_update_internal(pool_uuid, &target_list, opc, hint,\n\t\t\t\t     &updated, evict_rank, map_version,\n\t\t\t\t     &tgt_map_ver);\n\tif (rc)\n\t\tD_GOTO(out, rc);\n\n\tif (!updated)\n\t\tD_GOTO(out, rc);\n\n\tswitch (opc) {\n\tcase POOL_EXCLUDE:\n\t\top = RB_OP_FAIL;\n\t\tbreak;\n\tcase POOL_DRAIN:\n\t\top = RB_OP_DRAIN;\n\t\tbreak;\n\tcase POOL_REINT:\n\t\top = RB_OP_REINT;\n\t\tbreak;\n\tcase POOL_EXTEND:\n\t\top = RB_OP_EXTEND;\n\t\tbreak;\n\tdefault:\n\t\tD_GOTO(out, rc);\n\t}\n\n\tenv = getenv(REBUILD_ENV);\n\tif ((env && !strcasecmp(env, REBUILD_ENV_DISABLED)) ||\n\t     daos_fail_check(DAOS_REBUILD_DISABLE)) {\n\t\tD_DEBUG(DB_TRACE, \"Rebuild is disabled\\n\");\n\t\tD_GOTO(out, rc = 0);\n\t}\n\n\tpool = ds_pool_lookup(pool_uuid);\n\tD_ASSERT(pool != NULL);\n\trc = ds_pool_iv_prop_fetch(pool, &prop);\n\tif (rc)\n\t\tD_GOTO(out, rc);\n\n\tentry = daos_prop_entry_get(&prop, DAOS_PROP_PO_SELF_HEAL);\n\tD_ASSERT(entry != NULL);\n\tif (!(entry->dpe_val & DAOS_SELF_HEAL_AUTO_REBUILD)) {\n\t\tD_DEBUG(DB_MGMT, \"self healing is disabled\\n\");\n\t\tD_GOTO(out, rc);\n\t}\n\n\tD_DEBUG(DF_DSMS, \"map ver %u/%u\\n\", map_version ? *map_version : -1,\n\t\ttgt_map_ver);\n\tif (tgt_map_ver != 0) {\n\t\trc = ds_rebuild_schedule(pool, tgt_map_ver, &target_list, op);\n\t\tif (rc != 0) {\n\t\t\tD_ERROR(\"rebuild fails rc: \"DF_RC\"\\n\", DP_RC(rc));\n\t\t\tD_GOTO(out, rc);\n\t\t}\n\t}\n\nout:\n\tif (pool)\n\t\tds_pool_put(pool);\n\tdaos_prop_fini(&prop);\n\tpool_target_id_list_free(&target_list);\n\treturn rc;\n}\n\n/*\n * Currently can only add racks/top level domains. There's not currently\n * any way to specify fault domain at a better level\n */\nstatic int\npool_extend_map(struct rdb_tx *tx, struct pool_svc *svc,\n\t\tuint32_t nnodes, uuid_t target_uuids[],\n\t\td_rank_list_t *rank_list, uint32_t ndomains,\n\t\tint32_t *domains, bool *updated_p, uint32_t *map_version_p,\n\t\tstruct rsvc_hint *hint)\n{\n\tstruct pool_buf\t\t*map_buf = NULL;\n\tstruct pool_map\t\t*map = NULL;\n\tuint32_t\t\tmap_version;\n\tbool\t\t\tupdated = false;\n\tint\t\t\tntargets;\n\tint\t\t\trc;\n\n\tntargets = nnodes * dss_tgt_nr;\n\n\t/* Create a temporary pool map based on the last committed version. */\n\trc = read_map(tx, &svc->ps_root, &map);\n\tif (rc != 0)\n\t\treturn rc;\n\n\tmap_version = pool_map_get_version(map) + 1;\n\n\trc = gen_pool_buf(map, &map_buf, map_version, ndomains, nnodes,\n\t\t\tntargets, domains, target_uuids, rank_list, NULL,\n\t\t\tdss_tgt_nr);\n\tif (rc != 0)\n\t\tD_GOTO(out_map_buf, rc);\n\n\t/* Extend the current pool map */\n\trc = pool_map_extend(map, map_version, map_buf);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\n\t/* Write the new pool map. */\n\trc = pool_buf_extract(map, &map_buf);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\n\trc = write_map_buf(tx, &svc->ps_root, map_buf, map_version);\n\tif (rc != 0)\n\t\tD_GOTO(out_map, rc);\n\n\trc = rdb_tx_commit(tx);\n\tif (rc != 0) {\n\t\tD_DEBUG(DB_MD, DF_UUID\": failed to commit: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\t\tD_GOTO(out_map, rc);\n\t}\n\n\tupdated = true;\n\t/* Update svc->ps_pool to match the new pool map. */\n\trc = ds_pool_tgt_map_update(svc->ps_pool, map_buf, map_version);\n\tif (rc != 0) {\n\t\t/*\n\t\t* We must resign to avoid handling future requests with a\n\t\t* stale pool map cache.\n\t\t*/\n\t\trdb_resign(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term);\n\t\trc = 0;\n\t\tgoto out_map;\n\t}\n\n\tds_rsvc_request_map_dist(&svc->ps_rsvc);\n\nout_map:\n\tif (map_version_p != NULL)\n\t\t*map_version_p = pool_map_get_version((map == NULL || rc != 0) ?\n\t\t\t\t\t\t      svc->ps_pool->sp_map :\n\t\t\t\t\t\t      map);\n\trdb_tx_end(tx);\n\nout_map_buf:\n\tif (map_buf != NULL)\n\t\tpool_buf_free(map_buf);\n\tif (updated_p)\n\t\t*updated_p = updated;\n\tif (map)\n\t\tpool_map_decref(map);\n\n\treturn rc;\n}\n\nstatic int\npool_extend_internal(uuid_t pool_uuid, struct rsvc_hint *hint,\n\t\t     uint32_t nnodes,\n\t\t     uuid_t target_uuids[], d_rank_list_t *rank_list,\n\t\t     uint32_t ndomains, int32_t *domains,\n\t\t     uint32_t *map_version_p)\n{\n\tstruct pool_svc\t\t*svc;\n\tstruct rdb_tx\t\ttx;\n\tbool\t\t\tupdated = false;\n\tstruct pool_target_id_list tgts = { 0 };\n\tint rc;\n\n\trc = pool_svc_lookup_leader(pool_uuid, &svc, hint);\n\tif (rc != 0)\n\t\treturn rc;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\t/*\n\t * Extend the pool map directly - this is more complicated than other\n\t * operations which are handled within ds_pool_update()\n\t */\n\trc = pool_extend_map(&tx, svc, ndomains, target_uuids,\n\t\t\t     rank_list, ndomains, domains,\n\t\t\t     &updated, map_version_p, hint);\n\n\tif (!updated)\n\t\tD_GOTO(out_lock, rc);\n\n\t/* Get a list of all the targets being added */\n\trc = pool_map_find_targets_on_ranks(svc->ps_pool->sp_map, rank_list,\n\t\t\t\t\t    &tgts);\n\tif (rc <= 0) {\n\t\tD_ERROR(\"failed to schedule extend rc: \"DF_RC\"\\n\", DP_RC(rc));\n\t\tD_GOTO(out_lock, rc);\n\t}\n\n\t/* Schedule an extension rebuild for those targets */\n\trc = ds_rebuild_schedule(svc->ps_pool, *map_version_p, &tgts,\n\t\t\t\t RB_OP_EXTEND);\n\tif (rc != 0) {\n\t\tD_ERROR(\"failed to schedule extend rc: \"DF_RC\"\\n\", DP_RC(rc));\n\t\tD_GOTO(out_lock, rc);\n\t}\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\nout_svc:\n\tpool_target_id_list_free(&tgts);\n\tif (hint != NULL)\n\t\tds_rsvc_set_hint(&svc->ps_rsvc, hint);\n\tpool_svc_put_leader(svc);\n\treturn rc;\n}\n\nvoid\nds_pool_extend_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_extend_in\t*in = crt_req_get(rpc);\n\tstruct pool_extend_out\t*out = crt_reply_get(rpc);\n\tuuid_t\t\t\tpool_uuid;\n\tuuid_t\t\t\t*target_uuids;\n\td_rank_list_t\t\trank_list;\n\tuint32_t\t\tndomains;\n\tint32_t\t\t\t*domains;\n\tint rc;\n\n\tuuid_copy(pool_uuid, in->pei_op.pi_uuid);\n\ttarget_uuids = in->pei_tgt_uuids.ca_arrays;\n\trank_list.rl_nr = in->pei_tgt_ranks->rl_nr;\n\trank_list.rl_ranks = in->pei_tgt_ranks->rl_ranks;\n\tndomains = in->pei_ndomains;\n\tdomains = in->pei_domains.ca_arrays;\n\n\trc = pool_extend_internal(pool_uuid, &out->peo_op.po_hint, ndomains,\n\t\t\t\t  target_uuids, &rank_list, ndomains, domains,\n\t\t\t\t  &out->peo_op.po_map_version);\n\n\tout->peo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pei_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nvoid\nds_pool_update_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_tgt_update_in\t*in = crt_req_get(rpc);\n\tstruct pool_tgt_update_out\t*out = crt_reply_get(rpc);\n\tstruct pool_target_addr_list\tlist = { 0 };\n\tstruct pool_target_addr_list\tout_list = { 0 };\n\tint\t\t\t\trc;\n\n\tif (in->pti_addr_list.ca_arrays == NULL ||\n\t    in->pti_addr_list.ca_count == 0)\n\t\tD_GOTO(out, rc = -DER_INVAL);\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: ntargets=%zu\\n\",\n\t\tDP_UUID(in->pti_op.pi_uuid), rpc, in->pti_addr_list.ca_count);\n\n\tlist.pta_number = in->pti_addr_list.ca_count;\n\tlist.pta_addrs = in->pti_addr_list.ca_arrays;\n\trc = ds_pool_update(in->pti_op.pi_uuid, opc_get(rpc->cr_opc), &list,\n\t\t\t    &out_list, &out->pto_op.po_map_version,\n\t\t\t    &out->pto_op.po_hint, false);\n\tif (rc)\n\t\tD_GOTO(out, rc);\n\n\tout->pto_addr_list.ca_arrays = out_list.pta_addrs;\n\tout->pto_addr_list.ca_count = out_list.pta_number;\n\nout:\n\tout->pto_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pti_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n\tpool_target_addr_list_free(&out_list);\n}\n\nint\nds_pool_evict_rank(uuid_t pool_uuid, d_rank_t rank)\n{\n\tstruct pool_target_addr_list\tlist;\n\tstruct pool_target_addr_list\tout_list = { 0 };\n\tstruct pool_target_addr\t\ttgt_rank;\n\tuint32_t\t\t\tmap_version = 0;\n\tint\t\t\t\trc;\n\n\ttgt_rank.pta_rank = rank;\n\ttgt_rank.pta_target = -1;\n\tlist.pta_number = 1;\n\tlist.pta_addrs = &tgt_rank;\n\n\trc = ds_pool_update(pool_uuid, POOL_EXCLUDE, &list, &out_list,\n\t\t\t    &map_version, NULL, true);\n\n\tD_DEBUG(DB_MGMT, \"Exclude pool \"DF_UUID\"/%u rank %u: rc %d\\n\",\n\t\tDP_UUID(pool_uuid), map_version, rank, rc);\n\n\tpool_target_addr_list_free(&out_list);\n\n\treturn rc;\n}\n\nstruct evict_iter_arg {\n\tuuid_t *eia_hdl_uuids;\n\tsize_t\teia_hdl_uuids_size;\n\tint\teia_n_hdl_uuids;\n};\n\nstatic int\nevict_iter_cb(daos_handle_t ih, d_iov_t *key, d_iov_t *val, void *varg)\n{\n\tstruct evict_iter_arg  *arg = varg;\n\n\tD_ASSERT(arg->eia_hdl_uuids != NULL);\n\tD_ASSERT(arg->eia_hdl_uuids_size > sizeof(uuid_t));\n\n\tif (key->iov_len != sizeof(uuid_t) ||\n\t    val->iov_len != sizeof(struct pool_hdl)) {\n\t\tD_ERROR(\"invalid key/value size: key=\"DF_U64\" value=\"DF_U64\"\\n\",\n\t\t\tkey->iov_len, val->iov_len);\n\t\treturn -DER_IO;\n\t}\n\n\t/*\n\t * Make sure arg->eia_hdl_uuids[arg->eia_hdl_uuids_size] have enough\n\t * space for this handle.\n\t */\n\tif (sizeof(uuid_t) * (arg->eia_n_hdl_uuids + 1) >\n\t    arg->eia_hdl_uuids_size) {\n\t\tuuid_t *hdl_uuids_tmp;\n\t\tsize_t\thdl_uuids_size_tmp;\n\n\t\thdl_uuids_size_tmp = arg->eia_hdl_uuids_size * 2;\n\t\tD_ALLOC(hdl_uuids_tmp, hdl_uuids_size_tmp);\n\t\tif (hdl_uuids_tmp == NULL)\n\t\t\treturn -DER_NOMEM;\n\t\tmemcpy(hdl_uuids_tmp, arg->eia_hdl_uuids,\n\t\t       arg->eia_hdl_uuids_size);\n\t\tD_FREE(arg->eia_hdl_uuids);\n\t\targ->eia_hdl_uuids = hdl_uuids_tmp;\n\t\targ->eia_hdl_uuids_size = hdl_uuids_size_tmp;\n\t}\n\n\tuuid_copy(arg->eia_hdl_uuids[arg->eia_n_hdl_uuids], key->iov_buf);\n\targ->eia_n_hdl_uuids++;\n\treturn 0;\n}\n\n/*\n * Callers are responsible for freeing *hdl_uuids if this function returns zero.\n */\nstatic int\nfind_hdls_to_evict(struct rdb_tx *tx, struct pool_svc *svc, uuid_t **hdl_uuids,\n\t\t   size_t *hdl_uuids_size, int *n_hdl_uuids)\n{\n\tstruct evict_iter_arg\targ;\n\tint\t\t\trc;\n\n\targ.eia_hdl_uuids_size = sizeof(uuid_t) * 4;\n\tD_ALLOC(arg.eia_hdl_uuids, arg.eia_hdl_uuids_size);\n\tif (arg.eia_hdl_uuids == NULL)\n\t\treturn -DER_NOMEM;\n\targ.eia_n_hdl_uuids = 0;\n\n\trc = rdb_tx_iterate(tx, &svc->ps_handles, false /* backward */,\n\t\t\t    evict_iter_cb, &arg);\n\tif (rc != 0) {\n\t\tD_FREE(arg.eia_hdl_uuids);\n\t\treturn rc;\n\t}\n\n\t*hdl_uuids = arg.eia_hdl_uuids;\n\t*hdl_uuids_size = arg.eia_hdl_uuids_size;\n\t*n_hdl_uuids = arg.eia_n_hdl_uuids;\n\treturn 0;\n}\n\n/*\n * Callers are responsible for freeing *hdl_uuids if this function returns zero.\n */\nstatic int\nvalidate_hdls_to_evict(struct rdb_tx *tx, struct pool_svc *svc,\n\t\t       uuid_t **hdl_uuids, int *n_hdl_uuids, uuid_t *hdl_list,\n\t\t       int n_hdl_list) {\n\tuuid_t\t\t*valid_list;\n\tint\t\tn_valid_list = 0;\n\tint\t\ti;\n\tint\t\trc = 0;\n\td_iov_t\t\tkey;\n\td_iov_t\t\tvalue;\n\tstruct pool_hdl\thdl;\n\n\tif (hdl_list == NULL || n_hdl_list == 0) {\n\t\treturn -DER_INVAL;\n\t}\n\n\t/* Assume the entire list is valid */\n\tD_ALLOC(valid_list, sizeof(uuid_t) * n_hdl_list);\n\tif (valid_list == NULL)\n\t\treturn -DER_NOMEM;\n\n\tfor (i = 0; i < n_hdl_list; i++) {\n\t\td_iov_set(&key, hdl_list[i], sizeof(uuid_t));\n\t\td_iov_set(&value, &hdl, sizeof(hdl));\n\t\trc = rdb_tx_lookup(tx, &svc->ps_handles, &key, &value);\n\n\t\tif (rc == 0) {\n\t\t\tuuid_copy(valid_list[n_valid_list], hdl_list[i]);\n\t\t\tn_valid_list++;\n\t\t} else if (rc == -DER_NONEXIST) {\n\t\t\tD_DEBUG(DF_DSMS, \"Skipping invalid handle\" DF_UUID \"\\n\",\n\t\t\t\tDP_UUID(hdl_list[i]));\n\t\t\t/* Reset RC in case we're the last entry */\n\t\t\trc = 0;\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tD_FREE(valid_list);\n\t\t\tD_GOTO(out, rc);\n\t\t}\n\t}\n\n\t*hdl_uuids = valid_list;\n\t*n_hdl_uuids = n_valid_list;\n\nout:\n\treturn rc;\n}\n\nvoid\nds_pool_evict_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_evict_in   *in = crt_req_get(rpc);\n\tstruct pool_evict_out  *out = crt_reply_get(rpc);\n\tstruct pool_svc\t       *svc;\n\tstruct rdb_tx\t\ttx;\n\tuuid_t\t\t       *hdl_uuids = NULL;\n\tsize_t\t\t\thdl_uuids_size;\n\tint\t\t\tn_hdl_uuids = 0;\n\tint\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->pvi_op.pi_uuid), rpc);\n\n\trc = pool_svc_lookup_leader(in->pvi_op.pi_uuid, &svc,\n\t\t\t\t    &out->pvo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_wrlock(svc->ps_lock);\n\n\t/*\n\t * If a subset of handles is specified use them instead of iterating\n\t * through all handles for the pool uuid\n\t */\n\tif (in->pvi_hdls.ca_arrays) {\n\t\trc = validate_hdls_to_evict(&tx, svc, &hdl_uuids, &n_hdl_uuids,\n\t\t\t\t\t    in->pvi_hdls.ca_arrays,\n\t\t\t\t\t    in->pvi_hdls.ca_count);\n\t} else {\n\t\trc = find_hdls_to_evict(&tx, svc, &hdl_uuids, &hdl_uuids_size,\n\t\t\t\t\t&n_hdl_uuids);\n\t}\n\n\tif (rc != 0)\n\t\tD_GOTO(out_lock, rc);\n\n\tif (n_hdl_uuids > 0) {\n\t\t/* If pool destroy but not forcibly, error: the pool is busy */\n\n\t\tif (in->pvi_pool_destroy && !in->pvi_pool_destroy_force) {\n\t\t\tD_DEBUG(DF_DSMS, DF_UUID\": busy, %u open handles\\n\",\n\t\t\t\tDP_UUID(in->pvi_op.pi_uuid), n_hdl_uuids);\n\t\t\tD_GOTO(out_free, rc = -DER_BUSY);\n\t\t} else {\n\t\t\t/* Pool evict, or pool destroy with force=true */\n\t\t\trc = pool_disconnect_hdls(&tx, svc, hdl_uuids,\n\t\t\t\t\t\t  n_hdl_uuids, rpc->cr_ctx);\n\t\t\tif (rc != 0)\n\t\t\t\tD_GOTO(out_free, rc);\n\t\t}\n\t}\n\n\t/* If pool destroy and not error case, disable new connections */\n\tif (in->pvi_pool_destroy) {\n\t\tuint32_t\tconnectable = 0;\n\t\td_iov_t\t\tvalue;\n\n\t\td_iov_set(&value, &connectable, sizeof(connectable));\n\t\trc = rdb_tx_update(&tx, &svc->ps_root,\n\t\t\t\t   &ds_pool_prop_connectable, &value);\n\t\tif (rc != 0)\n\t\t\tD_GOTO(out_free, rc);\n\n\t\tds_pool_iv_srv_hdl_invalidate(svc->ps_pool);\n\t\tD_DEBUG(DF_DSMS, DF_UUID\": pool destroy/evict: mark pool for \"\n\t\t\t\"no new connections\\n\", DP_UUID(in->pvi_op.pi_uuid));\n\t}\n\n\trc = rdb_tx_commit(&tx);\n\t/* No need to set out->pvo_op.po_map_version. */\nout_free:\n\tD_FREE(hdl_uuids);\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->pvo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->pvo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pvi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\n/**\n * Send a CaRT message to the pool svc to test and\n * (if applicable based on destoy and force option) evict all open handles\n * on a pool.\n *\n * \\param[in]\tpool_uuid\tUUID of the pool\n * \\param[in]\tranks\t\tPool service replicas\n * \\param[in]\thandles\t\tList of handles to selectively evict\n * \\param[in]\tn_handles\tNumber of items in handles\n * \\param[in]\tdestroy\t\tIf true the evict request is a destroy request\n * \\param[in]\tforce\t\tIf true and destroy is true request all handles\n *\t\t\t\tbe forcibly evicted\n *\n * \\return\t0\t\tSuccess\n *\t\t-DER_BUSY\tOpen pool handles exist and no force requested\n *\n */\nint\nds_pool_svc_check_evict(uuid_t pool_uuid, d_rank_list_t *ranks,\n\t\t\tuuid_t *handles, size_t n_handles,\n\t\t\tuint32_t destroy, uint32_t force)\n{\n\tint\t\t\t rc;\n\tstruct rsvc_client\t client;\n\tcrt_endpoint_t\t\t ep;\n\tstruct dss_module_info\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t*rpc;\n\tstruct pool_evict_in\t*in;\n\tstruct pool_evict_out\t*out;\n\n\tD_DEBUG(DB_MGMT,\n\t\tDF_UUID\": Destroy pool (force: %d), inspect/evict handles\\n\",\n\t\tDP_UUID(pool_uuid), force);\n\n\trc = rsvc_client_init(&client, ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": cannot find pool service: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(pool_uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_EVICT, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to create pool evict rpc: %d\\n\",\n\t\t\tDP_UUID(pool_uuid), rc);\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->pvi_op.pi_uuid, pool_uuid);\n\tuuid_clear(in->pvi_op.pi_hdl);\n\tin->pvi_hdls.ca_arrays = handles;\n\tin->pvi_hdls.ca_count = n_handles;\n\n\t/* Pool destroy (force=false): assert no open handles / do not evict.\n\t * Pool destroy (force=true): evict any/all open handles on the pool.\n\t */\n\tin->pvi_pool_destroy = destroy;\n\tin->pvi_pool_destroy_force = force;\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->pvo_op.po_rc,\n\t\t\t\t      &out->pvo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->pvo_op.po_rc;\n\tif (rc != 0)\n\t\tD_ERROR(DF_UUID\": pool destroy failed to evict handles, \"\n\t\t\t\"rc: %d\\n\", DP_UUID(pool_uuid), rc);\n\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\nstatic int\nranks_get_bulk_create(crt_context_t ctx, crt_bulk_t *bulk,\n\t\t      d_rank_t *buf, daos_size_t nranks)\n{\n\td_iov_t\t\tiov;\n\td_sg_list_t\tsgl;\n\n\td_iov_set(&iov, buf, nranks * sizeof(d_rank_t));\n\tsgl.sg_nr = 1;\n\tsgl.sg_nr_out = 0;\n\tsgl.sg_iovs = &iov;\n\n\treturn crt_bulk_create(ctx, &sgl, CRT_BULK_RW, bulk);\n}\n\nstatic void\nranks_get_bulk_destroy(crt_bulk_t bulk)\n{\n\tif (bulk != CRT_BULK_NULL)\n\t\tcrt_bulk_free(bulk);\n}\n\n/*\n * Transfer list of pool ranks to \"remote_bulk\". If the remote bulk buffer\n * is too small, then return -DER_TRUNC. RPC response will contain the number\n * of ranks in the pool that the client can use to resize its buffer\n * for another RPC request.\n */\nstatic int\ntransfer_ranks_buf(d_rank_t *ranks_buf, size_t nranks,\n\t\t   struct pool_svc *svc, crt_rpc_t *rpc, crt_bulk_t remote_bulk)\n{\n\tsize_t\t\t\t\t ranks_buf_size;\n\tdaos_size_t\t\t\t remote_bulk_size;\n\td_iov_t\t\t\t\t ranks_iov;\n\td_sg_list_t\t\t\t ranks_sgl;\n\tcrt_bulk_t\t\t\t bulk = CRT_BULK_NULL;\n\tstruct crt_bulk_desc\t\t bulk_desc;\n\tcrt_bulk_opid_t\t\t\t bulk_opid;\n\tABT_eventual\t\t\t eventual;\n\tint\t\t\t\t*status;\n\tint\t\t\t\t rc;\n\n\tD_ASSERT(nranks > 0);\n\tranks_buf_size = nranks * sizeof(d_rank_t);\n\n\t/* Check if the client bulk buffer is large enough. */\n\trc = crt_bulk_get_len(remote_bulk, &remote_bulk_size);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\tif (remote_bulk_size < ranks_buf_size) {\n\t\tD_ERROR(DF_UUID \": remote ranks buffer(\" DF_U64 \")\"\n\t\t\t\" < required (%lu)\\n\", DP_UUID(svc->ps_uuid),\n\t\t\tremote_bulk_size, ranks_buf_size);\n\t\tD_GOTO(out, rc = -DER_TRUNC);\n\t}\n\n\td_iov_set(&ranks_iov, ranks_buf, ranks_buf_size);\n\tranks_sgl.sg_nr = 1;\n\tranks_sgl.sg_nr_out = 0;\n\tranks_sgl.sg_iovs = &ranks_iov;\n\n\trc = crt_bulk_create(rpc->cr_ctx, &ranks_sgl, CRT_BULK_RO, &bulk);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/* Prepare for crt_bulk_transfer(). */\n\tbulk_desc.bd_rpc = rpc;\n\tbulk_desc.bd_bulk_op = CRT_BULK_PUT;\n\tbulk_desc.bd_remote_hdl = remote_bulk;\n\tbulk_desc.bd_remote_off = 0;\n\tbulk_desc.bd_local_hdl = bulk;\n\tbulk_desc.bd_local_off = 0;\n\tbulk_desc.bd_len = ranks_iov.iov_len;\n\n\trc = ABT_eventual_create(sizeof(*status), &eventual);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_bulk, rc = dss_abterr2der(rc));\n\n\trc = crt_bulk_transfer(&bulk_desc, bulk_cb, &eventual, &bulk_opid);\n\tif (rc != 0)\n\t\tD_GOTO(out_eventual, rc);\n\n\trc = ABT_eventual_wait(eventual, (void **)&status);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_eventual, rc = dss_abterr2der(rc));\n\n\tif (*status != 0)\n\t\tD_GOTO(out_eventual, rc = *status);\n\nout_eventual:\n\tABT_eventual_free(&eventual);\nout_bulk:\n\tif (bulk != CRT_BULK_NULL)\n\t\tcrt_bulk_free(bulk);\nout:\n\treturn rc;\n}\n\n/**\n * Send CaRT RPC to pool svc to get list of storage server ranks.\n *\n * \\param[in]\tuuid\t\tUUID of the pool\n * \\param[in]\tsvc_ranks\tPool service replicas\n * \\param[out]\tranks\t\tStorage server ranks (allocated, caller-freed)\n *\n * return\t0\t\tSuccess\n *\n */\nint\nds_pool_svc_ranks_get(uuid_t uuid, d_rank_list_t *svc_ranks,\n\t\t      d_rank_list_t **ranks)\n{\n\tint\t\t\t\t rc;\n\tstruct rsvc_client\t\t client;\n\tcrt_endpoint_t\t\t\t ep;\n\tstruct dss_module_info\t\t*info = dss_get_module_info();\n\tcrt_rpc_t\t\t\t*rpc;\n\tstruct pool_ranks_get_in\t*in;\n\tstruct pool_ranks_get_out\t*out;\n\tuint32_t\t\t\t resp_nranks = 2048;\n\td_rank_list_t\t\t\t*out_ranks = NULL;\n\n\tD_DEBUG(DB_MGMT, DF_UUID \": Getting storage ranks\\n\", DP_UUID(uuid));\n\n\trc = rsvc_client_init(&client, svc_ranks);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\nrechoose:\n\tep.ep_grp = NULL; /* primary group */\n\trc = rsvc_client_choose(&client, &ep);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID \": cannot find pool service: \" DF_RC \"\\n\",\n\t\t\tDP_UUID(uuid), DP_RC(rc));\n\t\tgoto out_client;\n\t}\n\nrealloc_resp:\n\trc = pool_req_create(info->dmi_ctx, &ep, POOL_RANKS_GET, &rpc);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID \": failed to create POOL_RANKS_GET rpc, \"\n\t\t\tDF_RC \"\\n\", DP_UUID(uuid), DP_RC(rc));\n\t\tD_GOTO(out_client, rc);\n\t}\n\n\t/* Allocate response buffer */\n\tout_ranks = d_rank_list_alloc(resp_nranks);\n\tif (out_ranks == NULL)\n\t\tD_GOTO(out_rpc, rc = -DER_NOMEM);\n\n\tin = crt_req_get(rpc);\n\tuuid_copy(in->prgi_op.pi_uuid, uuid);\n\tuuid_clear(in->prgi_op.pi_hdl);\n\tin->prgi_nranks = resp_nranks;\n\trc = ranks_get_bulk_create(info->dmi_ctx, &in->prgi_ranks_bulk,\n\t\t\t\t   out_ranks->rl_ranks, in->prgi_nranks);\n\tif (rc != 0)\n\t\tD_GOTO(out_resp_buf, rc);\n\n\tD_DEBUG(DF_DSMS, DF_UUID \": send POOL_RANKS_GET to PS rank %u, \"\n\t\t\"reply capacity %u\\n\", DP_UUID(uuid), ep.ep_rank, resp_nranks);\n\n\trc = dss_rpc_send(rpc);\n\tout = crt_reply_get(rpc);\n\tD_ASSERT(out != NULL);\n\n\trc = rsvc_client_complete_rpc(&client, &ep, rc,\n\t\t\t\t      out->prgo_op.po_rc,\n\t\t\t\t      &out->prgo_op.po_hint);\n\tif (rc == RSVC_CLIENT_RECHOOSE) {\n\t\t/* To simplify logic, destroy bulk hdl and buffer each time */\n\t\tranks_get_bulk_destroy(in->prgi_ranks_bulk);\n\t\td_rank_list_free(out_ranks);\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(rechoose, rc);\n\t}\n\n\trc = out->prgo_op.po_rc;\n\tif (rc == -DER_TRUNC) {\n\t\t/* out_ranks too small - realloc with server-provided nranks */\n\t\tresp_nranks = out->prgo_nranks;\n\t\tranks_get_bulk_destroy(in->prgi_ranks_bulk);\n\t\td_rank_list_free(out_ranks);\n\t\tcrt_req_decref(rpc);\n\t\tdss_sleep(1000 /* ms */);\n\t\tD_GOTO(realloc_resp, rc);\n\t} else if (rc != 0) {\n\t\tD_ERROR(DF_UUID \": failed to get ranks, \" DF_RC \"\\n\",\n\t\t\tDP_UUID(uuid), DP_RC(rc));\n\t} else {\n\t\tout_ranks->rl_nr = out->prgo_nranks;\n\t\t*ranks = out_ranks;\n\t}\n\n\tranks_get_bulk_destroy(in->prgi_ranks_bulk);\nout_resp_buf:\n\tif (rc != 0)\n\t\td_rank_list_free(out_ranks);\nout_rpc:\n\tcrt_req_decref(rpc);\nout_client:\n\trsvc_client_fini(&client);\nout:\n\treturn rc;\n}\n\n/* CaRT RPC handler run in PS leader to return pool storage ranks\n */\nvoid\nds_pool_ranks_get_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_ranks_get_in\t*in = crt_req_get(rpc);\n\tstruct pool_ranks_get_out\t*out = crt_reply_get(rpc);\n\tuint32_t\t\t\t nranks = 0;\n\td_rank_list_t\t\t\tout_ranks = {0};\n\tstruct pool_svc\t\t\t*svc;\n\tint\t\t\t\t rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID \": processing rpc %p:\\n\",\n\t\tDP_UUID(in->prgi_op.pi_uuid), rpc);\n\n\trc = pool_svc_lookup_leader(in->prgi_op.pi_uuid, &svc,\n\t\t\t\t    &out->prgo_op.po_hint);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\t/* This is a server to server RPC only */\n\tif (daos_rpc_from_client(rpc))\n\t\tD_GOTO(out, rc = -DER_INVAL);\n\n\trc = ds_pool_get_ranks(in->prgi_op.pi_uuid, MAP_RANKS_UP, &out_ranks);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID \": get ranks failed, \" DF_RC \"\\n\",\n\t\t\tDP_UUID(in->prgi_op.pi_uuid), DP_RC(rc));\n\t\tD_GOTO(out_svc, rc);\n\t} else if ((in->prgi_nranks > 0) &&\n\t\t   (out_ranks.rl_nr > in->prgi_nranks)) {\n\t\tD_DEBUG(DF_DSMS, DF_UUID \": %u ranks (more than client: %u)\\n\",\n\t\t\tDP_UUID(in->prgi_op.pi_uuid), out_ranks.rl_nr,\n\t\t\tin->prgi_nranks);\n\t\tD_GOTO(out_free, rc = -DER_TRUNC);\n\t} else {\n\t\tD_DEBUG(DF_DSMS, DF_UUID \": %u ranks\\n\",\n\t\t\tDP_UUID(in->prgi_op.pi_uuid), out_ranks.rl_nr);\n\t\tif ((out_ranks.rl_nr > 0) && (in->prgi_nranks > 0) &&\n\t\t    (in->prgi_ranks_bulk != CRT_BULK_NULL))\n\t\t\trc = transfer_ranks_buf(out_ranks.rl_ranks,\n\t\t\t\t\t\tout_ranks.rl_nr, svc, rpc,\n\t\t\t\t\t\tin->prgi_ranks_bulk);\n\t}\n\nout_free:\n\tnranks = out_ranks.rl_nr;\n\tmap_ranks_fini(&out_ranks);\n\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->prgo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->prgo_op.po_rc = rc;\n\tout->prgo_nranks = nranks;\n\tD_DEBUG(DF_DSMS, DF_UUID \": replying rpc %p: %d\\n\",\n\t\tDP_UUID(in->prgi_op.pi_uuid), rpc, rc);\n\tcrt_reply_send(rpc);\n}\n\n/* This RPC could be implemented by ds_rsvc. */\nvoid\nds_pool_svc_stop_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_svc_stop_in\t       *in = crt_req_get(rpc);\n\tstruct pool_svc_stop_out       *out = crt_reply_get(rpc);\n\td_iov_t\t\t\tid;\n\tint\t\t\t\trc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p\\n\",\n\t\tDP_UUID(in->psi_op.pi_uuid), rpc);\n\n\td_iov_set(&id, in->psi_op.pi_uuid, sizeof(uuid_t));\n\trc = ds_rsvc_stop_leader(DS_RSVC_CLASS_POOL, &id, &out->pso_op.po_hint);\n\n\tout->pso_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->psi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\n/**\n * Get a copy of the latest pool map buffer. Callers are responsible for\n * freeing iov->iov_buf with D_FREE.\n */\nint\nds_pool_map_buf_get(uuid_t uuid, d_iov_t *iov, uint32_t *map_version)\n{\n\tstruct pool_svc\t*svc;\n\tstruct rdb_tx\ttx;\n\tstruct pool_buf\t*map_buf;\n\tint\t\trc;\n\n\trc = pool_svc_lookup_leader(uuid, &svc, NULL /* hint */);\n\tif (rc != 0)\n\t\tD_GOTO(out, rc);\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\tABT_rwlock_rdlock(svc->ps_lock);\n\trc = read_map_buf(&tx, &svc->ps_root, &map_buf, map_version);\n\tif (rc != 0) {\n\t\tD_ERROR(DF_UUID\": failed to read pool map: \"DF_RC\"\\n\",\n\t\t\tDP_UUID(svc->ps_uuid), DP_RC(rc));\n\t\tD_GOTO(out_lock, rc);\n\t}\n\tD_ASSERT(map_buf != NULL);\n\tiov->iov_buf = map_buf;\n\tiov->iov_len = pool_buf_size(map_buf->pb_nr);\n\tiov->iov_buf_len = pool_buf_size(map_buf->pb_nr);\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tpool_svc_put_leader(svc);\nout:\n\treturn rc;\n}\n\nvoid\nds_pool_iv_ns_update(struct ds_pool *pool, unsigned int master_rank)\n{\n\tds_iv_ns_update(pool->sp_iv_ns, master_rank);\n}\n\nint\nds_pool_svc_term_get(uuid_t uuid, uint64_t *term)\n{\n\tstruct pool_svc\t*svc;\n\tint\t\trc;\n\n\trc = pool_svc_lookup_leader(uuid, &svc, NULL /* hint */);\n\tif (rc != 0)\n\t\treturn rc;\n\n\t*term = svc->ps_rsvc.s_term;\n\n\tpool_svc_put_leader(svc);\n\treturn 0;\n}\n\nvoid\nds_pool_attr_set_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_attr_set_in  *in = crt_req_get(rpc);\n\tstruct pool_op_out\t *out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t *svc;\n\tstruct rdb_tx\t\t  tx;\n\tint\t\t\t  rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->pasi_op.pi_uuid), rpc, DP_UUID(in->pasi_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->pasi_op.pi_uuid, &svc, &out->po_hint);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tgoto out_svc;\n\n\tABT_rwlock_wrlock(svc->ps_lock);\n\trc = ds_rsvc_set_attr(&svc->ps_rsvc, &tx, &svc->ps_user,\n\t\t\t      in->pasi_bulk, rpc, in->pasi_count);\n\tif (rc != 0)\n\t\tgoto out_lock;\n\n\trc = rdb_tx_commit(&tx);\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pasi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nvoid\nds_pool_attr_del_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_attr_del_in  *in = crt_req_get(rpc);\n\tstruct pool_op_out\t *out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t *svc;\n\tstruct rdb_tx\t\t  tx;\n\tint\t\t\t  rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->padi_op.pi_uuid), rpc, DP_UUID(in->padi_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->padi_op.pi_uuid, &svc, &out->po_hint);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tgoto out_svc;\n\n\tABT_rwlock_wrlock(svc->ps_lock);\n\trc = ds_rsvc_del_attr(&svc->ps_rsvc, &tx, &svc->ps_user,\n\t\t\t      in->padi_bulk, rpc, in->padi_count);\n\tif (rc != 0)\n\t\tgoto out_lock;\n\n\trc = rdb_tx_commit(&tx);\n\nout_lock:\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->padi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nvoid\nds_pool_attr_get_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_attr_get_in  *in = crt_req_get(rpc);\n\tstruct pool_op_out\t *out = crt_reply_get(rpc);\n\tstruct pool_svc\t\t *svc;\n\tstruct rdb_tx\t\t  tx;\n\tint\t\t\t  rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->pagi_op.pi_uuid), rpc, DP_UUID(in->pagi_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->pagi_op.pi_uuid, &svc, &out->po_hint);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tgoto out_svc;\n\n\tABT_rwlock_rdlock(svc->ps_lock);\n\trc = ds_rsvc_get_attr(&svc->ps_rsvc, &tx, &svc->ps_user, in->pagi_bulk,\n\t\t\t      rpc, in->pagi_count, in->pagi_key_length);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pagi_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n\n}\n\nvoid\nds_pool_attr_list_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_attr_list_in\t*in\t    = crt_req_get(rpc);\n\tstruct pool_attr_list_out\t*out\t    = crt_reply_get(rpc);\n\tstruct pool_svc\t\t\t*svc;\n\tstruct rdb_tx\t\t\t tx;\n\tint\t\t\t\t rc;\n\n\tD_DEBUG(DF_DSMS, DF_UUID\": processing rpc %p: hdl=\"DF_UUID\"\\n\",\n\t\tDP_UUID(in->pali_op.pi_uuid), rpc, DP_UUID(in->pali_op.pi_hdl));\n\n\trc = pool_svc_lookup_leader(in->pali_op.pi_uuid, &svc,\n\t\t\t\t    &out->palo_op.po_hint);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tgoto out_svc;\n\n\tABT_rwlock_rdlock(svc->ps_lock);\n\trc = ds_rsvc_list_attr(&svc->ps_rsvc, &tx, &svc->ps_user,\n\t\t\t       in->pali_bulk, rpc, &out->palo_size);\n\tABT_rwlock_unlock(svc->ps_lock);\n\trdb_tx_end(&tx);\nout_svc:\n\tds_rsvc_set_hint(&svc->ps_rsvc, &out->palo_op.po_hint);\n\tpool_svc_put_leader(svc);\nout:\n\tout->palo_op.po_rc = rc;\n\tD_DEBUG(DF_DSMS, DF_UUID\": replying rpc %p: \"DF_RC\"\\n\",\n\t\tDP_UUID(in->pali_op.pi_uuid), rpc, DP_RC(rc));\n\tcrt_reply_send(rpc);\n}\n\nvoid\nds_pool_replicas_update_handler(crt_rpc_t *rpc)\n{\n\tstruct pool_membership_in\t*in = crt_req_get(rpc);\n\tstruct pool_membership_out\t*out = crt_reply_get(rpc);\n\td_rank_list_t\t\t\t*ranks;\n\td_iov_t\t\t\t\t id;\n\tint\t\t\t\t rc;\n\n\trc = daos_rank_list_dup(&ranks, in->pmi_targets);\n\tif (rc != 0)\n\t\tgoto out;\n\td_iov_set(&id, in->pmi_uuid, sizeof(uuid_t));\n\n\tswitch (opc_get(rpc->cr_opc)) {\n\tcase POOL_REPLICAS_ADD:\n\t\trc = ds_rsvc_add_replicas(DS_RSVC_CLASS_POOL, &id, ranks,\n\t\t\t\t\t  ds_rsvc_get_md_cap(), &out->pmo_hint);\n\t\tbreak;\n\n\tcase POOL_REPLICAS_REMOVE:\n\t\trc = ds_rsvc_remove_replicas(DS_RSVC_CLASS_POOL, &id, ranks,\n\t\t\t\t\t     &out->pmo_hint);\n\t\tbreak;\n\n\tdefault:\n\t\tD_ASSERT(0);\n\t}\n\n\tout->pmo_failed = ranks;\nout:\n\tout->pmo_rc = rc;\n\tcrt_reply_send(rpc);\n}\n\nint\nds_pool_elect_dtx_leader(struct ds_pool *pool, daos_unit_oid_t *oid,\n\t\t\t uint32_t version)\n{\n\tstruct pl_map\t\t*map;\n\tstruct pl_obj_layout\t*layout;\n\tstruct daos_obj_md\t md = { 0 };\n\tint\t\t\t rc = 0;\n\n\tmap = pl_map_find(pool->sp_uuid, oid->id_pub);\n\tif (map == NULL) {\n\t\tD_WARN(\"Failed to find pool map tp select leader for \"\n\t\t       DF_UOID\" version = %d\\n\", DP_UOID(*oid), version);\n\t\treturn -DER_INVAL;\n\t}\n\n\tmd.omd_id = oid->id_pub;\n\tmd.omd_ver = version;\n\trc = pl_obj_place(map, &md, NULL, &layout);\n\tif (rc != 0)\n\t\tgoto out;\n\n\trc = pl_select_leader(oid->id_pub, oid->id_shard / layout->ol_grp_size,\n\t\t\t      layout->ol_grp_size, true,\n\t\t\t      pl_obj_get_shard, layout);\n\tpl_obj_layout_free(layout);\n\tif (rc < 0)\n\t\tD_WARN(\"Failed to select leader for \"DF_UOID\n\t\t       \"version = %d: rc = %d\\n\",\n\t\t       DP_UOID(*oid), version, rc);\n\nout:\n\tpl_map_decref(map);\n\treturn rc;\n}\n\n/**\n * Check whether the leader replica of the given object resides\n * on current server or not.\n *\n * \\param [IN]\tpool\t\tPointer to the pool\n * \\param [IN]\toid\t\tThe OID of the object to be checked\n * \\param [IN]\tversion\t\tThe pool map version\n *\n * \\return\t\t\t+1 if leader is on current server.\n * \\return\t\t\tZero if the leader resides on another server.\n * \\return\t\t\tNegative value if error.\n */\nint\nds_pool_check_dtx_leader(struct ds_pool *pool, daos_unit_oid_t *oid,\n\t\t\t uint32_t version)\n{\n\tstruct pool_target\t*target;\n\td_rank_t\t\t myrank;\n\tint\t\t\t leader;\n\tint\t\t\t rc;\n\n\tleader = ds_pool_elect_dtx_leader(pool, oid, version);\n\tif (leader < 0)\n\t\treturn leader;\n\n\tD_DEBUG(DB_TRACE, \"get new leader tgt id %d\\n\", leader);\n\trc = pool_map_find_target(pool->sp_map, leader, &target);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (rc != 1)\n\t\treturn -DER_INVAL;\n\n\trc = crt_group_rank(NULL, &myrank);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tif (myrank != target->ta_comp.co_rank)\n\t\trc = 0;\n\telse\n\t\trc = 1;\n\n\treturn rc;\n}\n\n/* Update pool map version for current xstream. */\nint\nds_pool_child_map_refresh_sync(struct ds_pool_child *dpc)\n{\n\tstruct pool_map_refresh_ult_arg\targ;\n\tABT_eventual\t\t\teventual;\n\tint\t\t\t\t*status;\n\tint\t\t\t\trc;\n\n\trc = ABT_eventual_create(sizeof(*status), &eventual);\n\tif (rc != ABT_SUCCESS)\n\t\treturn dss_abterr2der(rc);\n\n\targ.iua_pool_version = dpc->spc_map_version;\n\tuuid_copy(arg.iua_pool_uuid, dpc->spc_uuid);\n\targ.iua_eventual = eventual;\n\n\trc = dss_ult_create(ds_pool_map_refresh_ult, &arg, DSS_XS_SYS,\n\t\t\t    0, 0, NULL);\n\tif (rc)\n\t\tD_GOTO(out_eventual, rc);\n\n\trc = ABT_eventual_wait(eventual, (void **)&status);\n\tif (rc != ABT_SUCCESS)\n\t\tD_GOTO(out_eventual, rc = dss_abterr2der(rc));\n\tif (*status != 0)\n\t\tD_GOTO(out_eventual, rc = *status);\n\nout_eventual:\n\tABT_eventual_free(&eventual);\n\treturn rc;\n}\n\nint\nds_pool_child_map_refresh_async(struct ds_pool_child *dpc)\n{\n\tstruct pool_map_refresh_ult_arg\t*arg;\n\tint\t\t\t\trc;\n\n\tD_ALLOC_PTR(arg);\n\tif (arg == NULL)\n\t\treturn -DER_NOMEM;\n\targ->iua_pool_version = dpc->spc_map_version;\n\tuuid_copy(arg->iua_pool_uuid, dpc->spc_uuid);\n\n\trc = dss_ult_create(ds_pool_map_refresh_ult, arg, DSS_XS_SYS,\n\t\t\t    0, 0, NULL);\n\treturn rc;\n}\n\n\nint ds_pool_prop_fetch(struct ds_pool *pool, unsigned int bits,\n\t\t       daos_prop_t **prop_out)\n{\n\tstruct pool_svc\t*svc;\n\tstruct rdb_tx\ttx;\n\tint\t\trc;\n\n\trc = pool_svc_lookup_leader(pool->sp_uuid, &svc, NULL);\n\tif (rc != 0)\n\t\treturn rc;\n\n\trc = rdb_tx_begin(svc->ps_rsvc.s_db, svc->ps_rsvc.s_term, &tx);\n\tif (rc != 0)\n\t\tD_GOTO(out_svc, rc);\n\n\t/* read optional properties */\n\tABT_rwlock_rdlock(svc->ps_lock);\n\trc = pool_prop_read(&tx, svc, bits, prop_out);\n\tABT_rwlock_unlock(svc->ps_lock);\n\tif (rc != 0)\n\t\tD_GOTO(out_tx, rc);\nout_tx:\n\trdb_tx_end(&tx);\nout_svc:\n\tpool_svc_put_leader(svc);\n\treturn rc;\n}\n\nbool\nis_container_from_srv(uuid_t pool_uuid, uuid_t coh_uuid)\n{\n\tstruct ds_pool\t*pool;\n\tuuid_t\t\thdl_uuid;\n\tint\t\trc;\n\tbool\t\tresult = false;\n\n\tpool = ds_pool_lookup(pool_uuid);\n\tif (pool == NULL) {\n\t\tD_ERROR(DF_UUID\": failed to get ds_pool\\n\",\n\t\t\tDP_UUID(pool_uuid));\n\t\treturn false;\n\t}\n\n\tif (uuid_compare(coh_uuid, pool->sp_srv_cont_hdl) == 0) {\n\t\t/* Compare if the handle uuid is from another server */\n\t\tresult = true;\n\t\tD_GOTO(output, result);\n\t}\n\n\trc = ds_pool_iv_srv_hdl_fetch_non_sys(pool, &hdl_uuid, NULL);\n\tif (rc) {\n\t\tD_ERROR(DF_UUID\" fetch srv hdl: %d\\n\", DP_UUID(pool_uuid), rc);\n\t\tD_GOTO(output, result);\n\t}\n\n\tresult = !uuid_compare(coh_uuid, hdl_uuid);\noutput:\n\tds_pool_put(pool);\n\treturn result;\n}\n\nbool\nis_pool_from_srv(uuid_t pool_uuid, uuid_t poh_uuid)\n{\n\tstruct ds_pool\t*pool;\n\tuuid_t\t\thdl_uuid;\n\tint\t\trc;\n\n\tpool = ds_pool_lookup(pool_uuid);\n\tif (pool == NULL) {\n\t\tD_ERROR(DF_UUID\": failed to get ds_pool\\n\",\n\t\t\tDP_UUID(pool_uuid));\n\t\treturn false;\n\t}\n\n\trc = ds_pool_iv_srv_hdl_fetch(pool, &hdl_uuid, NULL);\n\tds_pool_put(pool);\n\tif (rc) {\n\t\tD_ERROR(DF_UUID\" fetch srv hdl: %d\\n\", DP_UUID(pool_uuid), rc);\n\t\treturn false;\n\t}\n\n\treturn !uuid_compare(poh_uuid, hdl_uuid);\n}\n\n",
  "patch": "@@ -947,6 +947,7 @@ out:\n \t\tD_ERROR(\"pool \"DF_UUID\" event %d failed: rc %d\\n\",\n \t\t\tDP_UUID(svc->ps_uuid), src, rc);\n \tdaos_prop_fini(&prop);\n+#endif\n }\n \n static void\n",
  "msg": "This will be removed.",
  "id": 171235,
  "y": 1
}

--- Sample 5 ---
{
  "oldf": "// Licensed to Elasticsearch B.V. under one or more contributor\n// license agreements. See the NOTICE file distributed with\n// this work for additional information regarding copyright\n// ownership. Elasticsearch B.V. licenses this file to you under\n// the Apache License, Version 2.0 (the \"License\"); you may\n// not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage api\n\nimport (\n\t\"net/http\"\n\t\"net/http/pprof\"\n\t\"regexp\"\n\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/elastic/beats/v7/libbeat/beat\"\n\t\"github.com/elastic/beats/v7/libbeat/logp\"\n\t\"github.com/elastic/beats/v7/libbeat/monitoring\"\n\n\t\"github.com/elastic/apm-server/agentcfg\"\n\tapisourcemap \"github.com/elastic/apm-server/beater/api/asset/sourcemap\"\n\t\"github.com/elastic/apm-server/beater/api/config/agent\"\n\t\"github.com/elastic/apm-server/beater/api/intake\"\n\t\"github.com/elastic/apm-server/beater/api/profile\"\n\t\"github.com/elastic/apm-server/beater/api/root\"\n\t\"github.com/elastic/apm-server/beater/auth\"\n\t\"github.com/elastic/apm-server/beater/config\"\n\t\"github.com/elastic/apm-server/beater/middleware\"\n\t\"github.com/elastic/apm-server/beater/ratelimit\"\n\t\"github.com/elastic/apm-server/beater/request\"\n\tlogs \"github.com/elastic/apm-server/log\"\n\t\"github.com/elastic/apm-server/model\"\n\t\"github.com/elastic/apm-server/model/modelprocessor\"\n\t\"github.com/elastic/apm-server/processor/stream\"\n\t\"github.com/elastic/apm-server/publish\"\n\t\"github.com/elastic/apm-server/sourcemap\"\n)\n\nconst (\n\t// RootPath defines the server's root path\n\tRootPath = \"/\"\n\n\t// Backend routes\n\n\t// AgentConfigPath defines the path to query for agent config management\n\tAgentConfigPath = \"/config/v1/agents\"\n\t// AssetSourcemapPath defines the path to upload sourcemaps\n\tAssetSourcemapPath = \"/assets/v1/sourcemaps\"\n\t// IntakePath defines the path to ingest monitored events\n\tIntakePath = \"/intake/v2/events\"\n\t// ProfilePath defines the path to ingest profiles\n\tProfilePath = \"/intake/v2/profile\"\n\n\t// RUM routes\n\n\t// AgentConfigRUMPath defines the path to query for the RUM agent config management\n\tAgentConfigRUMPath = \"/config/v1/rum/agents\"\n\t// IntakeRUMPath defines the path to ingest monitored RUM events\n\tIntakeRUMPath = \"/intake/v2/rum/events\"\n\n\tIntakeRUMV3Path = \"/intake/v3/rum/events\"\n)\n\n// NewMux registers apm handlers to paths building up the APM Server API.\nfunc NewMux(\n\tbeatInfo beat.Info,\n\tbeaterConfig *config.Config,\n\treport publish.Reporter,\n\tbatchProcessor model.BatchProcessor,\n\tauthenticator *auth.Authenticator,\n\tfetcher agentcfg.Fetcher,\n\tratelimitStore *ratelimit.Store,\n\tsourcemapStore *sourcemap.Store,\n\tfleetManaged bool,\n) (*http.ServeMux, error) {\n\tpool := request.NewContextPool()\n\tmux := http.NewServeMux()\n\tlogger := logp.NewLogger(logs.Handler)\n\n\tbuilder := routeBuilder{\n\t\tinfo:           beatInfo,\n\t\tcfg:            beaterConfig,\n\t\tauthenticator:  authenticator,\n\t\treporter:       report,\n\t\tbatchProcessor: batchProcessor,\n\t\tratelimitStore: ratelimitStore,\n\t\tsourcemapStore: sourcemapStore,\n\t\tfleetManaged:   fleetManaged,\n\t}\n\n\ttype route struct {\n\t\tpath      string\n\t\thandlerFn func() (request.Handler, error)\n\t}\n\trouteMap := []route{\n\t\t{RootPath, builder.rootHandler},\n\t\t{AssetSourcemapPath, builder.sourcemapHandler},\n\t\t{AgentConfigPath, builder.backendAgentConfigHandler(fetcher)},\n\t\t{AgentConfigRUMPath, builder.rumAgentConfigHandler(fetcher)},\n\t\t{IntakeRUMPath, builder.rumIntakeHandler(stream.RUMV2Processor)},\n\t\t{IntakeRUMV3Path, builder.rumIntakeHandler(stream.RUMV3Processor)},\n\t\t{IntakePath, builder.backendIntakeHandler},\n\t\t// The profile endpoint is in Beta\n\t\t{ProfilePath, builder.profileHandler},\n\t}\n\n\tfor _, route := range routeMap {\n\t\th, err := route.handlerFn()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tlogger.Infof(\"Path %s added to request handler\", route.path)\n\t\tmux.Handle(route.path, pool.HTTPHandler(h))\n\t}\n\tif beaterConfig.Expvar.Enabled {\n\t\tpath := beaterConfig.Expvar.URL\n\t\tlogger.Infof(\"Path %s added to request handler\", path)\n\t\tmux.Handle(path, http.HandlerFunc(debugVarsHandler))\n\t}\n\tif beaterConfig.Pprof.Enabled {\n\t\tconst path = \"/debug/pprof\"\n\t\tlogger.Infof(\"Path %s added to request handler\", path)\n\t\tmux.Handle(path+\"/\", http.HandlerFunc(pprof.Index))\n\t\tmux.Handle(path+\"/cmdline\", http.HandlerFunc(pprof.Cmdline))\n\t\tmux.Handle(path+\"/profile\", http.HandlerFunc(pprof.Profile))\n\t\tmux.Handle(path+\"/symbol\", http.HandlerFunc(pprof.Symbol))\n\t\tmux.Handle(path+\"/trace\", http.HandlerFunc(pprof.Trace))\n\t}\n\treturn mux, nil\n}\n\ntype routeBuilder struct {\n\tinfo           beat.Info\n\tcfg            *config.Config\n\tauthenticator  *auth.Authenticator\n\treporter       publish.Reporter\n\tbatchProcessor model.BatchProcessor\n\tratelimitStore *ratelimit.Store\n\tsourcemapStore *sourcemap.Store\n\tfleetManaged   bool\n}\n\nfunc (r *routeBuilder) profileHandler() (request.Handler, error) {\n\trequestMetadataFunc := emptyRequestMetadata\n\tif r.cfg.AugmentEnabled {\n\t\trequestMetadataFunc = backendRequestMetadata\n\t}\n\th := profile.Handler(requestMetadataFunc, r.batchProcessor)\n\treturn middleware.Wrap(h, backendMiddleware(r.cfg, r.authenticator, r.ratelimitStore, profile.MonitoringMap)...)\n}\n\nfunc (r *routeBuilder) backendIntakeHandler() (request.Handler, error) {\n\trequestMetadataFunc := emptyRequestMetadata\n\tif r.cfg.AugmentEnabled {\n\t\trequestMetadataFunc = backendRequestMetadata\n\t}\n\th := intake.Handler(stream.BackendProcessor(r.cfg), requestMetadataFunc, r.batchProcessor)\n\treturn middleware.Wrap(h, backendMiddleware(r.cfg, r.authenticator, r.ratelimitStore, intake.MonitoringMap)...)\n}\n\nfunc (r *routeBuilder) rumIntakeHandler(newProcessor func(*config.Config) *stream.Processor) func() (request.Handler, error) {\n\trequestMetadataFunc := emptyRequestMetadata\n\tif r.cfg.AugmentEnabled {\n\t\trequestMetadataFunc = rumRequestMetadata\n\t}\n\treturn func() (request.Handler, error) {\n\t\tvar batchProcessors modelprocessor.Chained\n\t\t// The order of these processors is important. Source mapping must happen before identifying library frames, or\n\t\t// frames to exclude from error grouping; identifying library frames must happen before updating the error culprit.\n\t\tif r.sourcemapStore != nil {\n\t\t\tbatchProcessors = append(batchProcessors, sourcemap.BatchProcessor{\n\t\t\t\tStore:   r.sourcemapStore,\n\t\t\t\tTimeout: r.cfg.RumConfig.SourceMapping.Timeout,\n\t\t\t})\n\t\t}\n\t\tif r.cfg.RumConfig.LibraryPattern != \"\" {\n\t\t\tre, err := regexp.Compile(r.cfg.RumConfig.LibraryPattern)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"invalid library pattern regex\")\n\t\t\t}\n\t\t\tbatchProcessors = append(batchProcessors, modelprocessor.SetLibraryFrame{Pattern: re})\n\t\t}\n\t\tif r.cfg.RumConfig.ExcludeFromGrouping != \"\" {\n\t\t\tre, err := regexp.Compile(r.cfg.RumConfig.ExcludeFromGrouping)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, errors.Wrap(err, \"invalid exclude from grouping regex\")\n\t\t\t}\n\t\t\tbatchProcessors = append(batchProcessors, modelprocessor.SetExcludeFromGrouping{Pattern: re})\n\t\t}\n\t\tif r.sourcemapStore != nil {\n\t\t\tbatchProcessors = append(batchProcessors, modelprocessor.SetCulprit{})\n\t\t}\n\t\tbatchProcessors = append(batchProcessors, r.batchProcessor) // r.batchProcessor always goes last\n\t\th := intake.Handler(newProcessor(r.cfg), requestMetadataFunc, batchProcessors)\n\t\treturn middleware.Wrap(h, rumMiddleware(r.cfg, r.authenticator, r.ratelimitStore, intake.MonitoringMap)...)\n\t}\n}\n\nfunc (r *routeBuilder) sourcemapHandler() (request.Handler, error) {\n\th := apisourcemap.Handler(r.reporter, r.sourcemapStore)\n\treturn middleware.Wrap(h, sourcemapMiddleware(r.cfg, r.authenticator, r.ratelimitStore)...)\n}\n\nfunc (r *routeBuilder) rootHandler() (request.Handler, error) {\n\th := root.Handler(root.HandlerConfig{Version: r.info.Version})\n\treturn middleware.Wrap(h, rootMiddleware(r.cfg, r.authenticator)...)\n}\n\nfunc (r *routeBuilder) backendAgentConfigHandler(f agentcfg.Fetcher) func() (request.Handler, error) {\n\treturn func() (request.Handler, error) {\n\t\treturn agentConfigHandler(r.cfg, r.authenticator, r.ratelimitStore, backendMiddleware, f, r.fleetManaged)\n\t}\n}\n\nfunc (r *routeBuilder) rumAgentConfigHandler(f agentcfg.Fetcher) func() (request.Handler, error) {\n\treturn func() (request.Handler, error) {\n\t\treturn agentConfigHandler(r.cfg, r.authenticator, r.ratelimitStore, rumMiddleware, f, r.fleetManaged)\n\t}\n}\n\ntype middlewareFunc func(*config.Config, *auth.Authenticator, *ratelimit.Store, map[request.ResultID]*monitoring.Int) []middleware.Middleware\n\nfunc agentConfigHandler(\n\tcfg *config.Config,\n\tauthenticator *auth.Authenticator,\n\tratelimitStore *ratelimit.Store,\n\tmiddlewareFunc middlewareFunc,\n\tf agentcfg.Fetcher,\n\tfleetManaged bool,\n) (request.Handler, error) {\n\tmw := middlewareFunc(cfg, authenticator, ratelimitStore, agent.MonitoringMap)\n\th := agent.NewHandler(f, cfg.KibanaAgentConfig, cfg.DefaultServiceEnvironment, cfg.AgentAuth.Anonymous.AllowAgent)\n\n\tif !cfg.Kibana.Enabled && !fleetManaged {\n\t\tmsg := \"Agent remote configuration is disabled. \" +\n\t\t\t\"Configure the `apm-server.kibana` section in apm-server.yml to enable it. \" +\n\t\t\t\"If you are using a RUM agent, you also need to configure the `apm-server.rum` section. \" +\n\t\t\t\"If you are not using remote configuration, you can safely ignore this error.\"\n\t\tmw = append(mw, middleware.KillSwitchMiddleware(cfg.Kibana.Enabled, msg))\n\t}\n\n\treturn middleware.Wrap(h, mw...)\n}\n\nfunc apmMiddleware(m map[request.ResultID]*monitoring.Int) []middleware.Middleware {\n\treturn []middleware.Middleware{\n\t\tmiddleware.LogMiddleware(),\n\t\tmiddleware.TimeoutMiddleware(),\n\t\tmiddleware.RecoverPanicMiddleware(),\n\t\tmiddleware.MonitoringMiddleware(m),\n\t\tmiddleware.RequestTimeMiddleware(),\n\t}\n}\n\nfunc backendMiddleware(cfg *config.Config, authenticator *auth.Authenticator, ratelimitStore *ratelimit.Store, m map[request.ResultID]*monitoring.Int) []middleware.Middleware {\n\tbackendMiddleware := append(apmMiddleware(m),\n\t\tmiddleware.ResponseHeadersMiddleware(cfg.ResponseHeaders),\n\t\tmiddleware.AuthMiddleware(authenticator, true),\n\t\tmiddleware.AnonymousRateLimitMiddleware(ratelimitStore),\n\t)\n\treturn backendMiddleware\n}\n\nfunc rumMiddleware(cfg *config.Config, authenticator *auth.Authenticator, ratelimitStore *ratelimit.Store, m map[request.ResultID]*monitoring.Int) []middleware.Middleware {\n\tmsg := \"RUM endpoint is disabled. \" +\n\t\t\"Configure the `apm-server.rum` section in apm-server.yml to enable ingestion of RUM events. \" +\n\t\t\"If you are not using the RUM agent, you can safely ignore this error.\"\n\trumMiddleware := append(apmMiddleware(m),\n\t\tmiddleware.ResponseHeadersMiddleware(cfg.ResponseHeaders),\n\t\tmiddleware.ResponseHeadersMiddleware(cfg.RumConfig.ResponseHeaders),\n\t\tmiddleware.CORSMiddleware(cfg.RumConfig.AllowOrigins, cfg.RumConfig.AllowHeaders),\n\t\tmiddleware.AuthMiddleware(authenticator, true),\n\t\tmiddleware.AnonymousRateLimitMiddleware(ratelimitStore),\n\t)\n\treturn append(rumMiddleware, middleware.KillSwitchMiddleware(cfg.RumConfig.Enabled, msg))\n}\n\nfunc sourcemapMiddleware(cfg *config.Config, auth *auth.Authenticator, ratelimitStore *ratelimit.Store) []middleware.Middleware {\n\tmsg := \"Sourcemap upload endpoint is disabled. \" +\n\t\t\"Configure the `apm-server.rum` section in apm-server.yml to enable sourcemap uploads. \" +\n\t\t\"If you are not using the RUM agent, you can safely ignore this error.\"\n\tif cfg.DataStreams.Enabled {\n\t\tmsg = \"When APM Server is managed by Fleet, Sourcemaps must be uploaded directly to Elasticsearch.\"\n\t}\n\tenabled := cfg.RumConfig.Enabled && cfg.RumConfig.SourceMapping.Enabled && !cfg.DataStreams.Enabled\n\tbackendMiddleware := backendMiddleware(cfg, auth, ratelimitStore, apisourcemap.MonitoringMap)\n\treturn append(backendMiddleware, middleware.KillSwitchMiddleware(enabled, msg))\n}\n\nfunc rootMiddleware(cfg *config.Config, authenticator *auth.Authenticator) []middleware.Middleware {\n\treturn append(apmMiddleware(root.MonitoringMap),\n\t\tmiddleware.ResponseHeadersMiddleware(cfg.ResponseHeaders),\n\t\tmiddleware.AuthMiddleware(authenticator, false),\n\t)\n}\n\nfunc emptyRequestMetadata(c *request.Context) model.APMEvent {\n\treturn model.APMEvent{}\n}\n\nfunc backendRequestMetadata(c *request.Context) model.APMEvent {\n\treturn model.APMEvent{Host: model.Host{IP: c.SourceIP}}\n}\n\nfunc rumRequestMetadata(c *request.Context) model.APMEvent {\n\treturn model.APMEvent{\n\t\tClient:    model.Client{IP: c.SourceIP},\n\t\tUserAgent: model.UserAgent{Original: c.UserAgent},\n\t}\n}\n",
  "patch": "@@ -160,6 +160,11 @@ func (r *routeBuilder) profileHandler() (request.Handler, error) {\n \treturn middleware.Wrap(h, backendMiddleware(r.cfg, r.authenticator, r.ratelimitStore, profile.MonitoringMap)...)\n }\n \n+func (r *routeBuilder) firehoseLogHandler() (request.Handler, error) {\n+\th := firehose.Handler(r.batchProcessor, r.authenticator)\n+\treturn middleware.Wrap(h, firehoseMiddleware(r.cfg, intake.MonitoringMap)...)\n+}\n+\n func (r *routeBuilder) backendIntakeHandler() (request.Handler, error) {\n \trequestMetadataFunc := emptyRequestMetadata\n \tif r.cfg.AugmentEnabled {\n",
  "msg": "nit: `firehoseLogHandler` vs. `firehoseMiddleware` looks like a naming inconsistency? (`log` is not used anywhere else).",
  "id": 32732,
  "y": 1
}



Dataset: Diff_Quality_Estimation
Path: Datasets/Diff_Quality_Estimation/cls-train-chunk-0.jsonl
Total lines (records): 66459
Top keys and frequencies:
  oldf: 66459
  patch: 66459
  msg: 66459
  id: 66459
  y: 66459
  idx: 33293
  proj: 33293
  lang: 33293

Samples (first records):
--- Sample 1 ---
{
  "oldf": "// Copyright (c) 2014-2016, The Monero Project\n//\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without modification, are\n// permitted provided that the following conditions are met:\n//\n// 1. Redistributions of source code must retain the above copyright notice, this list of\n//    conditions and the following disclaimer.\n//\n// 2. Redistributions in binary form must reproduce the above copyright notice, this list\n//    of conditions and the following disclaimer in the documentation and/or other\n//    materials provided with the distribution.\n//\n// 3. Neither the name of the copyright holder nor the names of its contributors may be\n//    used to endorse or promote products derived from this software without specific\n//    prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY\n// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n// MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL\n// THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,\n// STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF\n// THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Parts of this file are originally copyright (c) 2012-2013 The Cryptonote developers\n\n\n#include \"wallet_manager.h\"\n#include \"wallet.h\"\n\n#include <boost/filesystem.hpp>\n#include <boost/regex.hpp>\n\n\nnamespace epee {\n    unsigned int g_test_dbg_lock_sleep = 0;\n}\n\nnamespace Bitmonero {\n\nWallet *WalletManagerImpl::createWallet(const std::string &path, const std::string &password,\n                                    const std::string &language, bool testnet)\n{\n    WalletImpl * wallet = new WalletImpl(testnet);\n    wallet->create(path, password, language);\n    return wallet;\n}\n\nWallet *WalletManagerImpl::openWallet(const std::string &path, const std::string &password, bool testnet)\n{\n    WalletImpl * wallet = new WalletImpl(testnet);\n    wallet->open(path, password);\n    return wallet;\n}\n\nWallet *WalletManagerImpl::recoveryWallet(const std::string &path, const std::string &memo, bool testnet, uint64_t restoreHeight)\n{\n    WalletImpl * wallet = new WalletImpl(testnet);\n    if(restoreHeight > 0){\n        wallet->setRefreshFromBlockHeight(restoreHeight);\n    }\n    wallet->recover(path, memo);\n    return wallet;\n}\n\nbool WalletManagerImpl::closeWallet(Wallet *wallet)\n{\n    WalletImpl * wallet_ = dynamic_cast<WalletImpl*>(wallet);\n    bool result = wallet_->close();\n    if (!result) {\n        m_errorString = wallet_->errorString();\n    } else {\n        delete wallet_;\n    }\n    return result;\n}\n\nbool WalletManagerImpl::walletExists(const std::string &path)\n{\n    return false;\n}\n\n\nstd::vector<std::string> WalletManagerImpl::findWallets(const std::string &path)\n{\n    std::vector<std::string> result;\n    boost::filesystem::path work_dir(path);\n    // return empty result if path doesn't exist\n    if(!boost::filesystem::is_directory(path)){\n      return result;\n    }\n    const boost::regex wallet_rx(\"(.*)\\\\.(keys)$\"); // searching for <wallet_name>.keys files\n    boost::filesystem::recursive_directory_iterator end_itr; // Default ctor yields past-the-end\n    for (boost::filesystem::recursive_directory_iterator itr(path); itr != end_itr; ++itr) {\n        // Skip if not a file\n        if (!boost::filesystem::is_regular_file(itr->status()))\n            continue;\n        boost::smatch what;\n        std::string filename = itr->path().filename().string();\n\n        LOG_PRINT_L3(\"Checking filename: \" << filename);\n\n        bool matched = boost::regex_match(filename, what, wallet_rx);\n        if (matched) {\n            // if keys file found, checking if there's wallet file itself\n            std::string wallet_file = (itr->path().parent_path() /= what[1].str()).string();\n            if (boost::filesystem::exists(wallet_file)) {\n                LOG_PRINT_L3(\"Found wallet: \" << wallet_file);\n                result.push_back(wallet_file);\n            }\n        }\n    }\n    return result;\n}\n\nstd::string WalletManagerImpl::errorString() const\n{\n    return m_errorString;\n}\n\nvoid WalletManagerImpl::setDaemonHost(const std::string &hostname)\n{\n\n}\n\n\n\n///////////////////// WalletManagerFactory implementation //////////////////////\nWalletManager *WalletManagerFactory::getWalletManager()\n{\n\n    static WalletManagerImpl * g_walletManager = nullptr;\n\n    if  (!g_walletManager) {\n        epee::log_space::log_singletone::add_logger(LOGGER_CONSOLE, NULL, NULL, LOG_LEVEL_MAX);\n        g_walletManager = new WalletManagerImpl();\n    }\n\n    return g_walletManager;\n}\n\nvoid WalletManagerFactory::setLogLevel(int level)\n{\n    epee::log_space::log_singletone::get_set_log_detalisation_level(true, level);\n}\n\n\n\n}\n",
  "patch": "@@ -91,7 +91,7 @@ std::vector<std::string> WalletManagerImpl::findWallets(const std::string &path)\n     boost::filesystem::path work_dir(path);\n     // return empty result if path doesn't exist\n     if(!boost::filesystem::is_directory(path)){\n-      return result;\n+        return result;\n     }\n     const boost::regex wallet_rx(\"(.*)\\\\.(keys)$\"); // searching for <wallet_name>.keys files\n     boost::filesystem::recursive_directory_iterator end_itr; // Default ctor yields past-the-end\n",
  "msg": "gratuitous whitespace changes are frowned upon (unless the previous state is dire)",
  "id": 20336,
  "y": 1
}

--- Sample 2 ---
{
  "oldf": "// Deprecate a method.\nconst deprecate = function (oldName, newName, fn) {\n  let warned = false\n  return function () {\n    if (!(warned || process.noDeprecation)) {\n      warned = true\n      deprecate.warn(oldName, newName)\n    }\n    return fn.apply(this, arguments)\n  }\n}\n\n// The method is aliases and the old method is retained for backwards compat\ndeprecate.alias = function (object, deprecatedName, existingName) {\n  let warned = false\n  const newMethod = function () {\n    if (!(warned || process.noDeprecation)) {\n      warned = true\n      deprecate.warn(deprecatedName, existingName)\n    }\n    return this[existingName].apply(this, arguments)\n  }\n  if (typeof object === 'function') {\n    object.prototype[deprecatedName] = newMethod\n  } else {\n    object[deprecatedName] = newMethod\n  }\n}\n\ndeprecate.warn = (oldName, newName) => {\n  return deprecate.log(`'${oldName}' is deprecated. Use '${newName}' instead.`)\n}\n\nlet deprecationHandler = null\n\n// Print deprecation message.\ndeprecate.log = (message) => {\n  if (typeof deprecationHandler === 'function') {\n    deprecationHandler(message)\n  } else if (process.throwDeprecation) {\n    throw new Error(message)\n  } else if (process.traceDeprecation) {\n    return console.trace(message)\n  } else {\n    return console.warn(`(electron) ${message}`)\n  }\n}\n\ndeprecate.setHandler = (handler) => {\n  deprecationHandler = handler\n}\n\ndeprecate.getHandler = () => deprecationHandler\n\n// None of the below methods are used, and so will be commented\n// out until such time that they are needed to be used and tested.\n\n// // Forward the method to member.\n// deprecate.member = (object, method, member) => {\n//   let warned = false\n//   object.prototype[method] = function () {\n//     if (!(warned || process.noDeprecation)) {\n//       warned = true\n//       deprecate.warn(method, `${member}.${method}`)\n//     }\n//     return this[member][method].apply(this[member], arguments)\n//   }\n// }\n//\n// // Deprecate a property.\n// deprecate.property = (object, property, method) => {\n//   return Object.defineProperty(object, property, {\n//     get: function () {\n//       let warned = false\n//       if (!(warned || process.noDeprecation)) {\n//         warned = true\n//         deprecate.warn(`${property} property`, `${method} method`)\n//       }\n//       return this[method]()\n//     }\n//   })\n// }\n//\n// // Deprecate an event.\n// deprecate.event = (emitter, oldName, newName, fn) => {\n//   let warned = false\n//   return emitter.on(newName, function (...args) {\n//     if (this.listenerCount(oldName) > 0) {\n//       if (!(warned || process.noDeprecation)) {\n//         warned = true\n//         deprecate.warn(`'${oldName}' event`, `'${newName}' event`)\n//       }\n//       if (fn != null) {\n//         fn.apply(this, arguments)\n//       } else {\n//         this.emit.apply(this, [oldName].concat(args))\n//       }\n//     }\n//   })\n// }\n\nmodule.exports = deprecate\n",
  "patch": "@@ -46,6 +46,26 @@ deprecate.log = (message) => {\n   }\n }\n \n+// Deprecate an event.\n+deprecate.event = (emitter, oldName, newName, fn) => {\n+  let warned = false\n+  return emitter.on(newName, function (...args) {\n+    // There are no listeners for this event\n+    if (this.listenerCount(oldName) === 0) { return }\n+    // noDeprecation set or if user has already been warned\n+    if (warned || process.noDeprecation) { return }\n+    warned = true\n+    // Event name starts with a -, which means it's an internal event.\n+    // For deprecation, that means the event cannot be used anymore.\n+    if (!newName.startsWith('-')) { deprecate.warn(`'${oldName}' event`, `'${newName}' event`) } else { deprecate.log(`'${oldName}' event has been deprecated.`) }\n+    if (fn != null) {\n+      fn.apply(this, arguments)\n+    } else {\n+      this.emit.apply(this, [oldName].concat(args))\n+    }\n+  })\n+}\n+\n deprecate.setHandler = (handler) => {\n   deprecationHandler = handler\n }\n",
  "msg": ":-( Linter fix made this line long.",
  "id": 85038,
  "y": 1
}

--- Sample 3 ---
{
  "oldf": "",
  "patch": "@@ -0,0 +1,95 @@\n+/*\n+ * Copyright (c) MuleSoft, Inc.  All rights reserved.  http://www.mulesoft.com\n+ * The software in this package is published under the terms of the CPAL v1.0\n+ * license, a copy of which has been included with this distribution in the\n+ * LICENSE.txt file.\n+ */\n+\n+package org.mule.routing;\n+\n+import org.mule.api.DefaultMuleException;\n+import org.mule.api.ExceptionPayload;\n+import org.mule.api.MuleEvent;\n+import org.mule.api.MuleException;\n+import org.mule.api.config.MuleProperties;\n+import org.mule.api.routing.RouterResultsHandler;\n+import org.mule.config.i18n.Message;\n+import org.mule.config.i18n.MessageFactory;\n+import org.mule.message.DefaultExceptionPayload;\n+\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Default implementation of {@link EventMergeStrategy} for the\n+ * {@link ScatterGatherRouter}. It merges the events by using a\n+ * {@link DefaultRouterResultsHandler} and throws a {@link CompositeRoutingException}\n+ * in case of failure\n+ * \n+ * @since 3.5.0\n+ */\n+public class ScatterGatherEventMergeStrategy implements EventMergeStrategy\n+{\n+\n+    private RouterResultsHandler resultsHandler = new DefaultRouterResultsHandler();\n+\n+    @Override\n+    public MuleEvent merge(MuleEvent originalEvent, List<MuleEvent> events) throws MuleException\n+    {\n+        MuleEvent response = this.resultsHandler.aggregateResults(events, originalEvent,\n+            originalEvent.getMuleContext());\n+\n+        Map<Integer, Exception> exceptions = this.collectExceptions(events);\n+        if (exceptions.isEmpty())\n+        {\n+            return response;\n+        }\n+        else\n+        {\n+            originalEvent.getMessage().setPayload(response.getMessage().getPayload());\n+\n+            MuleException exception = new CompositeRoutingException(this.buildExceptionMessage(exceptions),\n+                originalEvent, exceptions);\n+            originalEvent.getMessage().setExceptionPayload(new DefaultExceptionPayload(exception));\n+\n+            throw exception;\n+        }\n+    }\n+\n+    private Message buildExceptionMessage(Map<Integer, Exception> exceptions)\n+    {\n+        StringBuilder builder = new StringBuilder();\n+        for (Integer route : exceptions.keySet())\n+        {\n+            if (builder.length() > 0)\n+            {\n+                builder.append(\", \");\n+            }\n+\n+            builder.append(route);\n+        }\n+\n+        builder.insert(0, \"Exception was found for route(s): \");\n+        return MessageFactory.createStaticMessage(builder.toString());\n+    }\n+\n+    private Map<Integer, Exception> collectExceptions(List<MuleEvent> events)\n+    {\n+        Map<Integer, Exception> exceptions = new LinkedHashMap<Integer, Exception>(events.size());\n+        for (MuleEvent event : events)\n+        {\n+            ExceptionPayload ep = event.getMessage().getExceptionPayload();\n+            if (ep != null && ep.getException() != null)\n+            {\n+                Integer routeIndex = (Integer) ep.getInfo().get(\n+                    MuleProperties.MULE_CORRELATION_SEQUENCE_PROPERTY);\n+                Throwable t = ep.getException();\n+                Exception e = t instanceof Exception ? (Exception) t : new DefaultMuleException(t);\n+                exceptions.put(routeIndex, e);\n+            }\n+        }\n+\n+        return exceptions;\n+    }\n+}\n",
  "msg": "This names should really give me an idea of the behaviour to expect, shouldn't it?",
  "id": 47316,
  "y": 1
}

--- Sample 4 ---
{
  "oldf": "# borg cli interface / toplevel archiver code\n\nimport sys\nimport traceback\n\ntry:\n    import argparse\n    import collections\n    import configparser\n    import faulthandler\n    import functools\n    import hashlib\n    import inspect\n    import itertools\n    import json\n    import logging\n    import os\n    import re\n    import shlex\n    import shutil\n    import signal\n    import stat\n    import subprocess\n    import tarfile\n    import textwrap\n    import time\n    from binascii import unhexlify\n    from contextlib import contextmanager\n    from datetime import datetime, timedelta\n\n    from .logger import create_logger, setup_logging\n\n    logger = create_logger()\n\n    import borg\n    from . import __version__\n    from . import helpers\n    from .algorithms.checksums import crc32\n    from .archive import Archive, ArchiveChecker, ArchiveRecreater, Statistics, is_special\n    from .archive import BackupError, BackupOSError, backup_io, OsOpen, stat_update_check\n    from .archive import FilesystemObjectProcessors, MetadataCollector, ChunksProcessor\n    from .archive import has_link\n    from .cache import Cache, assert_secure, SecurityManager\n    from .constants import *  # NOQA\n    from .compress import CompressionSpec\n    from .crypto.key import key_creator, key_argument_names, tam_required_file, tam_required, RepoKey, PassphraseKey\n    from .crypto.keymanager import KeyManager\n    from .helpers import EXIT_SUCCESS, EXIT_WARNING, EXIT_ERROR\n    from .helpers import Error, NoManifestError, set_ec\n    from .helpers import positive_int_validator, location_validator, archivename_validator, ChunkerParams, Location\n    from .helpers import PrefixSpec, GlobSpec, CommentSpec, SortBySpec, FilesCacheMode\n    from .helpers import BaseFormatter, ItemFormatter, ArchiveFormatter\n    from .helpers import format_timedelta, format_file_size, parse_file_size, format_archive\n    from .helpers import safe_encode, remove_surrogates, bin_to_hex, prepare_dump_dict\n    from .helpers import interval, prune_within, prune_split, PRUNING_PATTERNS\n    from .helpers import timestamp\n    from .helpers import get_cache_dir, os_stat\n    from .helpers import Manifest, AI_HUMAN_SORT_KEYS\n    from .helpers import hardlinkable\n    from .helpers import StableDict\n    from .helpers import check_python, check_extension_modules\n    from .helpers import dir_is_tagged, is_slow_msgpack, is_supported_msgpack, yes, sysinfo\n    from .helpers import log_multi\n    from .helpers import signal_handler, raising_signal_handler, SigHup, SigTerm\n    from .helpers import ErrorIgnoringTextIOWrapper\n    from .helpers import ProgressIndicatorPercent\n    from .helpers import basic_json_data, json_print\n    from .helpers import replace_placeholders\n    from .helpers import ChunkIteratorFileWrapper\n    from .helpers import popen_with_error_handling, prepare_subprocess_env\n    from .helpers import dash_open\n    from .helpers import umount\n    from .helpers import flags_root, flags_dir, flags_special_follow, flags_special\n    from .helpers import msgpack\n    from .helpers import sig_int\n    from .nanorst import rst_to_terminal\n    from .patterns import ArgparsePatternAction, ArgparseExcludeFileAction, ArgparsePatternFileAction, parse_exclude_pattern\n    from .patterns import PatternMatcher\n    from .item import Item\n    from .platform import get_flags, get_process_id, SyncFile\n    from .remote import RepositoryServer, RemoteRepository, cache_if_remote\n    from .repository import Repository, LIST_SCAN_LIMIT, TAG_PUT, TAG_DELETE, TAG_COMMIT\n    from .selftest import selftest\n    from .upgrader import AtticRepositoryUpgrader, BorgRepositoryUpgrader\nexcept BaseException:\n    # an unhandled exception in the try-block would cause the borg cli command to exit with rc 1 due to python's\n    # default behavior, see issue #4424.\n    # as borg defines rc 1 as WARNING, this would be a mismatch, because a crash should be an ERROR (rc 2).\n    traceback.print_exc()\n    sys.exit(2)  # == EXIT_ERROR\n\nassert EXIT_ERROR == 2, \"EXIT_ERROR is not 2, as expected - fix assert AND exception handler right above this line.\"\n\n\nSTATS_HEADER = \"                       Original size      Compressed size    Deduplicated size\"\n\nPURE_PYTHON_MSGPACK_WARNING = \"Using a pure-python msgpack! This will result in lower performance.\"\n\n\ndef argument(args, str_or_bool):\n    \"\"\"If bool is passed, return it. If str is passed, retrieve named attribute from args.\"\"\"\n    if isinstance(str_or_bool, str):\n        return getattr(args, str_or_bool)\n    if isinstance(str_or_bool, (list, tuple)):\n        return any(getattr(args, item) for item in str_or_bool)\n    return str_or_bool\n\n\ndef with_repository(fake=False, invert_fake=False, create=False, lock=True,\n                    exclusive=False, manifest=True, cache=False, secure=True,\n                    compatibility=None):\n    \"\"\"\n    Method decorator for subcommand-handling methods: do_XYZ(self, args, repository, )\n\n    If a parameter (where allowed) is a str the attribute named of args is used instead.\n    :param fake: (str or bool) use None instead of repository, don't do anything else\n    :param create: create repository\n    :param lock: lock repository\n    :param exclusive: (str or bool) lock repository exclusively (for writing)\n    :param manifest: load manifest and key, pass them as keyword arguments\n    :param cache: open cache, pass it as keyword argument (implies manifest)\n    :param secure: do assert_secure after loading manifest\n    :param compatibility: mandatory if not create and (manifest or cache), specifies mandatory feature categories to check\n    \"\"\"\n\n    if not create and (manifest or cache):\n        if compatibility is None:\n            raise AssertionError(\"with_repository decorator used without compatibility argument\")\n        if type(compatibility) is not tuple:\n            raise AssertionError(\"with_repository decorator compatibility argument must be of type tuple\")\n    else:\n        if compatibility is not None:\n            raise AssertionError(\"with_repository called with compatibility argument but would not check\" + repr(compatibility))\n        if create:\n            compatibility = Manifest.NO_OPERATION_CHECK\n\n    def decorator(method):\n        @functools.wraps(method)\n        def wrapper(self, args, **kwargs):\n            location = args.location  # note: 'location' must be always present in args\n            append_only = getattr(args, 'append_only', False)\n            storage_quota = getattr(args, 'storage_quota', None)\n            make_parent_dirs = getattr(args, 'make_parent_dirs', False)\n            if argument(args, fake) ^ invert_fake:\n                return method(self, args, repository=None, **kwargs)\n            elif location.proto == 'ssh':\n                repository = RemoteRepository(location, create=create, exclusive=argument(args, exclusive),\n                                              lock_wait=self.lock_wait, lock=lock, append_only=append_only,\n                                              make_parent_dirs=make_parent_dirs, args=args)\n            else:\n                repository = Repository(location.path, create=create, exclusive=argument(args, exclusive),\n                                        lock_wait=self.lock_wait, lock=lock, append_only=append_only,\n                                        storage_quota=storage_quota, make_parent_dirs=make_parent_dirs)\n            with repository:\n                if manifest or cache:\n                    kwargs['manifest'], kwargs['key'] = Manifest.load(repository, compatibility)\n                    if 'compression' in args:\n                        kwargs['key'].compressor = args.compression.compressor\n                    if secure:\n                        assert_secure(repository, kwargs['manifest'], self.lock_wait)\n                if cache:\n                    with Cache(repository, kwargs['key'], kwargs['manifest'],\n                               progress=getattr(args, 'progress', False), lock_wait=self.lock_wait,\n                               cache_mode=getattr(args, 'files_cache_mode', DEFAULT_FILES_CACHE_MODE),\n                               consider_part_files=getattr(args, 'consider_part_files', False)) as cache_:\n                        return method(self, args, repository=repository, cache=cache_, **kwargs)\n                else:\n                    return method(self, args, repository=repository, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef with_archive(method):\n    @functools.wraps(method)\n    def wrapper(self, args, repository, key, manifest, **kwargs):\n        archive = Archive(repository, key, manifest, args.location.archive,\n                          numeric_owner=getattr(args, 'numeric_owner', False),\n                          nobsdflags=getattr(args, 'nobsdflags', False),\n                          cache=kwargs.get('cache'),\n                          consider_part_files=args.consider_part_files, log_json=args.log_json)\n        return method(self, args, repository=repository, manifest=manifest, key=key, archive=archive, **kwargs)\n    return wrapper\n\n\ndef parse_storage_quota(storage_quota):\n    parsed = parse_file_size(storage_quota)\n    if parsed < parse_file_size('10M'):\n        raise argparse.ArgumentTypeError('quota is too small (%s). At least 10M are required.' % storage_quota)\n    return parsed\n\n\ndef get_func(args):\n    # This works around http://bugs.python.org/issue9351\n    # func is used at the leaf parsers of the argparse parser tree,\n    # fallback_func at next level towards the root,\n    # fallback2_func at the 2nd next level (which is root in our case).\n    for name in 'func', 'fallback_func', 'fallback2_func':\n        func = getattr(args, name, None)\n        if func is not None:\n            return func\n    raise Exception('expected func attributes not found')\n\n\nclass Archiver:\n\n    def __init__(self, lock_wait=None, prog=None):\n        self.exit_code = EXIT_SUCCESS\n        self.lock_wait = lock_wait\n        self.prog = prog\n\n    def print_error(self, msg, *args):\n        msg = args and msg % args or msg\n        self.exit_code = EXIT_ERROR\n        logger.error(msg)\n\n    def print_warning(self, msg, *args):\n        msg = args and msg % args or msg\n        self.exit_code = EXIT_WARNING  # we do not terminate here, so it is a warning\n        logger.warning(msg)\n\n    def print_file_status(self, status, path):\n        if self.output_list and (self.output_filter is None or status in self.output_filter):\n            if self.log_json:\n                print(json.dumps({\n                    'type': 'file_status',\n                    'status': status,\n                    'path': remove_surrogates(path),\n                }), file=sys.stderr)\n            else:\n                logging.getLogger('borg.output.list').info(\"%1s %s\", status, remove_surrogates(path))\n\n    @staticmethod\n    def build_matcher(inclexcl_patterns, include_paths):\n        matcher = PatternMatcher()\n        matcher.add_inclexcl(inclexcl_patterns)\n        matcher.add_includepaths(include_paths)\n        return matcher\n\n    def do_serve(self, args):\n        \"\"\"Start in server mode. This command is usually not used manually.\"\"\"\n        RepositoryServer(\n            restrict_to_paths=args.restrict_to_paths,\n            restrict_to_repositories=args.restrict_to_repositories,\n            append_only=args.append_only,\n            storage_quota=args.storage_quota,\n        ).serve()\n        return EXIT_SUCCESS\n\n    @with_repository(create=True, exclusive=True, manifest=False)\n    def do_init(self, args, repository):\n        \"\"\"Initialize an empty repository\"\"\"\n        path = args.location.canonical_path()\n        logger.info('Initializing repository at \"%s\"' % path)\n        try:\n            key = key_creator(repository, args)\n        except (EOFError, KeyboardInterrupt):\n            repository.destroy()\n            return EXIT_WARNING\n        manifest = Manifest(key, repository)\n        manifest.key = key\n        manifest.write()\n        repository.commit(compact=False)\n        with Cache(repository, key, manifest, warn_if_unencrypted=False):\n            pass\n        if key.tam_required:\n            tam_file = tam_required_file(repository)\n            open(tam_file, 'w').close()\n            logger.warning(\n                '\\n'\n                'By default repositories initialized with this version will produce security\\n'\n                'errors if written to with an older version (up to and including Borg 1.0.8).\\n'\n                '\\n'\n                'If you want to use these older versions, you can disable the check by running:\\n'\n                'borg upgrade --disable-tam %s\\n'\n                '\\n'\n                'See https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability '\n                'for details about the security implications.', shlex.quote(path))\n\n        if key.NAME != 'plaintext':\n            if 'repokey' in key.NAME:\n                logger.warning(\n                    '\\n'\n                    'IMPORTANT: you will need both KEY AND PASSPHRASE to access this repo!\\n'\n                    'The key is included in the repository config, but should be backed up in case the repository gets corrupted.\\n'\n                    'Use \"borg key export\" to export the key, optionally in printable format.\\n'\n                    'Write down the passphrase. Store both at safe place(s).\\n')\n            else:\n                logger.warning(\n                    '\\n'\n                    'IMPORTANT: you will need both KEY AND PASSPHRASE to access this repo!\\n'\n                    'Use \"borg key export\" to export the key, optionally in printable format.\\n'\n                    'Write down the passphrase. Store both at safe place(s).\\n')\n        return self.exit_code\n\n    @with_repository(exclusive=True, manifest=False)\n    def do_check(self, args, repository):\n        \"\"\"Check repository consistency\"\"\"\n        if args.repair:\n            msg = (\"'check --repair' is an experimental feature that might result in data loss.\" +\n                   \"\\n\" +\n                   \"Type 'YES' if you understand this and want to continue: \")\n            if not yes(msg, false_msg=\"Aborting.\", invalid_msg=\"Invalid answer, aborting.\",\n                       truish=('YES', ), retry=False,\n                       env_var_override='BORG_CHECK_I_KNOW_WHAT_I_AM_DOING'):\n                return EXIT_ERROR\n        if args.repo_only and any(\n           (args.verify_data, args.first, args.last, args.prefix is not None, args.glob_archives)):\n            self.print_error(\"--repository-only contradicts --first, --last, --prefix and --verify-data arguments.\")\n            return EXIT_ERROR\n        if args.repair and args.max_duration:\n            self.print_error(\"--repair does not allow --max-duration argument.\")\n            return EXIT_ERROR\n        if args.max_duration and not args.repo_only:\n            # when doing a partial repo check, we can only check crc32 checksums in segment files,\n            # we can't build a fresh repo index in memory to verify the on-disk index against it.\n            # thus, we should not do an archives check based on a unknown-quality on-disk repo index.\n            # also, there is no max_duration support in the archives check code anyway.\n            self.print_error(\"--repository-only is required for --max-duration support.\")\n            return EXIT_ERROR\n        if not args.archives_only:\n            if not repository.check(repair=args.repair, save_space=args.save_space, max_duration=args.max_duration):\n                return EXIT_WARNING\n        if args.prefix is not None:\n            args.glob_archives = args.prefix + '*'\n        if not args.repo_only and not ArchiveChecker().check(\n                repository, repair=args.repair, archive=args.location.archive,\n                first=args.first, last=args.last, sort_by=args.sort_by or 'ts', glob=args.glob_archives,\n                verify_data=args.verify_data, save_space=args.save_space):\n            return EXIT_WARNING\n        return EXIT_SUCCESS\n\n    @with_repository(compatibility=(Manifest.Operation.CHECK,))\n    def do_change_passphrase(self, args, repository, manifest, key):\n        \"\"\"Change repository key file passphrase\"\"\"\n        if not hasattr(key, 'change_passphrase'):\n            print('This repository is not encrypted, cannot change the passphrase.')\n            return EXIT_ERROR\n        key.change_passphrase()\n        logger.info('Key updated')\n        if hasattr(key, 'find_key'):\n            # print key location to make backing it up easier\n            logger.info('Key location: %s', key.find_key())\n        return EXIT_SUCCESS\n\n    @with_repository(lock=False, exclusive=False, manifest=False, cache=False)\n    def do_key_export(self, args, repository):\n        \"\"\"Export the repository key for backup\"\"\"\n        manager = KeyManager(repository)\n        manager.load_keyblob()\n        if args.paper:\n            manager.export_paperkey(args.path)\n        else:\n            if not args.path:\n                self.print_error(\"output file to export key to expected\")\n                return EXIT_ERROR\n            try:\n                if args.qr:\n                    manager.export_qr(args.path)\n                else:\n                    manager.export(args.path)\n            except IsADirectoryError:\n                self.print_error(\"'{}' must be a file, not a directory\".format(args.path))\n                return EXIT_ERROR\n        return EXIT_SUCCESS\n\n    @with_repository(lock=False, exclusive=False, manifest=False, cache=False)\n    def do_key_import(self, args, repository):\n        \"\"\"Import the repository key from backup\"\"\"\n        manager = KeyManager(repository)\n        if args.paper:\n            if args.path:\n                self.print_error(\"with --paper import from file is not supported\")\n                return EXIT_ERROR\n            manager.import_paperkey(args)\n        else:\n            if not args.path:\n                self.print_error(\"input file to import key from expected\")\n                return EXIT_ERROR\n            if args.path != '-' and not os.path.exists(args.path):\n                self.print_error(\"input file does not exist: \" + args.path)\n                return EXIT_ERROR\n            manager.import_keyfile(args)\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False)\n    def do_migrate_to_repokey(self, args, repository):\n        \"\"\"Migrate passphrase -> repokey\"\"\"\n        manifest_data = repository.get(Manifest.MANIFEST_ID)\n        key_old = PassphraseKey.detect(repository, manifest_data)\n        key_new = RepoKey(repository)\n        key_new.target = repository\n        key_new.repository_id = repository.id\n        key_new.enc_key = key_old.enc_key\n        key_new.enc_hmac_key = key_old.enc_hmac_key\n        key_new.id_key = key_old.id_key\n        key_new.chunk_seed = key_old.chunk_seed\n        key_new.change_passphrase()  # option to change key protection passphrase, save\n        logger.info('Key updated')\n        return EXIT_SUCCESS\n\n    def do_benchmark_crud(self, args):\n        \"\"\"Benchmark Create, Read, Update, Delete for archives.\"\"\"\n        def measurement_run(repo, path):\n            archive = repo + '::borg-benchmark-crud'\n            compression = '--compression=none'\n            # measure create perf (without files cache to always have it chunking)\n            t_start = time.monotonic()\n            rc = self.do_create(self.parse_args(['create', compression, '--files-cache=disabled', archive + '1', path]))\n            t_end = time.monotonic()\n            dt_create = t_end - t_start\n            assert rc == 0\n            # now build files cache\n            rc1 = self.do_create(self.parse_args(['create', compression, archive + '2', path]))\n            rc2 = self.do_delete(self.parse_args(['delete', archive + '2']))\n            assert rc1 == rc2 == 0\n            # measure a no-change update (archive1 is still present)\n            t_start = time.monotonic()\n            rc1 = self.do_create(self.parse_args(['create', compression, archive + '3', path]))\n            t_end = time.monotonic()\n            dt_update = t_end - t_start\n            rc2 = self.do_delete(self.parse_args(['delete', archive + '3']))\n            assert rc1 == rc2 == 0\n            # measure extraction (dry-run: without writing result to disk)\n            t_start = time.monotonic()\n            rc = self.do_extract(self.parse_args(['extract', '--dry-run', archive + '1']))\n            t_end = time.monotonic()\n            dt_extract = t_end - t_start\n            assert rc == 0\n            # measure archive deletion (of LAST present archive with the data)\n            t_start = time.monotonic()\n            rc = self.do_delete(self.parse_args(['delete', archive + '1']))\n            t_end = time.monotonic()\n            dt_delete = t_end - t_start\n            assert rc == 0\n            return dt_create, dt_update, dt_extract, dt_delete\n\n        @contextmanager\n        def test_files(path, count, size, random):\n            path = os.path.join(path, 'borg-test-data')\n            os.makedirs(path)\n            for i in range(count):\n                fname = os.path.join(path, 'file_%d' % i)\n                data = b'\\0' * size if not random else os.urandom(size)\n                with SyncFile(fname, binary=True) as fd:  # used for posix_fadvise's sake\n                    fd.write(data)\n            yield path\n            shutil.rmtree(path)\n\n        if '_BORG_BENCHMARK_CRUD_TEST' in os.environ:\n            tests = [\n                ('Z-TEST', 1, 1, False),\n                ('R-TEST', 1, 1, True),\n            ]\n        else:\n            tests = [\n                ('Z-BIG', 10, 100000000, False),\n                ('R-BIG', 10, 100000000, True),\n                ('Z-MEDIUM', 1000, 1000000, False),\n                ('R-MEDIUM', 1000, 1000000, True),\n                ('Z-SMALL', 10000, 10000, False),\n                ('R-SMALL', 10000, 10000, True),\n            ]\n\n        for msg, count, size, random in tests:\n            with test_files(args.path, count, size, random) as path:\n                dt_create, dt_update, dt_extract, dt_delete = measurement_run(args.location.canonical_path(), path)\n            total_size_MB = count * size / 1e06\n            file_size_formatted = format_file_size(size)\n            content = 'random' if random else 'all-zero'\n            fmt = '%s-%-10s %9.2f MB/s (%d * %s %s files: %.2fs)'\n            print(fmt % ('C', msg, total_size_MB / dt_create, count, file_size_formatted, content, dt_create))\n            print(fmt % ('R', msg, total_size_MB / dt_extract, count, file_size_formatted, content, dt_extract))\n            print(fmt % ('U', msg, total_size_MB / dt_update, count, file_size_formatted, content, dt_update))\n            print(fmt % ('D', msg, total_size_MB / dt_delete, count, file_size_formatted, content, dt_delete))\n\n        return 0\n\n    @with_repository(fake='dry_run', exclusive=True, compatibility=(Manifest.Operation.WRITE,))\n    def do_create(self, args, repository, manifest=None, key=None):\n        \"\"\"Create new archive\"\"\"\n        matcher = PatternMatcher(fallback=True)\n        matcher.add_inclexcl(args.patterns)\n\n        def create_inner(archive, cache, fso):\n            # Add cache dir to inode_skip list\n            skip_inodes = set()\n            try:\n                st = os.stat(get_cache_dir())\n                skip_inodes.add((st.st_ino, st.st_dev))\n            except OSError:\n                pass\n            # Add local repository dir to inode_skip list\n            if not args.location.host:\n                try:\n                    st = os.stat(args.location.path)\n                    skip_inodes.add((st.st_ino, st.st_dev))\n                except OSError:\n                    pass\n            logger.debug('Processing files ...')\n            for path in args.paths:\n                if path == '-':  # stdin\n                    path = args.stdin_name\n                    if not dry_run:\n                        try:\n                            status = fso.process_stdin(path=path, cache=cache)\n                        except BackupOSError as e:\n                            status = 'E'\n                            self.print_warning('%s: %s', path, e)\n                    else:\n                        status = '-'\n                    self.print_file_status(status, path)\n                    continue\n                path = os.path.normpath(path)\n                parent_dir = os.path.dirname(path) or '.'\n                name = os.path.basename(path)\n                # note: for path == '/':  name == '' and parent_dir == '/'.\n                # the empty name will trigger a fall-back to path-based processing in os_stat and os_open.\n                with OsOpen(path=parent_dir, flags=flags_root, noatime=True, op='open_root') as parent_fd:\n                    try:\n                        st = os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False)\n                    except OSError as e:\n                        self.print_warning('%s: %s', path, e)\n                        continue\n                    if args.one_file_system:\n                        restrict_dev = st.st_dev\n                    else:\n                        restrict_dev = None\n                    self._process(path=path, parent_fd=parent_fd, name=name,\n                                  fso=fso, cache=cache, matcher=matcher,\n                                  exclude_caches=args.exclude_caches, exclude_if_present=args.exclude_if_present,\n                                  keep_exclude_tags=args.keep_exclude_tags, skip_inodes=skip_inodes,\n                                  restrict_dev=restrict_dev, read_special=args.read_special, dry_run=dry_run)\n            if not dry_run:\n                if args.progress:\n                    archive.stats.show_progress(final=True)\n                archive.stats += fso.stats\n                if sig_int:\n                    # do not save the archive if the user ctrl-c-ed - it is valid, but incomplete.\n                    # we already have a checkpoint archive in this case.\n                    self.print_error(\"Got Ctrl-C / SIGINT.\")\n                else:\n                    archive.save(comment=args.comment, timestamp=args.timestamp, stats=archive.stats)\n                args.stats |= args.json\n                if args.stats:\n                    if args.json:\n                        json_print(basic_json_data(manifest, cache=cache, extra={\n                            'archive': archive,\n                        }))\n                    else:\n                        log_multi(DASHES,\n                                  str(archive),\n                                  DASHES,\n                                  STATS_HEADER,\n                                  str(archive.stats),\n                                  str(cache),\n                                  DASHES, logger=logging.getLogger('borg.output.stats'))\n\n        self.output_filter = args.output_filter\n        self.output_list = args.output_list\n        self.nobsdflags = args.nobsdflags\n        self.exclude_nodump = args.exclude_nodump\n        dry_run = args.dry_run\n        t0 = datetime.utcnow()\n        t0_monotonic = time.monotonic()\n        logger.info('Creating archive at \"%s\"' % args.location.orig)\n        if not dry_run:\n            with Cache(repository, key, manifest, progress=args.progress,\n                       lock_wait=self.lock_wait, permit_adhoc_cache=args.no_cache_sync,\n                       cache_mode=args.files_cache_mode) as cache:\n                archive = Archive(repository, key, manifest, args.location.archive, cache=cache,\n                                  create=True, checkpoint_interval=args.checkpoint_interval,\n                                  numeric_owner=args.numeric_owner, noatime=not args.atime, noctime=args.noctime,\n                                  progress=args.progress,\n                                  chunker_params=args.chunker_params, start=t0, start_monotonic=t0_monotonic,\n                                  log_json=args.log_json)\n                metadata_collector = MetadataCollector(noatime=not args.atime, noctime=args.noctime,\n                    nobsdflags=args.nobsdflags, numeric_owner=args.numeric_owner, nobirthtime=args.nobirthtime)\n                cp = ChunksProcessor(cache=cache, key=key,\n                    add_item=archive.add_item, write_checkpoint=archive.write_checkpoint,\n                    checkpoint_interval=args.checkpoint_interval, rechunkify=False)\n                fso = FilesystemObjectProcessors(metadata_collector=metadata_collector, cache=cache, key=key,\n                    process_file_chunks=cp.process_file_chunks, add_item=archive.add_item,\n                    chunker_params=args.chunker_params, show_progress=args.progress)\n                create_inner(archive, cache, fso)\n        else:\n            create_inner(None, None, None)\n        return self.exit_code\n\n    def _process(self, *, path, parent_fd=None, name=None,\n                 fso, cache, matcher,\n                 exclude_caches, exclude_if_present, keep_exclude_tags, skip_inodes,\n                 restrict_dev, read_special=False, dry_run=False):\n        \"\"\"\n        Process *path* (or, preferably, parent_fd/name) recursively according to the various parameters.\n\n        This should only raise on critical errors. Per-item errors must be handled within this method.\n        \"\"\"\n        if sig_int and sig_int.action_done():\n            # the user says \"get out of here!\" and we have already completed the desired action.\n            return\n\n        try:\n            recurse_excluded_dir = False\n            if matcher.match(path):\n                with backup_io('stat'):\n                    st = os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False)\n            else:\n                self.print_file_status('x', path)\n                # get out here as quickly as possible:\n                # we only need to continue if we shall recurse into an excluded directory.\n                # if we shall not recurse, then do not even touch (stat()) the item, it\n                # could trigger an error, e.g. if access is forbidden, see #3209.\n                if not matcher.recurse_dir:\n                    return\n                with backup_io('stat'):\n                    st = os_stat(path=path, parent_fd=parent_fd, name=name, follow_symlinks=False)\n                recurse_excluded_dir = stat.S_ISDIR(st.st_mode)\n                if not recurse_excluded_dir:\n                    return\n\n            if (st.st_ino, st.st_dev) in skip_inodes:\n                return\n            # if restrict_dev is given, we do not want to recurse into a new filesystem,\n            # but we WILL save the mountpoint directory (or more precise: the root\n            # directory of the mounted filesystem that shadows the mountpoint dir).\n            recurse = restrict_dev is None or st.st_dev == restrict_dev\n            status = None\n            if self.exclude_nodump:\n                # Ignore if nodump flag is set\n                with backup_io('flags'):\n                    if get_flags(path=path, st=st) & stat.UF_NODUMP:\n                        self.print_file_status('x', path)\n                        return\n            if stat.S_ISREG(st.st_mode):\n                if not dry_run:\n                    status = fso.process_file(path=path, parent_fd=parent_fd, name=name, st=st, cache=cache)\n            elif stat.S_ISDIR(st.st_mode):\n                with OsOpen(path=path, parent_fd=parent_fd, name=name, flags=flags_dir,\n                            noatime=True, op='dir_open') as child_fd:\n                    # child_fd is None for directories on windows, in that case a race condition check is not possible.\n                    if child_fd is not None:\n                        with backup_io('fstat'):\n                            st = stat_update_check(st, os.fstat(child_fd))\n                    if recurse:\n                        tag_names = dir_is_tagged(path, exclude_caches, exclude_if_present)\n                        if tag_names:\n                            # if we are already recursing in an excluded dir, we do not need to do anything else than\n                            # returning (we do not need to archive or recurse into tagged directories), see #3991:\n                            if not recurse_excluded_dir:\n                                if keep_exclude_tags and not dry_run:\n                                    fso.process_dir(path=path, fd=child_fd, st=st)\n                                    for tag_name in tag_names:\n                                        tag_path = os.path.join(path, tag_name)\n                                        self._process(path=tag_path, parent_fd=child_fd, name=tag_name,\n                                                      fso=fso, cache=cache, matcher=matcher,\n                                                      exclude_caches=exclude_caches, exclude_if_present=exclude_if_present,\n                                                      keep_exclude_tags=keep_exclude_tags, skip_inodes=skip_inodes,\n                                                      restrict_dev=restrict_dev, read_special=read_special, dry_run=dry_run)\n                                self.print_file_status('x', path)\n                            return\n                    if not dry_run:\n                        if not recurse_excluded_dir:\n                            status = fso.process_dir(path=path, fd=child_fd, st=st)\n                    if recurse:\n                        with backup_io('scandir'):\n                            entries = helpers.scandir_inorder(path=path, fd=child_fd)\n                        for dirent in entries:\n                            normpath = os.path.normpath(os.path.join(path, dirent.name))\n                            self._process(path=normpath, parent_fd=child_fd, name=dirent.name,\n                                          fso=fso, cache=cache, matcher=matcher,\n                                          exclude_caches=exclude_caches, exclude_if_present=exclude_if_present,\n                                          keep_exclude_tags=keep_exclude_tags, skip_inodes=skip_inodes,\n                                          restrict_dev=restrict_dev, read_special=read_special, dry_run=dry_run)\n            elif stat.S_ISLNK(st.st_mode):\n                if not dry_run:\n                    if not read_special:\n                        status = fso.process_symlink(path=path, parent_fd=parent_fd, name=name, st=st)\n                    else:\n                        try:\n                            st_target = os.stat(name, dir_fd=parent_fd, follow_symlinks=True)\n                        except OSError:\n                            special = False\n                        else:\n                            special = is_special(st_target.st_mode)\n                        if special:\n                            status = fso.process_file(path=path, parent_fd=parent_fd, name=name, st=st_target,\n                                                      cache=cache, flags=flags_special_follow)\n                        else:\n                            status = fso.process_symlink(path=path, parent_fd=parent_fd, name=name, st=st)\n            elif stat.S_ISFIFO(st.st_mode):\n                if not dry_run:\n                    if not read_special:\n                        status = fso.process_fifo(path=path, parent_fd=parent_fd, name=name, st=st)\n                    else:\n                        status = fso.process_file(path=path, parent_fd=parent_fd, name=name, st=st,\n                                                  cache=cache, flags=flags_special)\n            elif stat.S_ISCHR(st.st_mode):\n                if not dry_run:\n                    if not read_special:\n                        status = fso.process_dev(path=path, parent_fd=parent_fd, name=name, st=st, dev_type='c')\n                    else:\n                        status = fso.process_file(path=path, parent_fd=parent_fd, name=name, st=st,\n                                                  cache=cache, flags=flags_special)\n            elif stat.S_ISBLK(st.st_mode):\n                if not dry_run:\n                    if not read_special:\n                        status = fso.process_dev(path=path, parent_fd=parent_fd, name=name, st=st, dev_type='b')\n                    else:\n                        status = fso.process_file(path=path, parent_fd=parent_fd, name=name, st=st,\n                                                  cache=cache, flags=flags_special)\n            elif stat.S_ISSOCK(st.st_mode):\n                # Ignore unix sockets\n                return\n            elif stat.S_ISDOOR(st.st_mode):\n                # Ignore Solaris doors\n                return\n            elif stat.S_ISPORT(st.st_mode):\n                # Ignore Solaris event ports\n                return\n            else:\n                self.print_warning('Unknown file type: %s', path)\n                return\n        except (BackupOSError, BackupError) as e:\n            self.print_warning('%s: %s', path, e)\n            status = 'E'\n        if status == 'C':\n            self.print_warning('%s: file changed while we backed it up', path)\n        # Status output\n        if status is None:\n            if not dry_run:\n                status = '?'  # need to add a status code somewhere\n            else:\n                status = '-'  # dry run, item was not backed up\n\n        if not recurse_excluded_dir:\n            self.print_file_status(status, path)\n\n    @staticmethod\n    def build_filter(matcher, peek_and_store_hardlink_masters, strip_components):\n        if strip_components:\n            def item_filter(item):\n                matched = matcher.match(item.path) and os.sep.join(item.path.split(os.sep)[strip_components:])\n                peek_and_store_hardlink_masters(item, matched)\n                return matched\n        else:\n            def item_filter(item):\n                matched = matcher.match(item.path)\n                peek_and_store_hardlink_masters(item, matched)\n                return matched\n        return item_filter\n\n    @with_repository(compatibility=(Manifest.Operation.READ,))\n    @with_archive\n    def do_extract(self, args, repository, manifest, key, archive):\n        \"\"\"Extract archive contents\"\"\"\n        # be restrictive when restoring files, restore permissions later\n        if sys.getfilesystemencoding() == 'ascii':\n            logger.warning('Warning: File system encoding is \"ascii\", extracting non-ascii filenames will not be supported.')\n            if sys.platform.startswith(('linux', 'freebsd', 'netbsd', 'openbsd', 'darwin', )):\n                logger.warning('Hint: You likely need to fix your locale setup. E.g. install locales and use: LANG=en_US.UTF-8')\n\n        matcher = self.build_matcher(args.patterns, args.paths)\n\n        progress = args.progress\n        output_list = args.output_list\n        dry_run = args.dry_run\n        stdout = args.stdout\n        sparse = args.sparse\n        strip_components = args.strip_components\n        dirs = []\n        partial_extract = not matcher.empty() or strip_components\n        hardlink_masters = {} if partial_extract or not has_link else None\n\n        def peek_and_store_hardlink_masters(item, matched):\n            # not has_link:\n            # OS does not have hardlink capability thus we need to remember the chunks so that\n            # we can extract all hardlinks as separate normal (not-hardlinked) files instead.\n            #\n            # partial_extract and not matched and hardlinkable:\n            # we do not extract the very first hardlink, so we need to remember the chunks\n            # in hardlinks_master, so we can use them when we extract some 2nd+ hardlink item\n            # that has no chunks list.\n            if ((not has_link or (partial_extract and not matched and hardlinkable(item.mode))) and\n                    (item.get('hardlink_master', True) and 'source' not in item)):\n                hardlink_masters[item.get('path')] = (item.get('chunks'), None)\n\n        filter = self.build_filter(matcher, peek_and_store_hardlink_masters, strip_components)\n        if progress:\n            pi = ProgressIndicatorPercent(msg='%5.1f%% Extracting: %s', step=0.1, msgid='extract')\n            pi.output('Calculating total archive size for the progress indicator (might take long for large archives)')\n            extracted_size = sum(item.get_size(hardlink_masters) for item in archive.iter_items(filter))\n            pi.total = extracted_size\n        else:\n            pi = None\n\n        for item in archive.iter_items(filter, partial_extract=partial_extract,\n                                       preload=True, hardlink_masters=hardlink_masters):\n            orig_path = item.path\n            if strip_components:\n                item.path = os.sep.join(orig_path.split(os.sep)[strip_components:])\n            if not args.dry_run:\n                while dirs and not item.path.startswith(dirs[-1].path):\n                    dir_item = dirs.pop(-1)\n                    try:\n                        archive.extract_item(dir_item, stdout=stdout)\n                    except BackupOSError as e:\n                        self.print_warning('%s: %s', remove_surrogates(dir_item.path), e)\n            if output_list:\n                logging.getLogger('borg.output.list').info(remove_surrogates(item.path))\n            try:\n                if dry_run:\n                    archive.extract_item(item, dry_run=True, pi=pi)\n                else:\n                    if stat.S_ISDIR(item.mode):\n                        dirs.append(item)\n                        archive.extract_item(item, stdout=stdout, restore_attrs=False)\n                    else:\n                        archive.extract_item(item, stdout=stdout, sparse=sparse, hardlink_masters=hardlink_masters,\n                                             stripped_components=strip_components, original_path=orig_path, pi=pi)\n            except (BackupOSError, BackupError) as e:\n                self.print_warning('%s: %s', remove_surrogates(orig_path), e)\n\n        if pi:\n            pi.finish()\n\n        if not args.dry_run:\n            pi = ProgressIndicatorPercent(total=len(dirs), msg='Setting directory permissions %3.0f%%',\n                                          msgid='extract.permissions')\n            while dirs:\n                pi.show()\n                dir_item = dirs.pop(-1)\n                try:\n                    archive.extract_item(dir_item, stdout=stdout)\n                except BackupOSError as e:\n                    self.print_warning('%s: %s', remove_surrogates(dir_item.path), e)\n        for pattern in matcher.get_unmatched_include_patterns():\n            self.print_warning(\"Include pattern '%s' never matched.\", pattern)\n        if pi:\n            # clear progress output\n            pi.finish()\n        return self.exit_code\n\n    @with_repository(compatibility=(Manifest.Operation.READ,))\n    @with_archive\n    def do_export_tar(self, args, repository, manifest, key, archive):\n        \"\"\"Export archive contents as a tarball\"\"\"\n        self.output_list = args.output_list\n\n        # A quick note about the general design of tar_filter and tarfile;\n        # The tarfile module of Python can provide some compression mechanisms\n        # by itself, using the builtin gzip, bz2 and lzma modules (and \"tarmodes\"\n        # such as \"w:xz\").\n        #\n        # Doing so would have three major drawbacks:\n        # For one the compressor runs on the same thread as the program using the\n        # tarfile, stealing valuable CPU time from Borg and thus reducing throughput.\n        # Then this limits the available options - what about lz4? Brotli? zstd?\n        # The third issue is that systems can ship more optimized versions than those\n        # built into Python, e.g. pigz or pxz, which can use more than one thread for\n        # compression.\n        #\n        # Therefore we externalize compression by using a filter program, which has\n        # none of these drawbacks. The only issue of using an external filter is\n        # that it has to be installed -- hardly a problem, considering that\n        # the decompressor must be installed as well to make use of the exported tarball!\n\n        filter = None\n        if args.tar_filter == 'auto':\n            # Note that filter remains None if tarfile is '-'.\n            if args.tarfile.endswith('.tar.gz'):\n                filter = 'gzip'\n            elif args.tarfile.endswith('.tar.bz2'):\n                filter = 'bzip2'\n            elif args.tarfile.endswith('.tar.xz'):\n                filter = 'xz'\n            logger.debug('Automatically determined tar filter: %s', filter)\n        else:\n            filter = args.tar_filter\n\n        tarstream = dash_open(args.tarfile, 'wb')\n        tarstream_close = args.tarfile != '-'\n\n        if filter:\n            # When we put a filter between us and the final destination,\n            # the selected output (tarstream until now) becomes the output of the filter (=filterout).\n            # The decision whether to close that or not remains the same.\n            filterout = tarstream\n            filterout_close = tarstream_close\n            env = prepare_subprocess_env(system=True)\n            # There is no deadlock potential here (the subprocess docs warn about this), because\n            # communication with the process is a one-way road, i.e. the process can never block\n            # for us to do something while we block on the process for something different.\n            filterproc = popen_with_error_handling(filter, stdin=subprocess.PIPE, stdout=filterout,\n                                                   log_prefix='--tar-filter: ', env=env)\n            if not filterproc:\n                return EXIT_ERROR\n            # Always close the pipe, otherwise the filter process would not notice when we are done.\n            tarstream = filterproc.stdin\n            tarstream_close = True\n\n        # The | (pipe) symbol instructs tarfile to use a streaming mode of operation\n        # where it never seeks on the passed fileobj.\n        tar = tarfile.open(fileobj=tarstream, mode='w|')\n\n        self._export_tar(args, archive, tar)\n\n        # This does not close the fileobj (tarstream) we passed to it -- a side effect of the | mode.\n        tar.close()\n\n        if tarstream_close:\n            tarstream.close()\n\n        if filter:\n            logger.debug('Done creating tar, waiting for filter to die...')\n            rc = filterproc.wait()\n            if rc:\n                logger.error('--tar-filter exited with code %d, output file is likely unusable!', rc)\n                self.exit_code = EXIT_ERROR\n            else:\n                logger.debug('filter exited with code %d', rc)\n\n            if filterout_close:\n                filterout.close()\n\n        return self.exit_code\n\n    def _export_tar(self, args, archive, tar):\n        matcher = self.build_matcher(args.patterns, args.paths)\n\n        progress = args.progress\n        output_list = args.output_list\n        strip_components = args.strip_components\n        partial_extract = not matcher.empty() or strip_components\n        hardlink_masters = {} if partial_extract else None\n\n        def peek_and_store_hardlink_masters(item, matched):\n            if (partial_extract and not matched and hardlinkable(item.mode) and\n                    item.get('hardlink_master', True) and 'source' not in item):\n                hardlink_masters[item.get('path')] = (item.get('chunks'), None)\n\n        filter = self.build_filter(matcher, peek_and_store_hardlink_masters, strip_components)\n\n        if progress:\n            pi = ProgressIndicatorPercent(msg='%5.1f%% Processing: %s', step=0.1, msgid='extract')\n            pi.output('Calculating size')\n            extracted_size = sum(item.get_size(hardlink_masters) for item in archive.iter_items(filter))\n            pi.total = extracted_size\n        else:\n            pi = None\n\n        def item_content_stream(item):\n            \"\"\"\n            Return a file-like object that reads from the chunks of *item*.\n            \"\"\"\n            chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _, _ in item.chunks])\n            if pi:\n                info = [remove_surrogates(item.path)]\n                return ChunkIteratorFileWrapper(chunk_iterator,\n                                                lambda read_bytes: pi.show(increase=len(read_bytes), info=info))\n            else:\n                return ChunkIteratorFileWrapper(chunk_iterator)\n\n        def item_to_tarinfo(item, original_path):\n            \"\"\"\n            Transform a Borg *item* into a tarfile.TarInfo object.\n\n            Return a tuple (tarinfo, stream), where stream may be a file-like object that represents\n            the file contents, if any, and is None otherwise. When *tarinfo* is None, the *item*\n            cannot be represented as a TarInfo object and should be skipped.\n            \"\"\"\n\n            # If we would use the PAX (POSIX) format (which we currently don't),\n            # we can support most things that aren't possible with classic tar\n            # formats, including GNU tar, such as:\n            # atime, ctime, possibly Linux capabilities (security.* xattrs)\n            # and various additions supported by GNU tar in POSIX mode.\n\n            stream = None\n            tarinfo = tarfile.TarInfo()\n            tarinfo.name = item.path\n            tarinfo.mtime = item.mtime / 1e9\n            tarinfo.mode = stat.S_IMODE(item.mode)\n            tarinfo.uid = item.uid\n            tarinfo.gid = item.gid\n            tarinfo.uname = item.user or ''\n            tarinfo.gname = item.group or ''\n            # The linkname in tar has the same dual use the 'source' attribute of Borg items,\n            # i.e. for symlinks it means the destination, while for hardlinks it refers to the\n            # file.\n            # Since hardlinks in tar have a different type code (LNKTYPE) the format might\n            # support hardlinking arbitrary objects (including symlinks and directories), but\n            # whether implementations actually support that is a whole different question...\n            tarinfo.linkname = \"\"\n\n            modebits = stat.S_IFMT(item.mode)\n            if modebits == stat.S_IFREG:\n                tarinfo.type = tarfile.REGTYPE\n                if 'source' in item:\n                    source = os.sep.join(item.source.split(os.sep)[strip_components:])\n                    if hardlink_masters is None:\n                        linkname = source\n                    else:\n                        chunks, linkname = hardlink_masters.get(item.source, (None, source))\n                    if linkname:\n                        # Master was already added to the archive, add a hardlink reference to it.\n                        tarinfo.type = tarfile.LNKTYPE\n                        tarinfo.linkname = linkname\n                    elif chunks is not None:\n                        # The item which has the chunks was not put into the tar, therefore\n                        # we do that now and update hardlink_masters to reflect that.\n                        item.chunks = chunks\n                        tarinfo.size = item.get_size()\n                        stream = item_content_stream(item)\n                        hardlink_masters[item.get('source') or original_path] = (None, item.path)\n                else:\n                    tarinfo.size = item.get_size()\n                    stream = item_content_stream(item)\n            elif modebits == stat.S_IFDIR:\n                tarinfo.type = tarfile.DIRTYPE\n            elif modebits == stat.S_IFLNK:\n                tarinfo.type = tarfile.SYMTYPE\n                tarinfo.linkname = item.source\n            elif modebits == stat.S_IFBLK:\n                tarinfo.type = tarfile.BLKTYPE\n                tarinfo.devmajor = os.major(item.rdev)\n                tarinfo.devminor = os.minor(item.rdev)\n            elif modebits == stat.S_IFCHR:\n                tarinfo.type = tarfile.CHRTYPE\n                tarinfo.devmajor = os.major(item.rdev)\n                tarinfo.devminor = os.minor(item.rdev)\n            elif modebits == stat.S_IFIFO:\n                tarinfo.type = tarfile.FIFOTYPE\n            else:\n                self.print_warning('%s: unsupported file type %o for tar export', remove_surrogates(item.path), modebits)\n                set_ec(EXIT_WARNING)\n                return None, stream\n            return tarinfo, stream\n\n        for item in archive.iter_items(filter, preload=True, hardlink_masters=hardlink_masters):\n            orig_path = item.path\n            if strip_components:\n                item.path = os.sep.join(orig_path.split(os.sep)[strip_components:])\n            tarinfo, stream = item_to_tarinfo(item, orig_path)\n            if tarinfo:\n                if output_list:\n                    logging.getLogger('borg.output.list').info(remove_surrogates(orig_path))\n                tar.addfile(tarinfo, stream)\n\n        if pi:\n            pi.finish()\n\n        for pattern in matcher.get_unmatched_include_patterns():\n            self.print_warning(\"Include pattern '%s' never matched.\", pattern)\n        return self.exit_code\n\n    @with_repository(compatibility=(Manifest.Operation.READ,))\n    @with_archive\n    def do_diff(self, args, repository, manifest, key, archive):\n        \"\"\"Diff contents of two archives\"\"\"\n\n        def print_output(diff, path):\n            print(\"{:<19} {}\".format(diff, path))\n\n        archive1 = archive\n        archive2 = Archive(repository, key, manifest, args.archive2,\n                           consider_part_files=args.consider_part_files)\n\n        can_compare_chunk_ids = archive1.metadata.get('chunker_params', False) == archive2.metadata.get(\n            'chunker_params', True) or args.same_chunker_params\n        if not can_compare_chunk_ids:\n            self.print_warning('--chunker-params might be different between archives, diff will be slow.\\n'\n                               'If you know for certain that they are the same, pass --same-chunker-params '\n                               'to override this check.')\n\n        matcher = self.build_matcher(args.patterns, args.paths)\n\n        diffs = Archive.compare_archives_iter(archive1, archive2, matcher, can_compare_chunk_ids=can_compare_chunk_ids)\n        # Conversion to string and filtering for diff.equal to save memory if sorting\n        diffs = ((path, str(diff)) for path, diff in diffs if not diff.equal)\n\n        if args.sort:\n            diffs = sorted(diffs)\n\n        for path, diff in diffs:\n            print_output(diff, path)\n\n        for pattern in matcher.get_unmatched_include_patterns():\n            self.print_warning(\"Include pattern '%s' never matched.\", pattern)\n\n        return self.exit_code\n\n    @with_repository(exclusive=True, cache=True, compatibility=(Manifest.Operation.CHECK,))\n    @with_archive\n    def do_rename(self, args, repository, manifest, key, cache, archive):\n        \"\"\"Rename an existing archive\"\"\"\n        archive.rename(args.name)\n        manifest.write()\n        repository.commit(compact=False)\n        cache.commit()\n        return self.exit_code\n\n    @with_repository(exclusive=True, manifest=False)\n    def do_delete(self, args, repository):\n        \"\"\"Delete an existing repository or archives\"\"\"\n        archive_filter_specified = any((args.first, args.last, args.prefix is not None, args.glob_archives))\n        explicit_archives_specified = args.location.archive or args.archives\n        if archive_filter_specified and explicit_archives_specified:\n            self.print_error('Mixing archive filters and explicitly named archives is not supported.')\n            return self.exit_code\n        if archive_filter_specified or explicit_archives_specified:\n            return self._delete_archives(args, repository)\n        else:\n            return self._delete_repository(args, repository)\n\n    def _delete_archives(self, args, repository):\n        \"\"\"Delete archives\"\"\"\n        dry_run = args.dry_run\n\n        manifest, key = Manifest.load(repository, (Manifest.Operation.DELETE,))\n\n        if args.location.archive or args.archives:\n            archives = list(args.archives)\n            if args.location.archive:\n                archives.insert(0, args.location.archive)\n            archive_names = tuple(archives)\n        else:\n            archive_names = tuple(x.name for x in manifest.archives.list_considering(args))\n            if not archive_names:\n                return self.exit_code\n\n        if args.forced == 2:\n            deleted = False\n            for i, archive_name in enumerate(archive_names, 1):\n                try:\n                    current_archive = manifest.archives.pop(archive_name)\n                except KeyError:\n                    self.exit_code = EXIT_WARNING\n                    logger.warning('Archive {} not found ({}/{}).'.format(archive_name, i, len(archive_names)))\n                else:\n                    deleted = True\n                    msg = 'Would delete: {} ({}/{})' if dry_run else 'Deleted archive: {} ({}/{})'\n                    logger.info(msg.format(format_archive(current_archive), i, len(archive_names)))\n            if dry_run:\n                logger.info('Finished dry-run.')\n            elif deleted:\n                manifest.write()\n                # note: might crash in compact() after committing the repo\n                repository.commit(compact=False)\n                logger.warning('Done. Run \"borg check --repair\" to clean up the mess.')\n            else:\n                logger.warning('Aborted.')\n            return self.exit_code\n\n        stats = Statistics()\n        with Cache(repository, key, manifest, progress=args.progress, lock_wait=self.lock_wait) as cache:\n            msg_delete = 'Would delete archive: {} ({}/{})' if dry_run else 'Deleting archive: {} ({}/{})'\n            msg_not_found = 'Archive {} not found ({}/{}).'\n            for i, archive_name in enumerate(archive_names, 1):\n                try:\n                    archive_info = manifest.archives[archive_name]\n                except KeyError:\n                    logger.warning(msg_not_found.format(archive_name, i, len(archive_names)))\n                else:\n                    logger.info(msg_delete.format(format_archive(archive_info), i, len(archive_names)))\n                    if not dry_run:\n                        archive = Archive(repository, key, manifest, archive_name, cache=cache,\n                                          consider_part_files=args.consider_part_files)\n                        archive.delete(stats, progress=args.progress, forced=args.forced)\n\n            if not dry_run:\n                manifest.write()\n                repository.commit(compact=False, save_space=args.save_space)\n                cache.commit()\n            if args.stats:\n                log_multi(DASHES,\n                          STATS_HEADER,\n                          stats.summary.format(label='Deleted data:', stats=stats),\n                          str(cache),\n                          DASHES, logger=logging.getLogger('borg.output.stats'))\n\n        return self.exit_code\n\n    def _delete_repository(self, args, repository):\n        \"\"\"Delete a repository\"\"\"\n        dry_run = args.dry_run\n\n        if not args.cache_only:\n            msg = []\n            try:\n                manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)\n            except NoManifestError:\n                msg.append(\"You requested to completely DELETE the repository *including* all archives it may \"\n                           \"contain.\")\n                msg.append(\"This repository seems to have no manifest, so we can't tell anything about its \"\n                           \"contents.\")\n            else:\n                msg.append(\"You requested to completely DELETE the repository *including* all archives it \"\n                           \"contains:\")\n                for archive_info in manifest.archives.list(sort_by=['ts']):\n                    msg.append(format_archive(archive_info))\n            msg.append(\"Type 'YES' if you understand this and want to continue: \")\n            msg = '\\n'.join(msg)\n            if not yes(msg, false_msg=\"Aborting.\", invalid_msg='Invalid answer, aborting.', truish=('YES',),\n                       retry=False, env_var_override='BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'):\n                self.exit_code = EXIT_ERROR\n                return self.exit_code\n            if not dry_run:\n                repository.destroy()\n                logger.info(\"Repository deleted.\")\n                SecurityManager.destroy(repository)\n            else:\n                logger.info(\"Would delete repository.\")\n        if not dry_run:\n            Cache.destroy(repository)\n            logger.info(\"Cache deleted.\")\n        else:\n            logger.info(\"Would delete cache.\")\n        return self.exit_code\n\n    def do_mount(self, args):\n        \"\"\"Mount archive or an entire repository as a FUSE filesystem\"\"\"\n        # Perform these checks before opening the repository and asking for a passphrase.\n\n        try:\n            import borg.fuse\n        except ImportError as e:\n            self.print_error('borg mount not available: loading FUSE support failed [ImportError: %s]' % str(e))\n            return self.exit_code\n\n        if not os.path.isdir(args.mountpoint) or not os.access(args.mountpoint, os.R_OK | os.W_OK | os.X_OK):\n            self.print_error('%s: Mountpoint must be a writable directory' % args.mountpoint)\n            return self.exit_code\n\n        return self._do_mount(args)\n\n    @with_repository(compatibility=(Manifest.Operation.READ,))\n    def _do_mount(self, args, repository, manifest, key):\n        from .fuse import FuseOperations\n\n        with cache_if_remote(repository, decrypted_cache=key) as cached_repo:\n            operations = FuseOperations(key, repository, manifest, args, cached_repo)\n            logger.info(\"Mounting filesystem\")\n            try:\n                operations.mount(args.mountpoint, args.options, args.foreground)\n            except RuntimeError:\n                # Relevant error message already printed to stderr by FUSE\n                self.exit_code = EXIT_ERROR\n        return self.exit_code\n\n    def do_umount(self, args):\n        \"\"\"un-mount the FUSE filesystem\"\"\"\n        return umount(args.mountpoint)\n\n    @with_repository(compatibility=(Manifest.Operation.READ,))\n    def do_list(self, args, repository, manifest, key):\n        \"\"\"List archive or repository contents\"\"\"\n        if args.location.archive:\n            if args.json:\n                self.print_error('The --json option is only valid for listing archives, not archive contents.')\n                return self.exit_code\n            return self._list_archive(args, repository, manifest, key)\n        else:\n            if args.json_lines:\n                self.print_error('The --json-lines option is only valid for listing archive contents, not archives.')\n                return self.exit_code\n            return self._list_repository(args, repository, manifest, key)\n\n    def _list_archive(self, args, repository, manifest, key):\n        matcher = self.build_matcher(args.patterns, args.paths)\n        if args.format is not None:\n            format = args.format\n        elif args.short:\n            format = \"{path}{NL}\"\n        else:\n            format = \"{mode} {user:6} {group:6} {size:8} {mtime} {path}{extra}{NL}\"\n\n        def _list_inner(cache):\n            archive = Archive(repository, key, manifest, args.location.archive, cache=cache,\n                              consider_part_files=args.consider_part_files)\n\n            formatter = ItemFormatter(archive, format, json_lines=args.json_lines)\n            for item in archive.iter_items(lambda item: matcher.match(item.path)):\n                sys.stdout.write(formatter.format_item(item))\n\n        # Only load the cache if it will be used\n        if ItemFormatter.format_needs_cache(format):\n            with Cache(repository, key, manifest, lock_wait=self.lock_wait) as cache:\n                _list_inner(cache)\n        else:\n            _list_inner(cache=None)\n\n        return self.exit_code\n\n    def _list_repository(self, args, repository, manifest, key):\n        if args.format is not None:\n            format = args.format\n        elif args.short:\n            format = \"{archive}{NL}\"\n        else:\n            format = \"{archive:<36} {time} [{id}]{NL}\"\n        formatter = ArchiveFormatter(format, repository, manifest, key, json=args.json)\n\n        output_data = []\n\n        for archive_info in manifest.archives.list_considering(args):\n            if args.json:\n                output_data.append(formatter.get_item_data(archive_info))\n            else:\n                sys.stdout.write(formatter.format_item(archive_info))\n\n        if args.json:\n            json_print(basic_json_data(manifest, extra={\n                'archives': output_data\n            }))\n\n        return self.exit_code\n\n    @with_repository(cache=True, compatibility=(Manifest.Operation.READ,))\n    def do_info(self, args, repository, manifest, key, cache):\n        \"\"\"Show archive details such as disk space used\"\"\"\n        if any((args.location.archive, args.first, args.last, args.prefix is not None, args.glob_archives)):\n            return self._info_archives(args, repository, manifest, key, cache)\n        else:\n            return self._info_repository(args, repository, manifest, key, cache)\n\n    def _info_archives(self, args, repository, manifest, key, cache):\n        def format_cmdline(cmdline):\n            return remove_surrogates(' '.join(shlex.quote(x) for x in cmdline))\n\n        if args.location.archive:\n            archive_names = (args.location.archive,)\n        else:\n            archive_names = tuple(x.name for x in manifest.archives.list_considering(args))\n            if not archive_names:\n                return self.exit_code\n\n        output_data = []\n\n        for i, archive_name in enumerate(archive_names, 1):\n            archive = Archive(repository, key, manifest, archive_name, cache=cache,\n                              consider_part_files=args.consider_part_files)\n            info = archive.info()\n            if args.json:\n                output_data.append(info)\n            else:\n                info['duration'] = format_timedelta(timedelta(seconds=info['duration']))\n                info['command_line'] = format_cmdline(info['command_line'])\n                print(textwrap.dedent(\"\"\"\n                Archive name: {name}\n                Archive fingerprint: {id}\n                Comment: {comment}\n                Hostname: {hostname}\n                Username: {username}\n                Time (start): {start}\n                Time (end): {end}\n                Duration: {duration}\n                Number of files: {stats[nfiles]}\n                Command line: {command_line}\n                Utilization of maximum supported archive size: {limits[max_archive_size]:.0%}\n                ------------------------------------------------------------------------------\n                                       Original size      Compressed size    Deduplicated size\n                This archive:   {stats[original_size]:>20s} {stats[compressed_size]:>20s} {stats[deduplicated_size]:>20s}\n                {cache}\n                \"\"\").strip().format(cache=cache, **info))\n            if self.exit_code:\n                break\n            if not args.json and len(archive_names) - i:\n                print()\n\n        if args.json:\n            json_print(basic_json_data(manifest, cache=cache, extra={\n                'archives': output_data,\n            }))\n        return self.exit_code\n\n    def _info_repository(self, args, repository, manifest, key, cache):\n        info = basic_json_data(manifest, cache=cache, extra={\n            'security_dir': cache.security_manager.dir,\n        })\n\n        if args.json:\n            json_print(info)\n        else:\n            encryption = 'Encrypted: '\n            if key.NAME == 'plaintext':\n                encryption += 'No'\n            else:\n                encryption += 'Yes (%s)' % key.NAME\n            if key.NAME.startswith('key file'):\n                encryption += '\\nKey file: %s' % key.find_key()\n            info['encryption'] = encryption\n\n            print(textwrap.dedent(\"\"\"\n            Repository ID: {id}\n            Location: {location}\n            {encryption}\n            Cache: {cache.path}\n            Security dir: {security_dir}\n            \"\"\").strip().format(\n                id=bin_to_hex(repository.id),\n                location=repository._location.canonical_path(),\n                **info))\n            print(DASHES)\n            print(STATS_HEADER)\n            print(str(cache))\n        return self.exit_code\n\n    @with_repository(exclusive=True, compatibility=(Manifest.Operation.DELETE,))\n    def do_prune(self, args, repository, manifest, key):\n        \"\"\"Prune repository archives according to specified rules\"\"\"\n        if not any((args.secondly, args.minutely, args.hourly, args.daily,\n                    args.weekly, args.monthly, args.yearly, args.within)):\n            self.print_error('At least one of the \"keep-within\", \"keep-last\", '\n                             '\"keep-secondly\", \"keep-minutely\", \"keep-hourly\", \"keep-daily\", '\n                             '\"keep-weekly\", \"keep-monthly\" or \"keep-yearly\" settings must be specified.')\n            return self.exit_code\n        if args.prefix is not None:\n            args.glob_archives = args.prefix + '*'\n        checkpoint_re = r'\\.checkpoint(\\.\\d+)?'\n        archives_checkpoints = manifest.archives.list(glob=args.glob_archives,\n                                                      match_end=r'(%s)?\\Z' % checkpoint_re,\n                                                      sort_by=['ts'], reverse=True)\n        is_checkpoint = re.compile(r'(%s)\\Z' % checkpoint_re).search\n        checkpoints = [arch for arch in archives_checkpoints if is_checkpoint(arch.name)]\n        # keep the latest checkpoint, if there is no later non-checkpoint archive\n        if archives_checkpoints and checkpoints and archives_checkpoints[0] is checkpoints[0]:\n            keep_checkpoints = checkpoints[:1]\n        else:\n            keep_checkpoints = []\n        checkpoints = set(checkpoints)\n        # ignore all checkpoint archives to avoid keeping one (which is an incomplete backup)\n        # that is newer than a successfully completed backup - and killing the successful backup.\n        archives = [arch for arch in archives_checkpoints if arch not in checkpoints]\n        keep = []\n        # collect the rule responsible for the keeping of each archive in this dict\n        # keys are archive ids, values are a tuple\n        #   (<rulename>, <how many archives were kept by this rule so far >)\n        kept_because = {}\n\n        # find archives which need to be kept because of the keep-within rule\n        if args.within:\n            keep += prune_within(archives, args.within, kept_because)\n\n        # find archives which need to be kept because of the various time period rules\n        for rule in PRUNING_PATTERNS.keys():\n            num = getattr(args, rule, None)\n            if num is not None:\n                keep += prune_split(archives, rule, num, kept_because)\n\n        to_delete = (set(archives) | checkpoints) - (set(keep) | set(keep_checkpoints))\n        stats = Statistics()\n        with Cache(repository, key, manifest, lock_wait=self.lock_wait) as cache:\n            list_logger = logging.getLogger('borg.output.list')\n            # set up counters for the progress display\n            to_delete_len = len(to_delete)\n            archives_deleted = 0\n            pi = ProgressIndicatorPercent(total=len(to_delete), msg='Pruning archives %3.0f%%', msgid='prune')\n            for archive in archives_checkpoints:\n                if archive in to_delete:\n                    pi.show()\n                    if args.dry_run:\n                        log_message = 'Would prune:'\n                    else:\n                        archives_deleted += 1\n                        log_message = 'Pruning archive (%d/%d):' % (archives_deleted, to_delete_len)\n                        archive = Archive(repository, key, manifest, archive.name, cache,\n                                          consider_part_files=args.consider_part_files)\n                        archive.delete(stats, forced=args.forced)\n                else:\n                    if is_checkpoint(archive.name):\n                        log_message = 'Keeping checkpoint archive:'\n                    else:\n                        log_message = 'Keeping archive (rule: {rule} #{num}):'.format(\n                            rule=kept_because[archive.id][0], num=kept_because[archive.id][1]\n                        )\n                if args.output_list:\n                    list_logger.info(\"{message:<40} {archive}\".format(\n                        message=log_message, archive=format_archive(archive)\n                    ))\n            pi.finish()\n            if to_delete and not args.dry_run:\n                manifest.write()\n                repository.commit(compact=False, save_space=args.save_space)\n                cache.commit()\n            if args.stats:\n                log_multi(DASHES,\n                          STATS_HEADER,\n                          stats.summary.format(label='Deleted data:', stats=stats),\n                          str(cache),\n                          DASHES, logger=logging.getLogger('borg.output.stats'))\n        return self.exit_code\n\n    @with_repository(fake=('tam', 'disable_tam'), invert_fake=True, manifest=False, exclusive=True)\n    def do_upgrade(self, args, repository, manifest=None, key=None):\n        \"\"\"upgrade a repository from a previous version\"\"\"\n        if args.tam:\n            manifest, key = Manifest.load(repository, (Manifest.Operation.CHECK,), force_tam_not_required=args.force)\n\n            if not hasattr(key, 'change_passphrase'):\n                print('This repository is not encrypted, cannot enable TAM.')\n                return EXIT_ERROR\n\n            if not manifest.tam_verified or not manifest.config.get(b'tam_required', False):\n                # The standard archive listing doesn't include the archive ID like in borg 1.1.x\n                print('Manifest contents:')\n                for archive_info in manifest.archives.list(sort_by=['ts']):\n                    print(format_archive(archive_info), '[%s]' % bin_to_hex(archive_info.id))\n                manifest.config[b'tam_required'] = True\n                manifest.write()\n                repository.commit(compact=False)\n            if not key.tam_required:\n                key.tam_required = True\n                key.change_passphrase(key._passphrase)\n                print('Key updated')\n                if hasattr(key, 'find_key'):\n                    print('Key location:', key.find_key())\n            if not tam_required(repository):\n                tam_file = tam_required_file(repository)\n                open(tam_file, 'w').close()\n                print('Updated security database')\n        elif args.disable_tam:\n            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK, force_tam_not_required=True)\n            if tam_required(repository):\n                os.unlink(tam_required_file(repository))\n            if key.tam_required:\n                key.tam_required = False\n                key.change_passphrase(key._passphrase)\n                print('Key updated')\n                if hasattr(key, 'find_key'):\n                    print('Key location:', key.find_key())\n            manifest.config[b'tam_required'] = False\n            manifest.write()\n            repository.commit(compact=False)\n        else:\n            # mainly for upgrades from Attic repositories,\n            # but also supports borg 0.xx -> 1.0 upgrade.\n\n            repo = AtticRepositoryUpgrader(args.location.path, create=False)\n            try:\n                repo.upgrade(args.dry_run, inplace=args.inplace, progress=args.progress)\n            except NotImplementedError as e:\n                print(\"warning: %s\" % e)\n            repo = BorgRepositoryUpgrader(args.location.path, create=False)\n            try:\n                repo.upgrade(args.dry_run, inplace=args.inplace, progress=args.progress)\n            except NotImplementedError as e:\n                print(\"warning: %s\" % e)\n        return self.exit_code\n\n    @with_repository(cache=True, exclusive=True, compatibility=(Manifest.Operation.CHECK,))\n    def do_recreate(self, args, repository, manifest, key, cache):\n        \"\"\"Re-create archives\"\"\"\n        msg = (\"recreate is an experimental feature.\\n\"\n               \"Type 'YES' if you understand this and want to continue: \")\n        if not yes(msg, false_msg=\"Aborting.\", truish=('YES',),\n                   env_var_override='BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING'):\n            return EXIT_ERROR\n\n        matcher = self.build_matcher(args.patterns, args.paths)\n        self.output_list = args.output_list\n        self.output_filter = args.output_filter\n        recompress = args.recompress != 'never'\n        always_recompress = args.recompress == 'always'\n\n        recreater = ArchiveRecreater(repository, manifest, key, cache, matcher,\n                                     exclude_caches=args.exclude_caches, exclude_if_present=args.exclude_if_present,\n                                     keep_exclude_tags=args.keep_exclude_tags, chunker_params=args.chunker_params,\n                                     compression=args.compression, recompress=recompress, always_recompress=always_recompress,\n                                     progress=args.progress, stats=args.stats,\n                                     file_status_printer=self.print_file_status,\n                                     checkpoint_interval=args.checkpoint_interval,\n                                     dry_run=args.dry_run, timestamp=args.timestamp)\n\n        if args.location.archive:\n            name = args.location.archive\n            if recreater.is_temporary_archive(name):\n                self.print_error('Refusing to work on temporary archive of prior recreate: %s', name)\n                return self.exit_code\n            if not recreater.recreate(name, args.comment, args.target):\n                self.print_error('Nothing to do. Archive was not processed.\\n'\n                                 'Specify at least one pattern, PATH, --comment, re-compression or re-chunking option.')\n        else:\n            if args.target is not None:\n                self.print_error('--target: Need to specify single archive')\n                return self.exit_code\n            for archive in manifest.archives.list(sort_by=['ts']):\n                name = archive.name\n                if recreater.is_temporary_archive(name):\n                    continue\n                print('Processing', name)\n                if not recreater.recreate(name, args.comment):\n                    logger.info('Skipped archive %s: Nothing to do. Archive was not processed.', name)\n        if not args.dry_run:\n            manifest.write()\n            repository.commit(compact=False)\n            cache.commit()\n        return self.exit_code\n\n    @with_repository(manifest=False, exclusive=True)\n    def do_with_lock(self, args, repository):\n        \"\"\"run a user specified command with the repository lock held\"\"\"\n        # for a new server, this will immediately take an exclusive lock.\n        # to support old servers, that do not have \"exclusive\" arg in open()\n        # RPC API, we also do it the old way:\n        # re-write manifest to start a repository transaction - this causes a\n        # lock upgrade to exclusive for remote (and also for local) repositories.\n        # by using manifest=False in the decorator, we avoid having to require\n        # the encryption key (and can operate just with encrypted data).\n        data = repository.get(Manifest.MANIFEST_ID)\n        repository.put(Manifest.MANIFEST_ID, data)\n        # usually, a 0 byte (open for writing) segment file would be visible in the filesystem here.\n        # we write and close this file, to rather have a valid segment file on disk, before invoking the subprocess.\n        # we can only do this for local repositories (with .io), though:\n        if hasattr(repository, 'io'):\n            repository.io.close_segment()\n        env = prepare_subprocess_env(system=True)\n        try:\n            # we exit with the return code we get from the subprocess\n            return subprocess.call([args.command] + args.args, env=env)\n        finally:\n            # we need to commit the \"no change\" operation we did to the manifest\n            # because it created a new segment file in the repository. if we would\n            # roll back, the same file would be later used otherwise (for other content).\n            # that would be bad if somebody uses rsync with ignore-existing (or\n            # any other mechanism relying on existing segment data not changing).\n            # see issue #1867.\n            repository.commit(compact=False)\n\n    @with_repository(manifest=False, exclusive=True)\n    def do_compact(self, args, repository):\n        \"\"\"compact segment files in the repository\"\"\"\n        # see the comment in do_with_lock about why we do it like this:\n        data = repository.get(Manifest.MANIFEST_ID)\n        repository.put(Manifest.MANIFEST_ID, data)\n        threshold = args.threshold / 100\n        repository.commit(compact=True, threshold=threshold, cleanup_commits=args.cleanup_commits)\n        return EXIT_SUCCESS\n\n    @with_repository(exclusive=True, manifest=False)\n    def do_config(self, args, repository):\n        \"\"\"get, set, and delete values in a repository or cache config file\"\"\"\n\n        def repo_validate(section, name, value=None, check_value=True):\n            if section not in ['repository', ]:\n                raise ValueError('Invalid section')\n            if name in ['segments_per_dir', 'max_segment_size', 'storage_quota', ]:\n                if check_value:\n                    try:\n                        int(value)\n                    except ValueError:\n                        raise ValueError('Invalid value') from None\n                    if name == 'max_segment_size':\n                        if int(value) >= MAX_SEGMENT_SIZE_LIMIT:\n                            raise ValueError('Invalid value: max_segment_size >= %d' % MAX_SEGMENT_SIZE_LIMIT)\n            elif name in ['additional_free_space', ]:\n                if check_value:\n                    try:\n                        parse_file_size(value)\n                    except ValueError:\n                        raise ValueError('Invalid value') from None\n            elif name in ['append_only', ]:\n                if check_value and value not in ['0', '1']:\n                    raise ValueError('Invalid value')\n            elif name in ['id', ]:\n                if check_value:\n                    try:\n                        bin_id = unhexlify(value)\n                    except:\n                        raise ValueError('Invalid value, must be 64 hex digits') from None\n                    if len(bin_id) != 32:\n                        raise ValueError('Invalid value, must be 64 hex digits')\n            else:\n                raise ValueError('Invalid name')\n\n        def cache_validate(section, name, value=None, check_value=True):\n            if section not in ['cache', ]:\n                raise ValueError('Invalid section')\n            if name in ['previous_location', ]:\n                if check_value:\n                    Location(value)\n            else:\n                raise ValueError('Invalid name')\n\n        def list_config(config):\n            default_values = {\n                'version': '1',\n                'segments_per_dir': str(DEFAULT_SEGMENTS_PER_DIR),\n                'max_segment_size': str(MAX_SEGMENT_SIZE_LIMIT),\n                'additional_free_space': '0',\n                'storage_quota': repository.storage_quota,\n                'append_only': repository.append_only\n            }\n            print('[repository]')\n            for key in ['version', 'segments_per_dir', 'max_segment_size',\n                        'storage_quota', 'additional_free_space', 'append_only',\n                        'id']:\n                value = config.get('repository', key, fallback=False)\n                if value is None:\n                    value = default_values.get(key)\n                    if value is None:\n                        raise Error('The repository config is missing the %s key which has no default value' % key)\n                print('%s = %s' % (key, value))\n\n        if not args.list:\n            if args.name is None:\n                self.print_error('No config key name was provided.')\n                return self.exit_code\n\n            try:\n                section, name = args.name.split('.')\n            except ValueError:\n                section = args.cache and \"cache\" or \"repository\"\n                name = args.name\n\n        if args.cache:\n            manifest, key = Manifest.load(repository, (Manifest.Operation.WRITE,))\n            assert_secure(repository, manifest, self.lock_wait)\n            cache = Cache(repository, key, manifest, lock_wait=self.lock_wait)\n\n        try:\n            if args.cache:\n                cache.cache_config.load()\n                config = cache.cache_config._config\n                save = cache.cache_config.save\n                validate = cache_validate\n            else:\n                config = repository.config\n                save = lambda: repository.save_config(repository.path, repository.config)  # noqa\n                validate = repo_validate\n\n            if args.delete:\n                validate(section, name, check_value=False)\n                config.remove_option(section, name)\n                if len(config.options(section)) == 0:\n                    config.remove_section(section)\n                save()\n            elif args.list:\n                list_config(config)\n            elif args.value:\n                validate(section, name, args.value)\n                if section not in config.sections():\n                    config.add_section(section)\n                config.set(section, name, args.value)\n                save()\n            else:\n                try:\n                    print(config.get(section, name))\n                except (configparser.NoOptionError, configparser.NoSectionError) as e:\n                    print(e, file=sys.stderr)\n                    return EXIT_WARNING\n            return EXIT_SUCCESS\n        finally:\n            if args.cache:\n                cache.close()\n\n    def do_debug_info(self, args):\n        \"\"\"display system information for debugging / bug reports\"\"\"\n        print(sysinfo())\n\n        # Additional debug information\n        print('CRC implementation:', crc32.__name__)\n        print('Process ID:', get_process_id())\n        return EXIT_SUCCESS\n\n    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)\n    def do_debug_dump_archive_items(self, args, repository, manifest, key):\n        \"\"\"dump (decrypted, decompressed) archive items metadata (not: data)\"\"\"\n        archive = Archive(repository, key, manifest, args.location.archive,\n                          consider_part_files=args.consider_part_files)\n        for i, item_id in enumerate(archive.metadata.items):\n            data = key.decrypt(item_id, repository.get(item_id))\n            filename = '%06d_%s.items' % (i, bin_to_hex(item_id))\n            print('Dumping', filename)\n            with open(filename, 'wb') as fd:\n                fd.write(data)\n        print('Done.')\n        return EXIT_SUCCESS\n\n    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)\n    def do_debug_dump_archive(self, args, repository, manifest, key):\n        \"\"\"dump decoded archive metadata (not: data)\"\"\"\n\n        try:\n            archive_meta_orig = manifest.archives.get_raw_dict()[safe_encode(args.location.archive)]\n        except KeyError:\n            raise Archive.DoesNotExist(args.location.archive)\n\n        indent = 4\n\n        def do_indent(d):\n            return textwrap.indent(json.dumps(d, indent=indent), prefix=' ' * indent)\n\n        def output(fd):\n            # this outputs megabytes of data for a modest sized archive, so some manual streaming json output\n            fd.write('{\\n')\n            fd.write('    \"_name\": ' + json.dumps(args.location.archive) + \",\\n\")\n            fd.write('    \"_manifest_entry\":\\n')\n            fd.write(do_indent(prepare_dump_dict(archive_meta_orig)))\n            fd.write(',\\n')\n\n            data = key.decrypt(archive_meta_orig[b'id'], repository.get(archive_meta_orig[b'id']))\n            archive_org_dict = msgpack.unpackb(data, object_hook=StableDict)\n\n            fd.write('    \"_meta\":\\n')\n            fd.write(do_indent(prepare_dump_dict(archive_org_dict)))\n            fd.write(',\\n')\n            fd.write('    \"_items\": [\\n')\n\n            unpacker = msgpack.Unpacker(use_list=False, object_hook=StableDict)\n            first = True\n            for item_id in archive_org_dict[b'items']:\n                data = key.decrypt(item_id, repository.get(item_id))\n                unpacker.feed(data)\n                for item in unpacker:\n                    item = prepare_dump_dict(item)\n                    if first:\n                        first = False\n                    else:\n                        fd.write(',\\n')\n                    fd.write(do_indent(item))\n\n            fd.write('\\n')\n            fd.write('    ]\\n}\\n')\n\n        with dash_open(args.path, 'w') as fd:\n            output(fd)\n        return EXIT_SUCCESS\n\n    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)\n    def do_debug_dump_manifest(self, args, repository, manifest, key):\n        \"\"\"dump decoded repository manifest\"\"\"\n\n        data = key.decrypt(None, repository.get(manifest.MANIFEST_ID))\n\n        meta = prepare_dump_dict(msgpack.unpackb(data, object_hook=StableDict))\n\n        with dash_open(args.path, 'w') as fd:\n            json.dump(meta, fd, indent=4)\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False)\n    def do_debug_dump_repo_objs(self, args, repository):\n        \"\"\"dump (decrypted, decompressed) repo objects, repo index MUST be current/correct\"\"\"\n        from .crypto.key import key_factory\n\n        def decrypt_dump(i, id, cdata, tag=None, segment=None, offset=None):\n            if cdata is not None:\n                give_id = id if id != Manifest.MANIFEST_ID else None\n                data = key.decrypt(give_id, cdata)\n            else:\n                data = b''\n            tag_str = '' if tag is None else '_' + tag\n            segment_str = '_' + str(segment) if segment is not None else ''\n            offset_str = '_' + str(offset) if offset is not None else ''\n            id_str = '_' + bin_to_hex(id) if id is not None else ''\n            filename = '%08d%s%s%s%s.obj' % (i, segment_str, offset_str, tag_str, id_str)\n            print('Dumping', filename)\n            with open(filename, 'wb') as fd:\n                fd.write(data)\n\n        if args.ghost:\n            # dump ghosty stuff from segment files: not yet committed objects, deleted / superceded objects, commit tags\n\n            # set up the key without depending on a manifest obj\n            for id, cdata, tag, segment, offset in repository.scan_low_level():\n                if tag == TAG_PUT:\n                    key = key_factory(repository, cdata)\n                    break\n            i = 0\n            for id, cdata, tag, segment, offset in repository.scan_low_level():\n                if tag == TAG_PUT:\n                    decrypt_dump(i, id, cdata, tag='put', segment=segment, offset=offset)\n                elif tag == TAG_DELETE:\n                    decrypt_dump(i, id, None, tag='del', segment=segment, offset=offset)\n                elif tag == TAG_COMMIT:\n                    decrypt_dump(i, None, None, tag='commit', segment=segment, offset=offset)\n                i += 1\n        else:\n            # set up the key without depending on a manifest obj\n            ids = repository.list(limit=1, marker=None)\n            cdata = repository.get(ids[0])\n            key = key_factory(repository, cdata)\n            marker = None\n            i = 0\n            while True:\n                result = repository.scan(limit=LIST_SCAN_LIMIT, marker=marker)  # must use on-disk order scanning here\n                if not result:\n                    break\n                marker = result[-1]\n                for id in result:\n                    cdata = repository.get(id)\n                    decrypt_dump(i, id, cdata)\n                    i += 1\n        print('Done.')\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False)\n    def do_debug_search_repo_objs(self, args, repository):\n        \"\"\"search for byte sequences in repo objects, repo index MUST be current/correct\"\"\"\n        context = 32\n\n        def print_finding(info, wanted, data, offset):\n            before = data[offset - context:offset]\n            after = data[offset + len(wanted):offset + len(wanted) + context]\n            print('%s: %s %s %s == %r %r %r' % (info, before.hex(), wanted.hex(), after.hex(),\n                                                before, wanted, after))\n\n        wanted = args.wanted\n        try:\n            if wanted.startswith('hex:'):\n                wanted = unhexlify(wanted[4:])\n            elif wanted.startswith('str:'):\n                wanted = wanted[4:].encode()\n            else:\n                raise ValueError('unsupported search term')\n        except (ValueError, UnicodeEncodeError):\n            wanted = None\n        if not wanted:\n            self.print_error('search term needs to be hex:123abc or str:foobar style')\n            return EXIT_ERROR\n\n        from .crypto.key import key_factory\n        # set up the key without depending on a manifest obj\n        ids = repository.list(limit=1, marker=None)\n        cdata = repository.get(ids[0])\n        key = key_factory(repository, cdata)\n\n        marker = None\n        last_data = b''\n        last_id = None\n        i = 0\n        while True:\n            result = repository.scan(limit=LIST_SCAN_LIMIT, marker=marker)  # must use on-disk order scanning here\n            if not result:\n                break\n            marker = result[-1]\n            for id in result:\n                cdata = repository.get(id)\n                give_id = id if id != Manifest.MANIFEST_ID else None\n                data = key.decrypt(give_id, cdata)\n\n                # try to locate wanted sequence crossing the border of last_data and data\n                boundary_data = last_data[-(len(wanted) - 1):] + data[:len(wanted) - 1]\n                if wanted in boundary_data:\n                    boundary_data = last_data[-(len(wanted) - 1 + context):] + data[:len(wanted) - 1 + context]\n                    offset = boundary_data.find(wanted)\n                    info = '%d %s | %s' % (i, last_id.hex(), id.hex())\n                    print_finding(info, wanted, boundary_data, offset)\n\n                # try to locate wanted sequence in data\n                count = data.count(wanted)\n                if count:\n                    offset = data.find(wanted)  # only determine first occurance's offset\n                    info = \"%d %s #%d\" % (i, id.hex(), count)\n                    print_finding(info, wanted, data, offset)\n\n                last_id, last_data = id, data\n                i += 1\n                if i % 10000 == 0:\n                    print('%d objects processed.' % i)\n        print('Done.')\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False)\n    def do_debug_get_obj(self, args, repository):\n        \"\"\"get object contents from the repository and write it into file\"\"\"\n        hex_id = args.id\n        try:\n            id = unhexlify(hex_id)\n        except ValueError:\n            print(\"object id %s is invalid.\" % hex_id)\n        else:\n            try:\n                data = repository.get(id)\n            except Repository.ObjectNotFound:\n                print(\"object %s not found.\" % hex_id)\n            else:\n                with open(args.path, \"wb\") as f:\n                    f.write(data)\n                print(\"object %s fetched.\" % hex_id)\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False, exclusive=True)\n    def do_debug_put_obj(self, args, repository):\n        \"\"\"put file(s) contents into the repository\"\"\"\n        for path in args.paths:\n            with open(path, \"rb\") as f:\n                data = f.read()\n            h = hashlib.sha256(data)  # XXX hardcoded\n            repository.put(h.digest(), data)\n            print(\"object %s put.\" % h.hexdigest())\n        repository.commit(compact=False)\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False, exclusive=True)\n    def do_debug_delete_obj(self, args, repository):\n        \"\"\"delete the objects with the given IDs from the repo\"\"\"\n        modified = False\n        for hex_id in args.ids:\n            try:\n                id = unhexlify(hex_id)\n            except ValueError:\n                print(\"object id %s is invalid.\" % hex_id)\n            else:\n                try:\n                    repository.delete(id)\n                    modified = True\n                    print(\"object %s deleted.\" % hex_id)\n                except Repository.ObjectNotFound:\n                    print(\"object %s not found.\" % hex_id)\n        if modified:\n            repository.commit(compact=False)\n        print('Done.')\n        return EXIT_SUCCESS\n\n    @with_repository(manifest=False, exclusive=True, cache=True, compatibility=Manifest.NO_OPERATION_CHECK)\n    def do_debug_refcount_obj(self, args, repository, manifest, key, cache):\n        \"\"\"display refcounts for the objects with the given IDs\"\"\"\n        for hex_id in args.ids:\n            try:\n                id = unhexlify(hex_id)\n            except ValueError:\n                print(\"object id %s is invalid.\" % hex_id)\n            else:\n                try:\n                    refcount = cache.chunks[id][0]\n                    print(\"object %s has %d referrers [info from chunks cache].\" % (hex_id, refcount))\n                except KeyError:\n                    print(\"object %s not found [info from chunks cache].\" % hex_id)\n        return EXIT_SUCCESS\n\n    def do_debug_convert_profile(self, args):\n        \"\"\"convert Borg profile to Python profile\"\"\"\n        import marshal\n        with args.output, args.input:\n            marshal.dump(msgpack.mp_unpack(args.input, use_list=False, raw=False), args.output)\n        return EXIT_SUCCESS\n\n    @with_repository(lock=False, manifest=False)\n    def do_break_lock(self, args, repository):\n        \"\"\"Break the repository lock (e.g. in case it was left by a dead borg.\"\"\"\n        repository.break_lock()\n        Cache.break_lock(repository)\n        return self.exit_code\n\n    helptext = collections.OrderedDict()\n    helptext['patterns'] = textwrap.dedent('''\n        The path/filenames used as input for the pattern matching start from the\n        currently active recursion root. You usually give the recursion root(s)\n        when invoking borg and these can be either relative or absolute paths.\n\n        So, when you give `relative/` as root, the paths going into the matcher\n        will look like `relative/.../file.ext`. When you give `/absolute/` as root,\n        they will look like `/absolute/.../file.ext`. This is meant when we talk\n        about \"full path\" below.\n\n        File patterns support these styles: fnmatch, shell, regular expressions,\n        path prefixes and path full-matches. By default, fnmatch is used for\n        ``--exclude`` patterns and shell-style is used for the experimental ``--pattern``\n        option.\n\n        If followed by a colon (':') the first two characters of a pattern are used as a\n        style selector. Explicit style selection is necessary when a\n        non-default style is desired or when the desired pattern starts with\n        two alphanumeric characters followed by a colon (i.e. `aa:something/*`).\n\n        `Fnmatch <https://docs.python.org/3/library/fnmatch.html>`_, selector `fm:`\n            This is the default style for ``--exclude`` and ``--exclude-from``.\n            These patterns use a variant of shell pattern syntax, with '\\\\*' matching\n            any number of characters, '?' matching any single character, '[...]'\n            matching any single character specified, including ranges, and '[!...]'\n            matching any character not specified. For the purpose of these patterns,\n            the path separator (backslash for Windows and '/' on other systems) is not\n            treated specially. Wrap meta-characters in brackets for a literal\n            match (i.e. `[?]` to match the literal character `?`). For a path\n            to match a pattern, the full path must match, or it must match\n            from the start of the full path to just before a path separator. Except\n            for the root path, paths will never end in the path separator when\n            matching is attempted.  Thus, if a given pattern ends in a path\n            separator, a '\\\\*' is appended before matching is attempted.\n\n        Shell-style patterns, selector `sh:`\n            This is the default style for ``--pattern`` and ``--patterns-from``.\n            Like fnmatch patterns these are similar to shell patterns. The difference\n            is that the pattern may include `**/` for matching zero or more directory\n            levels, `*` for matching zero or more arbitrary characters with the\n            exception of any path separator.\n\n        Regular expressions, selector `re:`\n            Regular expressions similar to those found in Perl are supported. Unlike\n            shell patterns regular expressions are not required to match the full\n            path and any substring match is sufficient. It is strongly recommended to\n            anchor patterns to the start ('^'), to the end ('$') or both. Path\n            separators (backslash for Windows and '/' on other systems) in paths are\n            always normalized to a forward slash ('/') before applying a pattern. The\n            regular expression syntax is described in the `Python documentation for\n            the re module <https://docs.python.org/3/library/re.html>`_.\n\n        Path prefix, selector `pp:`\n            This pattern style is useful to match whole sub-directories. The pattern\n            `pp:root/somedir` matches `root/somedir` and everything therein.\n\n        Path full-match, selector `pf:`\n            This pattern style is (only) useful to match full paths.\n            This is kind of a pseudo pattern as it can not have any variable or\n            unspecified parts - the full path must be given.\n            `pf:root/file.ext` matches `root/file.txt` only.\n\n            Implementation note: this is implemented via very time-efficient O(1)\n            hashtable lookups (this means you can have huge amounts of such patterns\n            without impacting performance much).\n            Due to that, this kind of pattern does not respect any context or order.\n            If you use such a pattern to include a file, it will always be included\n            (if the directory recursion encounters it).\n            Other include/exclude patterns that would normally match will be ignored.\n            Same logic applies for exclude.\n\n        .. note::\n\n            `re:`, `sh:` and `fm:` patterns are all implemented on top of the Python SRE\n            engine. It is very easy to formulate patterns for each of these types which\n            requires an inordinate amount of time to match paths. If untrusted users\n            are able to supply patterns, ensure they cannot supply `re:` patterns.\n            Further, ensure that `sh:` and `fm:` patterns only contain a handful of\n            wildcards at most.\n\n        Exclusions can be passed via the command line option ``--exclude``. When used\n        from within a shell the patterns should be quoted to protect them from\n        expansion.\n\n        The ``--exclude-from`` option permits loading exclusion patterns from a text\n        file with one pattern per line. Lines empty or starting with the number sign\n        ('#') after removing whitespace on both ends are ignored. The optional style\n        selector prefix is also supported for patterns loaded from a file. Due to\n        whitespace removal paths with whitespace at the beginning or end can only be\n        excluded using regular expressions.\n\n        To test your exclusion patterns without performing an actual backup you can\n        run ``borg create --list --dry-run ...``.\n\n        Examples::\n\n            # Exclude '/home/user/file.o' but not '/home/user/file.odt':\n            $ borg create -e '*.o' backup /\n\n            # Exclude '/home/user/junk' and '/home/user/subdir/junk' but\n            # not '/home/user/importantjunk' or '/etc/junk':\n            $ borg create -e '/home/*/junk' backup /\n\n            # Exclude the contents of '/home/user/cache' but not the directory itself:\n            $ borg create -e /home/user/cache/ backup /\n\n            # The file '/home/user/cache/important' is *not* backed up:\n            $ borg create -e /home/user/cache/ backup / /home/user/cache/important\n\n            # The contents of directories in '/home' are not backed up when their name\n            # ends in '.tmp'\n            $ borg create --exclude 're:^/home/[^/]+\\\\.tmp/' backup /\n\n            # Load exclusions from file\n            $ cat >exclude.txt <<EOF\n            # Comment line\n            /home/*/junk\n            *.tmp\n            fm:aa:something/*\n            re:^/home/[^/]\\\\.tmp/\n            sh:/home/*/.thumbnails\n            EOF\n            $ borg create --exclude-from exclude.txt backup /\n\n        .. container:: experimental\n\n            A more general and easier to use way to define filename matching patterns exists\n            with the experimental ``--pattern`` and ``--patterns-from`` options. Using these, you\n            may specify the backup roots (starting points) and patterns for inclusion/exclusion.\n            A root path starts with the prefix `R`, followed by a path (a plain path, not a\n            file pattern). An include rule starts with the prefix +, an exclude rule starts\n            with the prefix -, an exclude-norecurse rule starts with !, all followed by a pattern.\n\n            .. note::\n\n                Via ``--pattern`` or ``--patterns-from`` you can define BOTH inclusion and exclusion\n                of files using pattern prefixes ``+`` and ``-``. With ``--exclude`` and\n                ``--exlude-from`` ONLY excludes are defined.\n\n            Inclusion patterns are useful to include paths that are contained in an excluded\n            path. The first matching pattern is used so if an include pattern matches before\n            an exclude pattern, the file is backed up. If an exclude-norecurse pattern matches\n            a directory, it won't recurse into it and won't discover any potential matches for\n            include rules below that directory.\n\n            Note that the default pattern style for ``--pattern`` and ``--patterns-from`` is\n            shell style (`sh:`), so those patterns behave similar to rsync include/exclude\n            patterns. The pattern style can be set via the `P` prefix.\n\n            Patterns (``--pattern``) and excludes (``--exclude``) from the command line are\n            considered first (in the order of appearance). Then patterns from ``--patterns-from``\n            are added. Exclusion patterns from ``--exclude-from`` files are appended last.\n\n            Examples::\n\n                # backup pics, but not the ones from 2018, except the good ones:\n                # note: using = is essential to avoid cmdline argument parsing issues.\n                borg create --pattern=+pics/2018/good --pattern=-pics/2018 repo::arch pics\n\n                # use a file with patterns:\n                borg create --patterns-from patterns.lst repo::arch\n\n            The patterns.lst file could look like that::\n\n                # \"sh:\" pattern style is the default, so the following line is not needed:\n                P sh\n                R /\n                # can be rebuild\n                - /home/*/.cache\n                # they're downloads for a reason\n                - /home/*/Downloads\n                # susan is a nice person\n                # include susans home\n                + /home/susan\n                # don't backup the other home directories\n                - /home/*\n                # don't even look in /proc\n                ! /proc\\n\\n''')\n    helptext['placeholders'] = textwrap.dedent('''\n        Repository (or Archive) URLs, ``--prefix``, ``--glob-archives``, ``--comment``\n        and ``--remote-path`` values support these placeholders:\n\n        {hostname}\n            The (short) hostname of the machine.\n\n        {fqdn}\n            The full name of the machine.\n\n        {reverse-fqdn}\n            The full name of the machine in reverse domain name notation.\n\n        {now}\n            The current local date and time, by default in ISO-8601 format.\n            You can also supply your own `format string <https://docs.python.org/3.5/library/datetime.html#strftime-and-strptime-behavior>`_, e.g. {now:%Y-%m-%d_%H:%M:%S}\n\n        {utcnow}\n            The current UTC date and time, by default in ISO-8601 format.\n            You can also supply your own `format string <https://docs.python.org/3.5/library/datetime.html#strftime-and-strptime-behavior>`_, e.g. {utcnow:%Y-%m-%d_%H:%M:%S}\n\n        {user}\n            The user name (or UID, if no name is available) of the user running borg.\n\n        {pid}\n            The current process ID.\n\n        {borgversion}\n            The version of borg, e.g.: 1.0.8rc1\n\n        {borgmajor}\n            The version of borg, only the major version, e.g.: 1\n\n        {borgminor}\n            The version of borg, only major and minor version, e.g.: 1.0\n\n        {borgpatch}\n            The version of borg, only major, minor and patch version, e.g.: 1.0.8\n\n        If literal curly braces need to be used, double them for escaping::\n\n            borg create /path/to/repo::{{literal_text}}\n\n        Examples::\n\n            borg create /path/to/repo::{hostname}-{user}-{utcnow} ...\n            borg create /path/to/repo::{hostname}-{now:%Y-%m-%d_%H:%M:%S} ...\n            borg prune --prefix '{hostname}-' ...\n\n        .. note::\n            systemd uses a difficult, non-standard syntax for command lines in unit files (refer to\n            the `systemd.unit(5)` manual page).\n\n            When invoking borg from unit files, pay particular attention to escaping,\n            especially when using the now/utcnow placeholders, since systemd performs its own\n            %-based variable replacement even in quoted text. To avoid interference from systemd,\n            double all percent signs (``{hostname}-{now:%Y-%m-%d_%H:%M:%S}``\n            becomes ``{hostname}-{now:%%Y-%%m-%%d_%%H:%%M:%%S}``).\\n\\n''')\n    helptext['compression'] = textwrap.dedent('''\n        It is no problem to mix different compression methods in one repo,\n        deduplication is done on the source data chunks (not on the compressed\n        or encrypted data).\n\n        If some specific chunk was once compressed and stored into the repo, creating\n        another backup that also uses this chunk will not change the stored chunk.\n        So if you use different compression specs for the backups, whichever stores a\n        chunk first determines its compression. See also borg recreate.\n\n        Compression is lz4 by default. If you want something else, you have to specify what you want.\n\n        Valid compression specifiers are:\n\n        none\n            Do not compress.\n\n        lz4\n            Use lz4 compression. Very high speed, very low compression. (default)\n\n        zstd[,L]\n            Use zstd (\"zstandard\") compression, a modern wide-range algorithm.\n            If you do not explicitly give the compression level L (ranging from 1\n            to 22), it will use level 3.\n            Archives compressed with zstd are not compatible with borg < 1.1.4.\n\n        zlib[,L]\n            Use zlib (\"gz\") compression. Medium speed, medium compression.\n            If you do not explicitly give the compression level L (ranging from 0\n            to 9), it will use level 6.\n            Giving level 0 (means \"no compression\", but still has zlib protocol\n            overhead) is usually pointless, you better use \"none\" compression.\n\n        lzma[,L]\n            Use lzma (\"xz\") compression. Low speed, high compression.\n            If you do not explicitly give the compression level L (ranging from 0\n            to 9), it will use level 6.\n            Giving levels above 6 is pointless and counterproductive because it does\n            not compress better due to the buffer size used by borg - but it wastes\n            lots of CPU cycles and RAM.\n\n        auto,C[,L]\n            Use a built-in heuristic to decide per chunk whether to compress or not.\n            The heuristic tries with lz4 whether the data is compressible.\n            For incompressible data, it will not use compression (uses \"none\").\n            For compressible data, it uses the given C[,L] compression - with C[,L]\n            being any valid compression specifier.\n\n        Examples::\n\n            borg create --compression lz4 REPO::ARCHIVE data\n            borg create --compression zstd REPO::ARCHIVE data\n            borg create --compression zstd,10 REPO::ARCHIVE data\n            borg create --compression zlib REPO::ARCHIVE data\n            borg create --compression zlib,1 REPO::ARCHIVE data\n            borg create --compression auto,lzma,6 REPO::ARCHIVE data\n            borg create --compression auto,lzma ...\\n\\n''')\n\n    def do_help(self, parser, commands, args):\n        if not args.topic:\n            parser.print_help()\n        elif args.topic in self.helptext:\n            print(rst_to_terminal(self.helptext[args.topic]))\n        elif args.topic in commands:\n            if args.epilog_only:\n                print(commands[args.topic].epilog)\n            elif args.usage_only:\n                commands[args.topic].epilog = None\n                commands[args.topic].print_help()\n            else:\n                commands[args.topic].print_help()\n        else:\n            msg_lines = []\n            msg_lines += ['No help available on %s.' % args.topic]\n            msg_lines += ['Try one of the following:']\n            msg_lines += ['    Commands: %s' % ', '.join(sorted(commands.keys()))]\n            msg_lines += ['    Topics: %s' % ', '.join(sorted(self.helptext.keys()))]\n            parser.error('\\n'.join(msg_lines))\n        return self.exit_code\n\n    def do_subcommand_help(self, parser, args):\n        \"\"\"display infos about subcommand\"\"\"\n        parser.print_help()\n        return EXIT_SUCCESS\n\n    do_maincommand_help = do_subcommand_help\n\n    def preprocess_args(self, args):\n        deprecations = [\n            # ('--old', '--new' or None, 'Warning: \"--old\" has been deprecated. Use \"--new\" instead.'),\n            ('--noatime', None, 'Warning: \"--noatime\" has been deprecated because it is the default now.')\n        ]\n        for i, arg in enumerate(args[:]):\n            for old_name, new_name, warning in deprecations:\n                if arg.startswith(old_name):\n                    if new_name is not None:\n                        args[i] = arg.replace(old_name, new_name)\n                    print(warning, file=sys.stderr)\n        return args\n\n    class CommonOptions:\n        \"\"\"\n        Support class to allow specifying common options directly after the top-level command.\n\n        Normally options can only be specified on the parser defining them, which means\n        that generally speaking *all* options go after all sub-commands. This is annoying\n        for common options in scripts, e.g. --remote-path or logging options.\n\n        This class allows adding the same set of options to both the top-level parser\n        and the final sub-command parsers (but not intermediary sub-commands, at least for now).\n\n        It does so by giving every option's target name (\"dest\") a suffix indicating its level\n        -- no two options in the parser hierarchy can have the same target --\n        then, after parsing the command line, multiple definitions are resolved.\n\n        Defaults are handled by only setting them on the top-level parser and setting\n        a sentinel object in all sub-parsers, which then allows one to discern which parser\n        supplied the option.\n        \"\"\"\n\n        def __init__(self, define_common_options, suffix_precedence):\n            \"\"\"\n            *define_common_options* should be a callable taking one argument, which\n            will be a argparse.Parser.add_argument-like function.\n\n            *define_common_options* will be called multiple times, and should call\n            the passed function to define common options exactly the same way each time.\n\n            *suffix_precedence* should be a tuple of the suffixes that will be used.\n            It is ordered from lowest precedence to highest precedence:\n            An option specified on the parser belonging to index 0 is overridden if the\n            same option is specified on any parser with a higher index.\n            \"\"\"\n            self.define_common_options = define_common_options\n            self.suffix_precedence = suffix_precedence\n\n            # Maps suffixes to sets of target names.\n            # E.g. common_options[\"_subcommand\"] = {..., \"log_level\", ...}\n            self.common_options = dict()\n            # Set of options with the 'append' action.\n            self.append_options = set()\n            # This is the sentinel object that replaces all default values in parsers\n            # below the top-level parser.\n            self.default_sentinel = object()\n\n        def add_common_group(self, parser, suffix, provide_defaults=False):\n            \"\"\"\n            Add common options to *parser*.\n\n            *provide_defaults* must only be True exactly once in a parser hierarchy,\n            at the top level, and False on all lower levels. The default is chosen\n            accordingly.\n\n            *suffix* indicates the suffix to use internally. It also indicates\n            which precedence the *parser* has for common options. See *suffix_precedence*\n            of __init__.\n            \"\"\"\n            assert suffix in self.suffix_precedence\n\n            def add_argument(*args, **kwargs):\n                if 'dest' in kwargs:\n                    kwargs.setdefault('action', 'store')\n                    assert kwargs['action'] in ('help', 'store_const', 'store_true', 'store_false', 'store', 'append')\n                    is_append = kwargs['action'] == 'append'\n                    if is_append:\n                        self.append_options.add(kwargs['dest'])\n                        assert kwargs['default'] == [], 'The default is explicitly constructed as an empty list in resolve()'\n                    else:\n                        self.common_options.setdefault(suffix, set()).add(kwargs['dest'])\n                    kwargs['dest'] += suffix\n                    if not provide_defaults:\n                        # Interpolate help now, in case the %(default)d (or so) is mentioned,\n                        # to avoid producing incorrect help output.\n                        # Assumption: Interpolated output can safely be interpolated again,\n                        # which should always be the case.\n                        # Note: We control all inputs.\n                        kwargs['help'] = kwargs['help'] % kwargs\n                        if not is_append:\n                            kwargs['default'] = self.default_sentinel\n\n                common_group.add_argument(*args, **kwargs)\n\n            common_group = parser.add_argument_group('Common options')\n            self.define_common_options(add_argument)\n\n        def resolve(self, args: argparse.Namespace):  # Namespace has \"in\" but otherwise is not like a dict.\n            \"\"\"\n            Resolve the multiple definitions of each common option to the final value.\n            \"\"\"\n            for suffix in self.suffix_precedence:\n                # From highest level to lowest level, so the \"most-specific\" option wins, e.g.\n                # \"borg --debug create --info\" shall result in --info being effective.\n                for dest in self.common_options.get(suffix, []):\n                    # map_from is this suffix' option name, e.g. log_level_subcommand\n                    # map_to is the target name, e.g. log_level\n                    map_from = dest + suffix\n                    map_to = dest\n                    # Retrieve value; depending on the action it may not exist, but usually does\n                    # (store_const/store_true/store_false), either because the action implied a default\n                    # or a default is explicitly supplied.\n                    # Note that defaults on lower levels are replaced with default_sentinel.\n                    # Only the top level has defaults.\n                    value = getattr(args, map_from, self.default_sentinel)\n                    if value is not self.default_sentinel:\n                        # value was indeed specified on this level. Transfer value to target,\n                        # and un-clobber the args (for tidiness - you *cannot* use the suffixed\n                        # names for other purposes, obviously).\n                        setattr(args, map_to, value)\n                    try:\n                        delattr(args, map_from)\n                    except AttributeError:\n                        pass\n\n            # Options with an \"append\" action need some special treatment. Instead of\n            # overriding values, all specified values are merged together.\n            for dest in self.append_options:\n                option_value = []\n                for suffix in self.suffix_precedence:\n                    # Find values of this suffix, if any, and add them to the final list\n                    extend_from = dest + suffix\n                    if extend_from in args:\n                        values = getattr(args, extend_from)\n                        delattr(args, extend_from)\n                        option_value.extend(values)\n                setattr(args, dest, option_value)\n\n    def build_parser(self):\n        # You can use :ref:`xyz` in the following usage pages. However, for plain-text view,\n        # e.g. through \"borg ... --help\", define a substitution for the reference here.\n        # It will replace the entire :ref:`foo` verbatim.\n        rst_plain_text_references = {\n            'a_status_oddity': '\"I am seeing A (added) status for a unchanged file!?\"',\n            'separate_compaction': '\"Separate compaction\"',\n        }\n\n        def process_epilog(epilog):\n            epilog = textwrap.dedent(epilog).splitlines()\n            try:\n                mode = borg.doc_mode\n            except AttributeError:\n                mode = 'command-line'\n            if mode in ('command-line', 'build_usage'):\n                epilog = [line for line in epilog if not line.startswith('.. man')]\n            epilog = '\\n'.join(epilog)\n            if mode == 'command-line':\n                epilog = rst_to_terminal(epilog, rst_plain_text_references)\n            return epilog\n\n        def define_common_options(add_common_option):\n            add_common_option('-h', '--help', action='help', help='show this help message and exit')\n            add_common_option('--critical', dest='log_level',\n                              action='store_const', const='critical', default='warning',\n                              help='work on log level CRITICAL')\n            add_common_option('--error', dest='log_level',\n                              action='store_const', const='error', default='warning',\n                              help='work on log level ERROR')\n            add_common_option('--warning', dest='log_level',\n                              action='store_const', const='warning', default='warning',\n                              help='work on log level WARNING (default)')\n            add_common_option('--info', '-v', '--verbose', dest='log_level',\n                              action='store_const', const='info', default='warning',\n                              help='work on log level INFO')\n            add_common_option('--debug', dest='log_level',\n                              action='store_const', const='debug', default='warning',\n                              help='enable debug output, work on log level DEBUG')\n            add_common_option('--debug-topic', metavar='TOPIC', dest='debug_topics', action='append', default=[],\n                              help='enable TOPIC debugging (can be specified multiple times). '\n                                   'The logger path is borg.debug.<TOPIC> if TOPIC is not fully qualified.')\n            add_common_option('-p', '--progress', dest='progress', action='store_true',\n                              help='show progress information')\n            add_common_option('--log-json', dest='log_json', action='store_true',\n                              help='Output one JSON object per log line instead of formatted text.')\n            add_common_option('--lock-wait', metavar='SECONDS', dest='lock_wait', type=int, default=1,\n                              help='wait at most SECONDS for acquiring a repository/cache lock (default: %(default)d).')\n            add_common_option('--show-version', dest='show_version', action='store_true',\n                              help='show/log the borg version')\n            add_common_option('--show-rc', dest='show_rc', action='store_true',\n                              help='show/log the return code (rc)')\n            add_common_option('--umask', metavar='M', dest='umask', type=lambda s: int(s, 8), default=UMASK_DEFAULT,\n                              help='set umask to M (local and remote, default: %(default)04o)')\n            add_common_option('--remote-path', metavar='PATH', dest='remote_path',\n                              help='use PATH as borg executable on the remote (default: \"borg\")')\n            add_common_option('--remote-ratelimit', metavar='RATE', dest='remote_ratelimit', type=int,\n                              help='set remote network upload rate limit in kiByte/s (default: 0=unlimited)')\n            add_common_option('--consider-part-files', dest='consider_part_files', action='store_true',\n                              help='treat part files like normal files (e.g. to list/extract them)')\n            add_common_option('--debug-profile', metavar='FILE', dest='debug_profile', default=None,\n                              help='Write execution profile in Borg format into FILE. For local use a Python-'\n                                   'compatible file can be generated by suffixing FILE with \".pyprof\".')\n            add_common_option('--rsh', metavar='RSH', dest='rsh',\n                              help=\"Use this command to connect to the 'borg serve' process (default: 'ssh')\")\n\n        def define_exclude_and_patterns(add_option, *, tag_files=False, strip_components=False):\n            add_option('-e', '--exclude', metavar='PATTERN', dest='patterns',\n                       type=parse_exclude_pattern, action='append',\n                       help='exclude paths matching PATTERN')\n            add_option('--exclude-from', metavar='EXCLUDEFILE', action=ArgparseExcludeFileAction,\n                       help='read exclude patterns from EXCLUDEFILE, one per line')\n            add_option('--pattern', metavar='PATTERN', action=ArgparsePatternAction,\n                       help='experimental: include/exclude paths matching PATTERN')\n            add_option('--patterns-from', metavar='PATTERNFILE', action=ArgparsePatternFileAction,\n                       help='experimental: read include/exclude patterns from PATTERNFILE, one per line')\n\n            if tag_files:\n                add_option('--exclude-caches', dest='exclude_caches', action='store_true',\n                           help='exclude directories that contain a CACHEDIR.TAG file '\n                                '(http://www.bford.info/cachedir/spec.html)')\n                add_option('--exclude-if-present', metavar='NAME', dest='exclude_if_present',\n                           action='append', type=str,\n                           help='exclude directories that are tagged by containing a filesystem object with '\n                                'the given NAME')\n                add_option('--keep-exclude-tags', dest='keep_exclude_tags',\n                           action='store_true',\n                           help='if tag objects are specified with ``--exclude-if-present``, '\n                                'don\\'t omit the tag objects themselves from the backup archive')\n\n            if strip_components:\n                add_option('--strip-components', metavar='NUMBER', dest='strip_components', type=int, default=0,\n                           help='Remove the specified number of leading path elements. '\n                                'Paths with fewer elements will be silently skipped.')\n\n        def define_exclusion_group(subparser, **kwargs):\n            exclude_group = subparser.add_argument_group('Exclusion options')\n            define_exclude_and_patterns(exclude_group.add_argument, **kwargs)\n            return exclude_group\n\n        def define_archive_filters_group(subparser, *, sort_by=True, first_last=True):\n            filters_group = subparser.add_argument_group('Archive filters',\n                                                         'Archive filters can be applied to repository targets.')\n            group = filters_group.add_mutually_exclusive_group()\n            group.add_argument('-P', '--prefix', metavar='PREFIX', dest='prefix', type=PrefixSpec, default=None,\n                               help='only consider archive names starting with this prefix.')\n            group.add_argument('-a', '--glob-archives', metavar='GLOB', dest='glob_archives',\n                               type=GlobSpec, default=None,\n                               help='only consider archive names matching the glob. '\n                                    'sh: rules apply, see \"borg help patterns\". '\n                                    '``--prefix`` and ``--glob-archives`` are mutually exclusive.')\n\n            if sort_by:\n                sort_by_default = 'timestamp'\n                filters_group.add_argument('--sort-by', metavar='KEYS', dest='sort_by',\n                                           type=SortBySpec, default=sort_by_default,\n                                           help='Comma-separated list of sorting keys; valid keys are: {}; default is: {}'\n                                           .format(', '.join(AI_HUMAN_SORT_KEYS), sort_by_default))\n\n            if first_last:\n                group = filters_group.add_mutually_exclusive_group()\n                group.add_argument('--first', metavar='N', dest='first', default=0, type=positive_int_validator,\n                                   help='consider first N archives after other filters were applied')\n                group.add_argument('--last', metavar='N', dest='last', default=0, type=positive_int_validator,\n                                   help='consider last N archives after other filters were applied')\n\n        def define_borg_mount(parser):\n            parser.set_defaults(func=self.do_mount)\n            parser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),\n                                help='repository or archive to mount')\n            parser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,\n                                help='where to mount filesystem')\n            parser.add_argument('-f', '--foreground', dest='foreground',\n                                action='store_true',\n                                help='stay in foreground, do not daemonize')\n            parser.add_argument('-o', dest='options', type=str,\n                                help='Extra mount options')\n            define_archive_filters_group(parser)\n            parser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                                   help='paths to extract; patterns are supported')\n            define_exclusion_group(parser, strip_components=True)\n\n        parser = argparse.ArgumentParser(prog=self.prog, description='Borg - Deduplicated Backups',\n                                         add_help=False)\n        # paths and patterns must have an empty list as default everywhere\n        parser.set_defaults(fallback2_func=functools.partial(self.do_maincommand_help, parser),\n                            paths=[], patterns=[])\n        parser.common_options = self.CommonOptions(define_common_options,\n                                                   suffix_precedence=('_maincommand', '_midcommand', '_subcommand'))\n        parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + __version__,\n                            help='show version number and exit')\n        parser.common_options.add_common_group(parser, '_maincommand', provide_defaults=True)\n\n        common_parser = argparse.ArgumentParser(add_help=False, prog=self.prog)\n        common_parser.set_defaults(paths=[], patterns=[])\n        parser.common_options.add_common_group(common_parser, '_subcommand')\n\n        mid_common_parser = argparse.ArgumentParser(add_help=False, prog=self.prog)\n        mid_common_parser.set_defaults(paths=[], patterns=[])\n        parser.common_options.add_common_group(mid_common_parser, '_midcommand')\n\n        # borg mount\n        mount_epilog = process_epilog(\"\"\"\n        This command mounts an archive as a FUSE filesystem. This can be useful for\n        browsing an archive or restoring individual files. Unless the ``--foreground``\n        option is given the command will run in the background until the filesystem\n        is ``umounted``.\n\n        The command ``borgfs`` provides a wrapper for ``borg mount``. This can also be\n        used in fstab entries:\n        ``/path/to/repo /mnt/point fuse.borgfs defaults,noauto 0 0``\n\n        To allow a regular user to use fstab entries, add the ``user`` option:\n        ``/path/to/repo /mnt/point fuse.borgfs defaults,noauto,user 0 0``\n\n        For FUSE configuration and mount options, see the mount.fuse(8) manual page.\n\n        Additional mount options supported by borg:\n\n        - versions: when used with a repository mount, this gives a merged, versioned\n          view of the files in the archives. EXPERIMENTAL, layout may change in future.\n        - allow_damaged_files: by default damaged files (where missing chunks were\n          replaced with runs of zeros by borg check ``--repair``) are not readable and\n          return EIO (I/O error). Set this option to read such files.\n        - ignore_permissions: for security reasons the \"default_permissions\" mount\n          option is internally enforced by borg. \"ignore_permissions\" can be given to\n          not enforce \"default_permissions\".\n\n        The BORG_MOUNT_DATA_CACHE_ENTRIES environment variable is meant for advanced users\n        to tweak the performance. It sets the number of cached data chunks; additional\n        memory usage can be up to ~8 MiB times this number. The default is the number\n        of CPU cores.\n\n        When the daemonized process receives a signal or crashes, it does not unmount.\n        Unmounting in these cases could cause an active rsync or similar process\n        to unintentionally delete data.\n\n        When running in the foreground ^C/SIGINT unmounts cleanly, but other\n        signals or crashes do not.\n        \"\"\")\n\n        if parser.prog == 'borgfs':\n            parser.description = self.do_mount.__doc__\n            parser.epilog = mount_epilog\n            parser.formatter_class = argparse.RawDescriptionHelpFormatter\n            parser.help = 'mount repository'\n            define_borg_mount(parser)\n            return parser\n\n        subparsers = parser.add_subparsers(title='required arguments', metavar='<command>')\n\n        # borg benchmark\n        benchmark_epilog = process_epilog(\"These commands do various benchmarks.\")\n\n        subparser = subparsers.add_parser('benchmark', parents=[mid_common_parser], add_help=False,\n                                          description='benchmark command',\n                                          epilog=benchmark_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='benchmark command')\n\n        benchmark_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')\n        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))\n\n        bench_crud_epilog = process_epilog(\"\"\"\n        This command benchmarks borg CRUD (create, read, update, delete) operations.\n\n        It creates input data below the given PATH and backups this data into the given REPO.\n        The REPO must already exist (it could be a fresh empty repo or an existing repo, the\n        command will create / read / update / delete some archives named borg-benchmark-crud\\\\* there.\n\n        Make sure you have free space there, you'll need about 1GB each (+ overhead).\n\n        If your repository is encrypted and borg needs a passphrase to unlock the key, use::\n\n            BORG_PASSPHRASE=mysecret borg benchmark crud REPO PATH\n\n        Measurements are done with different input file sizes and counts.\n        The file contents are very artificial (either all zero or all random),\n        thus the measurement results do not necessarily reflect performance with real data.\n        Also, due to the kind of content used, no compression is used in these benchmarks.\n\n        C- == borg create (1st archive creation, no compression, do not use files cache)\n              C-Z- == all-zero files. full dedup, this is primarily measuring reader/chunker/hasher.\n              C-R- == random files. no dedup, measuring throughput through all processing stages.\n\n        R- == borg extract (extract archive, dry-run, do everything, but do not write files to disk)\n              R-Z- == all zero files. Measuring heavily duplicated files.\n              R-R- == random files. No duplication here, measuring throughput through all processing\n              stages, except writing to disk.\n\n        U- == borg create (2nd archive creation of unchanged input files, measure files cache speed)\n              The throughput value is kind of virtual here, it does not actually read the file.\n              U-Z- == needs to check the 2 all-zero chunks' existence in the repo.\n              U-R- == needs to check existence of a lot of different chunks in the repo.\n\n        D- == borg delete archive (delete last remaining archive, measure deletion + compaction)\n              D-Z- == few chunks to delete / few segments to compact/remove.\n              D-R- == many chunks to delete / many segments to compact/remove.\n\n        Please note that there might be quite some variance in these measurements.\n        Try multiple measurements and having a otherwise idle machine (and network, if you use it).\n        \"\"\")\n        subparser = benchmark_parsers.add_parser('crud', parents=[common_parser], add_help=False,\n                                                 description=self.do_benchmark_crud.__doc__,\n                                                 epilog=bench_crud_epilog,\n                                                 formatter_class=argparse.RawDescriptionHelpFormatter,\n                                                 help='benchmarks borg CRUD (create, extract, update, delete).')\n        subparser.set_defaults(func=self.do_benchmark_crud)\n\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to use for benchmark (must exist)')\n\n        subparser.add_argument('path', metavar='PATH', help='path were to create benchmark input data')\n\n        # borg break-lock\n        break_lock_epilog = process_epilog(\"\"\"\n        This command breaks the repository and cache locks.\n        Please use carefully and only while no borg process (on any machine) is\n        trying to access the Cache or the Repository.\n        \"\"\")\n        subparser = subparsers.add_parser('break-lock', parents=[common_parser], add_help=False,\n                                          description=self.do_break_lock.__doc__,\n                                          epilog=break_lock_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='break repository and cache locks')\n        subparser.set_defaults(func=self.do_break_lock)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False),\n                               help='repository for which to break the locks')\n\n        # borg check\n        check_epilog = process_epilog(\"\"\"\n        The check command verifies the consistency of a repository and the corresponding archives.\n\n        First, the underlying repository data files are checked:\n\n        - For all segments the segment magic (header) is checked\n        - For all objects stored in the segments, all metadata (e.g. crc and size) and\n          all data is read. The read data is checked by size and CRC. Bit rot and other\n          types of accidental damage can be detected this way.\n        - If we are in repair mode and a integrity error is detected for a segment,\n          we try to recover as many objects from the segment as possible.\n        - In repair mode, it makes sure that the index is consistent with the data\n          stored in the segments.\n        - If you use a remote repo server via ssh:, the repo check is executed on the\n          repo server without causing significant network traffic.\n        - The repository check can be skipped using the ``--archives-only`` option.\n        - A repository check can be time consuming. Partial checks are possible with the ``--max-duration`` option.\n\n        Second, the consistency and correctness of the archive metadata is verified:\n\n        - Is the repo manifest present? If not, it is rebuilt from archive metadata\n          chunks (this requires reading and decrypting of all metadata and data).\n        - Check if archive metadata chunk is present. if not, remove archive from\n          manifest.\n        - For all files (items) in the archive, for all chunks referenced by these\n          files, check if chunk is present.\n          If a chunk is not present and we are in repair mode, replace it with a same-size\n          replacement chunk of zeros.\n          If a previously lost chunk reappears (e.g. via a later backup) and we are in\n          repair mode, the all-zero replacement chunk will be replaced by the correct chunk.\n          This requires reading of archive and file metadata, but not data.\n        - If we are in repair mode and we checked all the archives: delete orphaned\n          chunks from the repo.\n        - if you use a remote repo server via ssh:, the archive check is executed on\n          the client machine (because if encryption is enabled, the checks will require\n          decryption and this is always done client-side, because key access will be\n          required).\n        - The archive checks can be time consuming, they can be skipped using the\n          ``--repository-only`` option.\n\n        The ``--max-duration`` option can be used to split a long-running repository check into multiple partial checks.\n        After the given number of seconds the check is interrupted. The next partial check will continue where the\n        previous one stopped, until the complete repository has been checked. Example: Assuming a full check took 7\n        hours, then running a daily check with --max-duration=3600 (1 hour) would result in one full check per week.\n\n        Attention: Partial checks can only do way less checks than a full check (only the CRC32 checks on segment file\n        entries are done) and cannot be combined with ``--repair``. Partial checks may therefore be useful only with very\n        large repositories where a full check would take too long. Doing a full repository check aborts a partial check;\n        the next partial check will start from the beginning.\n\n        The ``--verify-data`` option will perform a full integrity verification (as opposed to\n        checking the CRC32 of the segment) of data, which means reading the data from the\n        repository, decrypting and decompressing it. This is a cryptographic verification,\n        which will detect (accidental) corruption. For encrypted repositories it is\n        tamper-resistant as well, unless the attacker has access to the keys.\n\n        It is also very slow.\n        \"\"\")\n        subparser = subparsers.add_parser('check', parents=[common_parser], add_help=False,\n                                          description=self.do_check.__doc__,\n                                          epilog=check_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='verify repository')\n        subparser.set_defaults(func=self.do_check)\n        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',\n                               type=location_validator(),\n                               help='repository or archive to check consistency of')\n        subparser.add_argument('--repository-only', dest='repo_only', action='store_true',\n                               help='only perform repository checks')\n        subparser.add_argument('--archives-only', dest='archives_only', action='store_true',\n                               help='only perform archives checks')\n        subparser.add_argument('--verify-data', dest='verify_data', action='store_true',\n                               help='perform cryptographic archive data integrity verification '\n                                    '(conflicts with ``--repository-only``)')\n        subparser.add_argument('--repair', dest='repair', action='store_true',\n                               help='attempt to repair any inconsistencies found')\n        subparser.add_argument('--save-space', dest='save_space', action='store_true',\n                               help='work slower, but using less space')\n        subparser.add_argument('--max-duration', metavar='SECONDS', dest='max_duration',\n                                   type=int, default=0,\n                                   help='do only a partial repo check for max. SECONDS seconds (Default: unlimited)')\n        define_archive_filters_group(subparser)\n\n        # borg compact\n        compact_epilog = process_epilog(\"\"\"\n        This command frees repository space by compacting segments.\n\n        Use this regularly to avoid running out of space - you do not need to use this\n        after each borg command though. It is especially useful after deleting archives,\n        because only compaction will really free repository space.\n\n        borg compact does not need a key, so it is possible to invoke it from the\n        client or also from the server.\n\n        Depending on the amount of segments that need compaction, it may take a while,\n        so consider using the ``--progress`` option.\n\n        A segment is compacted if the amount of saved space is above the percentage value\n        given by the ``--threshold`` option. If ommitted, a threshold of 10% is used.\n        When using ``--verbose``, borg will output an estimate of the freed space.\n\n        After upgrading borg (server) to 1.2+, you can use ``borg compact --cleanup-commits``\n        to clean up the numerous 17byte commit-only segments that borg 1.1 did not clean up\n        due to a bug. It is enough to do that once per repository.\n\n        See :ref:`separate_compaction` in Additional Notes for more details.\n        \"\"\")\n        subparser = subparsers.add_parser('compact', parents=[common_parser], add_help=False,\n                                          description=self.do_compact.__doc__,\n                                          epilog=compact_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='compact segment files / free space in repo')\n        subparser.set_defaults(func=self.do_compact)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False),\n                               help='repository to compact')\n        subparser.add_argument('--cleanup-commits', dest='cleanup_commits', action='store_true',\n                               help='cleanup commit-only 17-byte segment files')\n        subparser.add_argument('--threshold', metavar='PERCENT', dest='threshold',\n                               type=int, default=10,\n                               help='set minimum threshold for saved space in PERCENT (Default: 10)')\n\n        # borg config\n        config_epilog = process_epilog(\"\"\"\n        This command gets and sets options in a local repository or cache config file.\n        For security reasons, this command only works on local repositories.\n\n        To delete a config value entirely, use ``--delete``. To list the values\n        of the configuration file or the default values, use ``--list``.  To get and existing\n        key, pass only the key name. To set a key, pass both the key name and\n        the new value. Keys can be specified in the format \"section.name\" or\n        simply \"name\"; the section will default to \"repository\" and \"cache\" for\n        the repo and cache configs, respectively.\n\n\n        By default, borg config manipulates the repository config file. Using ``--cache``\n        edits the repository cache's config file instead.\n        \"\"\")\n        subparser = subparsers.add_parser('config', parents=[common_parser], add_help=False,\n                                          description=self.do_config.__doc__,\n                                          epilog=config_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='get and set configuration values')\n        subparser.set_defaults(func=self.do_config)\n        subparser.add_argument('-c', '--cache', dest='cache', action='store_true',\n                               help='get and set values from the repo cache')\n\n        group = subparser.add_mutually_exclusive_group()\n        group.add_argument('-d', '--delete', dest='delete', action='store_true',\n                               help='delete the key from the config file')\n        group.add_argument('-l', '--list', dest='list', action='store_true',\n                               help='list the configuration of the repo')\n\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False, proto='file'),\n                               help='repository to configure')\n        subparser.add_argument('name', metavar='NAME', nargs='?',\n                               help='name of config key')\n        subparser.add_argument('value', metavar='VALUE', nargs='?',\n                               help='new value for key')\n\n        # borg create\n        create_epilog = process_epilog(\"\"\"\n        This command creates a backup archive containing all files found while recursively\n        traversing all paths specified. Paths are added to the archive as they are given,\n        that means if relative paths are desired, the command has to be run from the correct\n        directory.\n\n        When giving '-' as path, borg will read data from standard input and create a\n        file 'stdin' in the created archive from that data.\n\n        The archive will consume almost no disk space for files or parts of files that\n        have already been stored in other archives.\n\n        The archive name needs to be unique. It must not end in '.checkpoint' or\n        '.checkpoint.N' (with N being a number), because these names are used for\n        checkpoints and treated in special ways.\n\n        In the archive name, you may use the following placeholders:\n        {now}, {utcnow}, {fqdn}, {hostname}, {user} and some others.\n\n        Backup speed is increased by not reprocessing files that are already part of\n        existing archives and weren't modified. The detection of unmodified files is\n        done by comparing multiple file metadata values with previous values kept in\n        the files cache.\n\n        This comparison can operate in different modes as given by ``--files-cache``:\n\n        - ctime,size,inode (default)\n        - mtime,size,inode (default behaviour of borg versions older than 1.1.0rc4)\n        - ctime,size (ignore the inode number)\n        - mtime,size (ignore the inode number)\n        - rechunk,ctime (all files are considered modified - rechunk, cache ctime)\n        - rechunk,mtime (all files are considered modified - rechunk, cache mtime)\n        - disabled (disable the files cache, all files considered modified - rechunk)\n\n        inode number: better safety, but often unstable on network filesystems\n\n        Normally, detecting file modifications will take inode information into\n        consideration to improve the reliability of file change detection.\n        This is problematic for files located on sshfs and similar network file\n        systems which do not provide stable inode numbers, such files will always\n        be considered modified. You can use modes without `inode` in this case to\n        improve performance, but reliability of change detection might be reduced.\n\n        ctime vs. mtime: safety vs. speed\n\n        - ctime is a rather safe way to detect changes to a file (metadata and contents)\n          as it can not be set from userspace. But, a metadata-only change will already\n          update the ctime, so there might be some unnecessary chunking/hashing even\n          without content changes. Some filesystems do not support ctime (change time).\n        - mtime usually works and only updates if file contents were changed. But mtime\n          can be arbitrarily set from userspace, e.g. to set mtime back to the same value\n          it had before a content change happened. This can be used maliciously as well as\n          well-meant, but in both cases mtime based cache modes can be problematic.\n\n        The mount points of filesystems or filesystem snapshots should be the same for every\n        creation of a new archive to ensure fast operation. This is because the file cache that\n        is used to determine changed files quickly uses absolute filenames.\n        If this is not possible, consider creating a bind mount to a stable location.\n\n        The ``--progress`` option shows (from left to right) Original, Compressed and Deduplicated\n        (O, C and D, respectively), then the Number of files (N) processed so far, followed by\n        the currently processed path.\n\n        When using ``--stats``, you will get some statistics about how much data was\n        added - the \"This Archive\" deduplicated size there is most interesting as that is\n        how much your repository will grow. Please note that the \"All archives\" stats refer to\n        the state after creation. Also, the ``--stats`` and ``--dry-run`` options are mutually\n        exclusive because the data is not actually compressed and deduplicated during a dry run.\n\n        See the output of the \"borg help patterns\" command for more help on exclude patterns.\n        See the output of the \"borg help placeholders\" command for more help on placeholders.\n\n        .. man NOTES\n\n        The ``--exclude`` patterns are not like tar. In tar ``--exclude`` .bundler/gems will\n        exclude foo/.bundler/gems. In borg it will not, you need to use ``--exclude``\n        '\\\\*/.bundler/gems' to get the same effect. See ``borg help patterns`` for\n        more information.\n\n        In addition to using ``--exclude`` patterns, it is possible to use\n        ``--exclude-if-present`` to specify the name of a filesystem object (e.g. a file\n        or folder name) which, when contained within another folder, will prevent the\n        containing folder from being backed up.  By default, the containing folder and\n        all of its contents will be omitted from the backup.  If, however, you wish to\n        only include the objects specified by ``--exclude-if-present`` in your backup,\n        and not include any other contents of the containing folder, this can be enabled\n        through using the ``--keep-exclude-tags`` option.\n\n        Item flags\n        ++++++++++\n\n        ``--list`` outputs a list of all files, directories and other\n        file system items it considered (no matter whether they had content changes\n        or not). For each item, it prefixes a single-letter flag that indicates type\n        and/or status of the item.\n\n        If you are interested only in a subset of that output, you can give e.g.\n        ``--filter=AME`` and it will only show regular files with A, M or E status (see\n        below).\n\n        A uppercase character represents the status of a regular file relative to the\n        \"files\" cache (not relative to the repo -- this is an issue if the files cache\n        is not used). Metadata is stored in any case and for 'A' and 'M' also new data\n        chunks are stored. For 'U' all data chunks refer to already existing chunks.\n\n        - 'A' = regular file, added (see also :ref:`a_status_oddity` in the FAQ)\n        - 'M' = regular file, modified\n        - 'U' = regular file, unchanged\n        - 'C' = regular file, it changed while we backed it up\n        - 'E' = regular file, an error happened while accessing/reading *this* file\n\n        A lowercase character means a file type other than a regular file,\n        borg usually just stores their metadata:\n\n        - 'd' = directory\n        - 'b' = block device\n        - 'c' = char device\n        - 'h' = regular file, hardlink (to already seen inodes)\n        - 's' = symlink\n        - 'f' = fifo\n\n        Other flags used include:\n\n        - 'i' = backup data was read from standard input (stdin)\n        - '-' = dry run, item was *not* backed up\n        - 'x' = excluded, item was *not* backed up\n        - '?' = missing status code (if you see this, please file a bug report!)\n        \"\"\")\n\n        subparser = subparsers.add_parser('create', parents=[common_parser], add_help=False,\n                                          description=self.do_create.__doc__,\n                                          epilog=create_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='create backup')\n        subparser.set_defaults(func=self.do_create)\n\n        dryrun_group = subparser.add_mutually_exclusive_group()\n        dryrun_group.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                               help='do not create a backup archive')\n        dryrun_group.add_argument('-s', '--stats', dest='stats', action='store_true',\n                               help='print statistics for the created archive')\n\n        subparser.add_argument('--list', dest='output_list', action='store_true',\n                               help='output verbose list of items (files, dirs, ...)')\n        subparser.add_argument('--filter', metavar='STATUSCHARS', dest='output_filter',\n                               help='only display items with the given status characters (see description)')\n        subparser.add_argument('--json', action='store_true',\n                               help='output stats as JSON. Implies ``--stats``.')\n        subparser.add_argument('--no-cache-sync', dest='no_cache_sync', action='store_true',\n                               help='experimental: do not synchronize the cache. Implies not using the files cache.')\n        subparser.add_argument('--stdin-name', metavar='NAME', dest='stdin_name', default='stdin',\n                               help='use NAME in archive for stdin data (default: \"stdin\")')\n\n        exclude_group = define_exclusion_group(subparser, tag_files=True)\n        exclude_group.add_argument('--exclude-nodump', dest='exclude_nodump', action='store_true',\n                                   help='exclude files flagged NODUMP')\n\n        fs_group = subparser.add_argument_group('Filesystem options')\n        fs_group.add_argument('-x', '--one-file-system', dest='one_file_system', action='store_true',\n                              help='stay in the same file system and do not store mount points of other file systems')\n        fs_group.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',\n                              help='only store numeric user and group identifiers')\n        # --noatime is the default now and the flag is deprecated. args.noatime is not used any more.\n        # use --atime if you want to store the atime (default behaviour before borg 1.2.0a7)..\n        fs_group.add_argument('--noatime', dest='noatime', action='store_true',\n                              help='do not store atime into archive')\n        fs_group.add_argument('--atime', dest='atime', action='store_true',\n                              help='do store atime into archive')\n        fs_group.add_argument('--noctime', dest='noctime', action='store_true',\n                              help='do not store ctime into archive')\n        fs_group.add_argument('--nobirthtime', dest='nobirthtime', action='store_true',\n                              help='do not store birthtime (creation date) into archive')\n        fs_group.add_argument('--nobsdflags', dest='nobsdflags', action='store_true',\n                              help='do not read and store bsdflags (e.g. NODUMP, IMMUTABLE) into archive')\n        fs_group.add_argument('--files-cache', metavar='MODE', dest='files_cache_mode',\n                              type=FilesCacheMode, default=DEFAULT_FILES_CACHE_MODE_UI,\n                              help='operate files cache in MODE. default: %s' % DEFAULT_FILES_CACHE_MODE_UI)\n        fs_group.add_argument('--read-special', dest='read_special', action='store_true',\n                              help='open and read block and char device files as well as FIFOs as if they were '\n                                   'regular files. Also follows symlinks pointing to these kinds of files.')\n\n        archive_group = subparser.add_argument_group('Archive options')\n        archive_group.add_argument('--comment', dest='comment', metavar='COMMENT', type=CommentSpec, default='',\n                                   help='add a comment text to the archive')\n        archive_group.add_argument('--timestamp', metavar='TIMESTAMP', dest='timestamp',\n                                   type=timestamp, default=None,\n                                   help='manually specify the archive creation date/time (UTC, yyyy-mm-ddThh:mm:ss format). '\n                                        'Alternatively, give a reference file/directory.')\n        archive_group.add_argument('-c', '--checkpoint-interval', metavar='SECONDS', dest='checkpoint_interval',\n                                   type=int, default=1800,\n                                   help='write checkpoint every SECONDS seconds (Default: 1800)')\n        archive_group.add_argument('--chunker-params', metavar='PARAMS', dest='chunker_params',\n                                   type=ChunkerParams, default=CHUNKER_PARAMS,\n                                   help='specify the chunker parameters (ALGO, CHUNK_MIN_EXP, CHUNK_MAX_EXP, '\n                                        'HASH_MASK_BITS, HASH_WINDOW_SIZE). default: %s,%d,%d,%d,%d' % CHUNKER_PARAMS)\n        archive_group.add_argument('-C', '--compression', metavar='COMPRESSION', dest='compression',\n                                   type=CompressionSpec, default=CompressionSpec('lz4'),\n                                   help='select compression algorithm, see the output of the '\n                                        '\"borg help compression\" command for details.')\n\n        subparser.add_argument('location', metavar='ARCHIVE',\n                               type=location_validator(archive=True),\n                               help='name of archive to create (must be also a valid directory name)')\n        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                               help='paths to archive')\n\n        # borg debug\n        debug_epilog = process_epilog(\"\"\"\n        These commands are not intended for normal use and potentially very\n        dangerous if used incorrectly.\n\n        They exist to improve debugging capabilities without direct system access, e.g.\n        in case you ever run into some severe malfunction. Use them only if you know\n        what you are doing or if a trusted developer tells you what to do.\"\"\")\n\n        subparser = subparsers.add_parser('debug', parents=[mid_common_parser], add_help=False,\n                                          description='debugging command (not intended for normal use)',\n                                          epilog=debug_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='debugging command (not intended for normal use)')\n\n        debug_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')\n        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))\n\n        debug_info_epilog = process_epilog(\"\"\"\n        This command displays some system information that might be useful for bug\n        reports and debugging problems. If a traceback happens, this information is\n        already appended at the end of the traceback.\n        \"\"\")\n        subparser = debug_parsers.add_parser('info', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_info.__doc__,\n                                          epilog=debug_info_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='show system infos for debugging / bug reports (debug)')\n        subparser.set_defaults(func=self.do_debug_info)\n\n        debug_dump_archive_items_epilog = process_epilog(\"\"\"\n        This command dumps raw (but decrypted and decompressed) archive items (only metadata) to files.\n        \"\"\")\n        subparser = debug_parsers.add_parser('dump-archive-items', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_dump_archive_items.__doc__,\n                                          epilog=debug_dump_archive_items_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='dump archive items (metadata) (debug)')\n        subparser.set_defaults(func=self.do_debug_dump_archive_items)\n        subparser.add_argument('location', metavar='ARCHIVE',\n                               type=location_validator(archive=True),\n                               help='archive to dump')\n\n        debug_dump_archive_epilog = process_epilog(\"\"\"\n        This command dumps all metadata of an archive in a decoded form to a file.\n        \"\"\")\n        subparser = debug_parsers.add_parser('dump-archive', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_dump_archive.__doc__,\n                                          epilog=debug_dump_archive_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='dump decoded archive metadata (debug)')\n        subparser.set_defaults(func=self.do_debug_dump_archive)\n        subparser.add_argument('location', metavar='ARCHIVE',\n                               type=location_validator(archive=True),\n                               help='archive to dump')\n        subparser.add_argument('path', metavar='PATH', type=str,\n                               help='file to dump data into')\n\n        debug_dump_manifest_epilog = process_epilog(\"\"\"\n        This command dumps manifest metadata of a repository in a decoded form to a file.\n        \"\"\")\n        subparser = debug_parsers.add_parser('dump-manifest', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_dump_manifest.__doc__,\n                                          epilog=debug_dump_manifest_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='dump decoded repository metadata (debug)')\n        subparser.set_defaults(func=self.do_debug_dump_manifest)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to dump')\n        subparser.add_argument('path', metavar='PATH', type=str,\n                               help='file to dump data into')\n\n        debug_dump_repo_objs_epilog = process_epilog(\"\"\"\n        This command dumps raw (but decrypted and decompressed) repo objects to files.\n        \"\"\")\n        subparser = debug_parsers.add_parser('dump-repo-objs', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_dump_repo_objs.__doc__,\n                                          epilog=debug_dump_repo_objs_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='dump repo objects (debug)')\n        subparser.set_defaults(func=self.do_debug_dump_repo_objs)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to dump')\n        subparser.add_argument('--ghost', dest='ghost', action='store_true',\n                               help='dump all segment file contents, including deleted/uncommitted objects and commits.')\n\n        debug_search_repo_objs_epilog = process_epilog(\"\"\"\n        This command searches raw (but decrypted and decompressed) repo objects for a specific bytes sequence.\n        \"\"\")\n        subparser = debug_parsers.add_parser('search-repo-objs', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_search_repo_objs.__doc__,\n                                          epilog=debug_search_repo_objs_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='search repo objects (debug)')\n        subparser.set_defaults(func=self.do_debug_search_repo_objs)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to search')\n        subparser.add_argument('wanted', metavar='WANTED', type=str,\n                               help='term to search the repo for, either 0x1234abcd hex term or a string')\n\n        debug_get_obj_epilog = process_epilog(\"\"\"\n        This command gets an object from the repository.\n        \"\"\")\n        subparser = debug_parsers.add_parser('get-obj', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_get_obj.__doc__,\n                                          epilog=debug_get_obj_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='get object from repository (debug)')\n        subparser.set_defaults(func=self.do_debug_get_obj)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to use')\n        subparser.add_argument('id', metavar='ID', type=str,\n                               help='hex object ID to get from the repo')\n        subparser.add_argument('path', metavar='PATH', type=str,\n                               help='file to write object data into')\n\n        debug_put_obj_epilog = process_epilog(\"\"\"\n        This command puts objects into the repository.\n        \"\"\")\n        subparser = debug_parsers.add_parser('put-obj', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_put_obj.__doc__,\n                                          epilog=debug_put_obj_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='put object to repository (debug)')\n        subparser.set_defaults(func=self.do_debug_put_obj)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to use')\n        subparser.add_argument('paths', metavar='PATH', nargs='+', type=str,\n                               help='file(s) to read and create object(s) from')\n\n        debug_delete_obj_epilog = process_epilog(\"\"\"\n        This command deletes objects from the repository.\n        \"\"\")\n        subparser = debug_parsers.add_parser('delete-obj', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_delete_obj.__doc__,\n                                          epilog=debug_delete_obj_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='delete object from repository (debug)')\n        subparser.set_defaults(func=self.do_debug_delete_obj)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to use')\n        subparser.add_argument('ids', metavar='IDs', nargs='+', type=str,\n                               help='hex object ID(s) to delete from the repo')\n\n        debug_refcount_obj_epilog = process_epilog(\"\"\"\n        This command displays the reference count for objects from the repository.\n        \"\"\")\n        subparser = debug_parsers.add_parser('refcount-obj', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_refcount_obj.__doc__,\n                                          epilog=debug_refcount_obj_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='show refcount for object from repository (debug)')\n        subparser.set_defaults(func=self.do_debug_refcount_obj)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to use')\n        subparser.add_argument('ids', metavar='IDs', nargs='+', type=str,\n                               help='hex object ID(s) to show refcounts for')\n\n        debug_convert_profile_epilog = process_epilog(\"\"\"\n        Convert a Borg profile to a Python cProfile compatible profile.\n        \"\"\")\n        subparser = debug_parsers.add_parser('convert-profile', parents=[common_parser], add_help=False,\n                                          description=self.do_debug_convert_profile.__doc__,\n                                          epilog=debug_convert_profile_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='convert Borg profile to Python profile (debug)')\n        subparser.set_defaults(func=self.do_debug_convert_profile)\n        subparser.add_argument('input', metavar='INPUT', type=argparse.FileType('rb'),\n                               help='Borg profile')\n        subparser.add_argument('output', metavar='OUTPUT', type=argparse.FileType('wb'),\n                               help='Output file')\n\n        # borg delete\n        delete_epilog = process_epilog(\"\"\"\n        This command deletes an archive from the repository or the complete repository.\n\n        Important: When deleting archives, repository disk space is **not** freed until\n        you run ``borg compact``.\n\n        If you delete the complete repository, the local cache for it (if any) is\n        also deleted. Alternatively, you can delete just the local cache with the\n        ``--cache-only`` option.\n\n        When using ``--stats``, you will get some statistics about how much data was\n        deleted - the \"Deleted data\" deduplicated size there is most interesting as\n        that is how much your repository will shrink.\n        Please note that the \"All archives\" stats refer to the state after deletion.\n\n        You can delete multiple archives by specifying their common prefix, if they\n        have one, using the ``--prefix PREFIX`` option. You can also specify a shell\n        pattern to match multiple archives using the ``--glob-archives GLOB`` option\n        (for more info on these patterns, see ``borg help patterns``). Note that these\n        two options are mutually exclusive.\n\n        To avoid accidentally deleting archives, especially when using glob patterns,\n        it might be helpful to use the ``--dry-run`` to test out the command without\n        actually making any changes to the repository.\n        \"\"\")\n        subparser = subparsers.add_parser('delete', parents=[common_parser], add_help=False,\n                                          description=self.do_delete.__doc__,\n                                          epilog=delete_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='delete archive')\n        subparser.set_defaults(func=self.do_delete)\n        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                               help='do not change repository')\n        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',\n                               help='print statistics for the deleted archive')\n        subparser.add_argument('--cache-only', dest='cache_only', action='store_true',\n                               help='delete only the local cache for the given repository')\n        subparser.add_argument('--force', dest='forced',\n                               action='count', default=0,\n                               help='force deletion of corrupted archives, '\n                                    'use ``--force --force`` in case ``--force`` does not work.')\n        subparser.add_argument('--save-space', dest='save_space', action='store_true',\n                               help='work slower, but using less space')\n        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',\n                               type=location_validator(),\n                               help='repository or archive to delete')\n        subparser.add_argument('archives', metavar='ARCHIVE', nargs='*',\n                               help='archives to delete')\n        define_archive_filters_group(subparser)\n\n        # borg diff\n        diff_epilog = process_epilog(\"\"\"\n            This command finds differences (file contents, user/group/mode) between archives.\n\n            A repository location and an archive name must be specified for REPO::ARCHIVE1.\n            ARCHIVE2 is just another archive name in same repository (no repository location\n            allowed).\n\n            For archives created with Borg 1.1 or newer diff automatically detects whether\n            the archives are created with the same chunker params. If so, only chunk IDs\n            are compared, which is very fast.\n\n            For archives prior to Borg 1.1 chunk contents are compared by default.\n            If you did not create the archives with different chunker params,\n            pass ``--same-chunker-params``.\n            Note that the chunker params changed from Borg 0.xx to 1.0.\n\n            See the output of the \"borg help patterns\" command for more help on exclude patterns.\n            \"\"\")\n        subparser = subparsers.add_parser('diff', parents=[common_parser], add_help=False,\n                                          description=self.do_diff.__doc__,\n                                          epilog=diff_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='find differences in archive contents')\n        subparser.set_defaults(func=self.do_diff)\n        subparser.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',\n                               help='only consider numeric user and group identifiers')\n        subparser.add_argument('--same-chunker-params', dest='same_chunker_params', action='store_true',\n                               help='Override check of chunker parameters.')\n        subparser.add_argument('--sort', dest='sort', action='store_true',\n                               help='Sort the output lines by file path.')\n        subparser.add_argument('location', metavar='REPO::ARCHIVE1',\n                               type=location_validator(archive=True),\n                               help='repository location and ARCHIVE1 name')\n        subparser.add_argument('archive2', metavar='ARCHIVE2',\n                               type=archivename_validator(),\n                               help='ARCHIVE2 name (no repository location allowed)')\n        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                               help='paths of items inside the archives to compare; patterns are supported')\n        define_exclusion_group(subparser)\n\n        # borg export-tar\n        export_tar_epilog = process_epilog(\"\"\"\n        This command creates a tarball from an archive.\n\n        When giving '-' as the output FILE, Borg will write a tar stream to standard output.\n\n        By default (``--tar-filter=auto``) Borg will detect whether the FILE should be compressed\n        based on its file extension and pipe the tarball through an appropriate filter\n        before writing it to FILE:\n\n        - .tar.gz: gzip\n        - .tar.bz2: bzip2\n        - .tar.xz: xz\n\n        Alternatively a ``--tar-filter`` program may be explicitly specified. It should\n        read the uncompressed tar stream from stdin and write a compressed/filtered\n        tar stream to stdout.\n\n        The generated tarball uses the GNU tar format.\n\n        export-tar is a lossy conversion:\n        BSD flags, ACLs, extended attributes (xattrs), atime and ctime are not exported.\n        Timestamp resolution is limited to whole seconds, not the nanosecond resolution\n        otherwise supported by Borg.\n\n        A ``--sparse`` option (as found in borg extract) is not supported.\n\n        By default the entire archive is extracted but a subset of files and directories\n        can be selected by passing a list of ``PATHs`` as arguments.\n        The file selection can further be restricted by using the ``--exclude`` option.\n\n        See the output of the \"borg help patterns\" command for more help on exclude patterns.\n\n        ``--progress`` can be slower than no progress display, since it makes one additional\n        pass over the archive metadata.\n        \"\"\")\n        subparser = subparsers.add_parser('export-tar', parents=[common_parser], add_help=False,\n                                          description=self.do_export_tar.__doc__,\n                                          epilog=export_tar_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='create tarball from archive')\n        subparser.set_defaults(func=self.do_export_tar)\n        subparser.add_argument('--tar-filter', dest='tar_filter', default='auto',\n                               help='filter program to pipe data through')\n        subparser.add_argument('--list', dest='output_list', action='store_true',\n                               help='output verbose list of items (files, dirs, ...)')\n        subparser.add_argument('location', metavar='ARCHIVE',\n                               type=location_validator(archive=True),\n                               help='archive to export')\n        subparser.add_argument('tarfile', metavar='FILE',\n                               help='output tar file. \"-\" to write to stdout instead.')\n        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                               help='paths to extract; patterns are supported')\n        define_exclusion_group(subparser, strip_components=True)\n\n        # borg extract\n        extract_epilog = process_epilog(\"\"\"\n        This command extracts the contents of an archive. By default the entire\n        archive is extracted but a subset of files and directories can be selected\n        by passing a list of ``PATHs`` as arguments. The file selection can further\n        be restricted by using the ``--exclude`` option.\n\n        See the output of the \"borg help patterns\" command for more help on exclude patterns.\n\n        By using ``--dry-run``, you can do all extraction steps except actually writing the\n        output data: reading metadata and data chunks from the repo, checking the hash/hmac,\n        decrypting, decompressing.\n\n        ``--progress`` can be slower than no progress display, since it makes one additional\n        pass over the archive metadata.\n\n        .. note::\n\n            Currently, extract always writes into the current working directory (\".\"),\n            so make sure you ``cd`` to the right place before calling ``borg extract``.\n        \"\"\")\n        subparser = subparsers.add_parser('extract', parents=[common_parser], add_help=False,\n                                          description=self.do_extract.__doc__,\n                                          epilog=extract_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='extract archive contents')\n        subparser.set_defaults(func=self.do_extract)\n        subparser.add_argument('--list', dest='output_list', action='store_true',\n                               help='output verbose list of items (files, dirs, ...)')\n        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                               help='do not actually change any files')\n        subparser.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',\n                               help='only obey numeric user and group identifiers')\n        subparser.add_argument('--nobsdflags', dest='nobsdflags', action='store_true',\n                               help='do not extract/set bsdflags (e.g. NODUMP, IMMUTABLE)')\n        subparser.add_argument('--stdout', dest='stdout', action='store_true',\n                               help='write all extracted data to stdout')\n        subparser.add_argument('--sparse', dest='sparse', action='store_true',\n                               help='create holes in output sparse file from all-zero chunks')\n        subparser.add_argument('location', metavar='ARCHIVE',\n                               type=location_validator(archive=True),\n                               help='archive to extract')\n        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                               help='paths to extract; patterns are supported')\n        define_exclusion_group(subparser, strip_components=True)\n\n        # borg help\n        subparser = subparsers.add_parser('help', parents=[common_parser], add_help=False,\n                                          description='Extra help')\n        subparser.add_argument('--epilog-only', dest='epilog_only', action='store_true')\n        subparser.add_argument('--usage-only', dest='usage_only', action='store_true')\n        subparser.set_defaults(func=functools.partial(self.do_help, parser, subparsers.choices))\n        subparser.add_argument('topic', metavar='TOPIC', type=str, nargs='?',\n                               help='additional help on TOPIC')\n\n        # borg info\n        info_epilog = process_epilog(\"\"\"\n        This command displays detailed information about the specified archive or repository.\n\n        Please note that the deduplicated sizes of the individual archives do not add\n        up to the deduplicated size of the repository (\"all archives\"), because the two\n        are meaning different things:\n\n        This archive / deduplicated size = amount of data stored ONLY for this archive\n        = unique chunks of this archive.\n        All archives / deduplicated size = amount of data stored in the repo\n        = all chunks in the repository.\n\n        Borg archives can only contain a limited amount of file metadata.\n        The size of an archive relative to this limit depends on a number of factors,\n        mainly the number of files, the lengths of paths and other metadata stored for files.\n        This is shown as *utilization of maximum supported archive size*.\n        \"\"\")\n        subparser = subparsers.add_parser('info', parents=[common_parser], add_help=False,\n                                          description=self.do_info.__doc__,\n                                          epilog=info_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='show repository or archive information')\n        subparser.set_defaults(func=self.do_info)\n        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',\n                               type=location_validator(),\n                               help='repository or archive to display information about')\n        subparser.add_argument('--json', action='store_true',\n                               help='format output as JSON')\n        define_archive_filters_group(subparser)\n\n        # borg init\n        init_epilog = process_epilog(\"\"\"\n        This command initializes an empty repository. A repository is a filesystem\n        directory containing the deduplicated data from zero or more archives.\n\n        Encryption can be enabled at repository init time. It cannot be changed later.\n\n        It is not recommended to work without encryption. Repository encryption protects\n        you e.g. against the case that an attacker has access to your backup repository.\n\n        But be careful with the key / the passphrase:\n\n        If you want \"passphrase-only\" security, use one of the repokey modes. The\n        key will be stored inside the repository (in its \"config\" file). In above\n        mentioned attack scenario, the attacker will have the key (but not the\n        passphrase).\n\n        If you want \"passphrase and having-the-key\" security, use one of the keyfile\n        modes. The key will be stored in your home directory (in .config/borg/keys).\n        In the attack scenario, the attacker who has just access to your repo won't\n        have the key (and also not the passphrase).\n\n        Make a backup copy of the key file (keyfile mode) or repo config file\n        (repokey mode) and keep it at a safe place, so you still have the key in\n        case it gets corrupted or lost. Also keep the passphrase at a safe place.\n        The backup that is encrypted with that key won't help you with that, of course.\n\n        Make sure you use a good passphrase. Not too short, not too simple. The real\n        encryption / decryption key is encrypted with / locked by your passphrase.\n        If an attacker gets your key, he can't unlock and use it without knowing the\n        passphrase.\n\n        Be careful with special or non-ascii characters in your passphrase:\n\n        - Borg processes the passphrase as unicode (and encodes it as utf-8),\n          so it does not have problems dealing with even the strangest characters.\n        - BUT: that does not necessarily apply to your OS / VM / keyboard configuration.\n\n        So better use a long passphrase made from simple ascii chars than one that\n        includes non-ascii stuff or characters that are hard/impossible to enter on\n        a different keyboard layout.\n\n        You can change your passphrase for existing repos at any time, it won't affect\n        the encryption/decryption key or other secrets.\n\n        Encryption modes\n        ++++++++++++++++\n\n        .. nanorst: inline-fill\n\n        +----------+---------------+------------------------+--------------------------+\n        | Hash/MAC | Not encrypted | Not encrypted,         | Encrypted (AEAD w/ AES)  |\n        |          | no auth       | but authenticated      | and authenticated        |\n        +----------+---------------+------------------------+--------------------------+\n        | SHA-256  | none          | `authenticated`        | repokey                  |\n        |          |               |                        | keyfile                  |\n        +----------+---------------+------------------------+--------------------------+\n        | BLAKE2b  | n/a           | `authenticated-blake2` | `repokey-blake2`         |\n        |          |               |                        | `keyfile-blake2`         |\n        +----------+---------------+------------------------+--------------------------+\n\n        .. nanorst: inline-replace\n\n        `Marked modes` are new in Borg 1.1 and are not backwards-compatible with Borg 1.0.x.\n\n        On modern Intel/AMD CPUs (except very cheap ones), AES is usually\n        hardware-accelerated.\n        BLAKE2b is faster than SHA256 on Intel/AMD 64-bit CPUs\n        (except AMD Ryzen and future CPUs with SHA extensions),\n        which makes `authenticated-blake2` faster than `none` and `authenticated`.\n\n        On modern ARM CPUs, NEON provides hardware acceleration for SHA256 making it faster\n        than BLAKE2b-256 there. NEON accelerates AES as well.\n\n        Hardware acceleration is always used automatically when available.\n\n        `repokey` and `keyfile` use AES-CTR-256 for encryption and HMAC-SHA256 for\n        authentication in an encrypt-then-MAC (EtM) construction. The chunk ID hash\n        is HMAC-SHA256 as well (with a separate key).\n        These modes are compatible with Borg 1.0.x.\n\n        `repokey-blake2` and `keyfile-blake2` are also authenticated encryption modes,\n        but use BLAKE2b-256 instead of HMAC-SHA256 for authentication. The chunk ID\n        hash is a keyed BLAKE2b-256 hash.\n        These modes are new and *not* compatible with Borg 1.0.x.\n\n        `authenticated` mode uses no encryption, but authenticates repository contents\n        through the same HMAC-SHA256 hash as the `repokey` and `keyfile` modes (it uses it\n        as the chunk ID hash). The key is stored like `repokey`.\n        This mode is new and *not* compatible with Borg 1.0.x.\n\n        `authenticated-blake2` is like `authenticated`, but uses the keyed BLAKE2b-256 hash\n        from the other blake2 modes.\n        This mode is new and *not* compatible with Borg 1.0.x.\n\n        `none` mode uses no encryption and no authentication. It uses SHA256 as chunk\n        ID hash. Not recommended, rather consider using an authenticated or\n        authenticated/encrypted mode. This mode has possible denial-of-service issues\n        when running ``borg create`` on contents controlled by an attacker.\n        Use it only for new repositories where no encryption is wanted **and** when compatibility\n        with 1.0.x is important. If compatibility with 1.0.x is not important, use\n        `authenticated-blake2` or `authenticated` instead.\n        This mode is compatible with Borg 1.0.x.\n        \"\"\")\n        subparser = subparsers.add_parser('init', parents=[common_parser], add_help=False,\n                                          description=self.do_init.__doc__, epilog=init_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='initialize empty repository')\n        subparser.set_defaults(func=self.do_init)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False),\n                               help='repository to create')\n        subparser.add_argument('-e', '--encryption', metavar='MODE', dest='encryption', required=True,\n                               choices=key_argument_names(),\n                               help='select encryption key mode **(required)**')\n        subparser.add_argument('--append-only', dest='append_only', action='store_true',\n                               help='create an append-only mode repository. Note that this only affects '\n                                    'the low level structure of the repository, and running `delete` '\n                                    'or `prune` will still be allowed. See :ref:`append_only_mode` in '\n                                    'Additional Notes for more details.')\n        subparser.add_argument('--storage-quota', metavar='QUOTA', dest='storage_quota', default=None,\n                               type=parse_storage_quota,\n                               help='Set storage quota of the new repository (e.g. 5G, 1.5T). Default: no quota.')\n        subparser.add_argument('--make-parent-dirs', dest='make_parent_dirs', action='store_true',\n                               help='create the parent directories of the repository directory, if they are missing.')\n\n        # borg key\n        subparser = subparsers.add_parser('key', parents=[mid_common_parser], add_help=False,\n                                          description=\"Manage a keyfile or repokey of a repository\",\n                                          epilog=\"\",\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='manage repository key')\n\n        key_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')\n        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))\n\n        key_export_epilog = process_epilog(\"\"\"\n        If repository encryption is used, the repository is inaccessible\n        without the key. This command allows one to backup this essential key.\n        Note that the backup produced does not include the passphrase itself\n        (i.e. the exported key stays encrypted). In order to regain access to a\n        repository, one needs both the exported key and the original passphrase.\n\n        There are three backup formats. The normal backup format is suitable for\n        digital storage as a file. The ``--paper`` backup format is optimized\n        for printing and typing in while importing, with per line checks to\n        reduce problems with manual input. The ``--qr-html`` creates a printable\n        HTML template with a QR code and a copy of the ``--paper``-formatted key.\n\n        For repositories using keyfile encryption the key is saved locally\n        on the system that is capable of doing backups. To guard against loss\n        of this key, the key needs to be backed up independently of the main\n        data backup.\n\n        For repositories using the repokey encryption the key is saved in the\n        repository in the config file. A backup is thus not strictly needed,\n        but guards against the repository becoming inaccessible if the file\n        is damaged for some reason.\n        \"\"\")\n        subparser = key_parsers.add_parser('export', parents=[common_parser], add_help=False,\n                                          description=self.do_key_export.__doc__,\n                                          epilog=key_export_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='export repository key for backup')\n        subparser.set_defaults(func=self.do_key_export)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False))\n        subparser.add_argument('path', metavar='PATH', nargs='?', type=str,\n                               help='where to store the backup')\n        subparser.add_argument('--paper', dest='paper', action='store_true',\n                               help='Create an export suitable for printing and later type-in')\n        subparser.add_argument('--qr-html', dest='qr', action='store_true',\n                               help='Create an html file suitable for printing and later type-in or qr scan')\n\n        key_import_epilog = process_epilog(\"\"\"\n        This command restores a key previously backed up with the export command.\n\n        If the ``--paper`` option is given, the import will be an interactive\n        process in which each line is checked for plausibility before\n        proceeding to the next line. For this format PATH must not be given.\n        \"\"\")\n        subparser = key_parsers.add_parser('import', parents=[common_parser], add_help=False,\n                                          description=self.do_key_import.__doc__,\n                                          epilog=key_import_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='import repository key from backup')\n        subparser.set_defaults(func=self.do_key_import)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False))\n        subparser.add_argument('path', metavar='PATH', nargs='?', type=str,\n                               help='path to the backup (\\'-\\' to read from stdin)')\n        subparser.add_argument('--paper', dest='paper', action='store_true',\n                               help='interactively import from a backup done with ``--paper``')\n\n        change_passphrase_epilog = process_epilog(\"\"\"\n        The key files used for repository encryption are optionally passphrase\n        protected. This command can be used to change this passphrase.\n\n        Please note that this command only changes the passphrase, but not any\n        secret protected by it (like e.g. encryption/MAC keys or chunker seed).\n        Thus, changing the passphrase after passphrase and borg key got compromised\n        does not protect future (nor past) backups to the same repository.\n        \"\"\")\n        subparser = key_parsers.add_parser('change-passphrase', parents=[common_parser], add_help=False,\n                                          description=self.do_change_passphrase.__doc__,\n                                          epilog=change_passphrase_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='change repository passphrase')\n        subparser.set_defaults(func=self.do_change_passphrase)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False))\n\n        migrate_to_repokey_epilog = process_epilog(\"\"\"\n        This command migrates a repository from passphrase mode (removed in Borg 1.0)\n        to repokey mode.\n\n        You will be first asked for the repository passphrase (to open it in passphrase\n        mode). This is the same passphrase as you used to use for this repo before 1.0.\n\n        It will then derive the different secrets from this passphrase.\n\n        Then you will be asked for a new passphrase (twice, for safety). This\n        passphrase will be used to protect the repokey (which contains these same\n        secrets in encrypted form). You may use the same passphrase as you used to\n        use, but you may also use a different one.\n\n        After migrating to repokey mode, you can change the passphrase at any time.\n        But please note: the secrets will always stay the same and they could always\n        be derived from your (old) passphrase-mode passphrase.\n        \"\"\")\n        subparser = key_parsers.add_parser('migrate-to-repokey', parents=[common_parser], add_help=False,\n                                          description=self.do_migrate_to_repokey.__doc__,\n                                          epilog=migrate_to_repokey_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='migrate passphrase-mode repository to repokey')\n        subparser.set_defaults(func=self.do_migrate_to_repokey)\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False))\n\n        # borg list\n        list_epilog = process_epilog(\"\"\"\n        This command lists the contents of a repository or an archive.\n\n        See the \"borg help patterns\" command for more help on exclude patterns.\n\n        .. man NOTES\n\n        The following keys are available for ``--format``:\n\n\n        \"\"\") + BaseFormatter.keys_help() + textwrap.dedent(\"\"\"\n\n        Keys for listing repository archives:\n\n        \"\"\") + ArchiveFormatter.keys_help() + textwrap.dedent(\"\"\"\n\n        Keys for listing archive files:\n\n        \"\"\") + ItemFormatter.keys_help()\n        subparser = subparsers.add_parser('list', parents=[common_parser], add_help=False,\n                                          description=self.do_list.__doc__,\n                                          epilog=list_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='list archive or repository contents')\n        subparser.set_defaults(func=self.do_list)\n        subparser.add_argument('--short', dest='short', action='store_true',\n                               help='only print file/directory names, nothing else')\n        subparser.add_argument('--format', '--list-format', metavar='FORMAT', dest='format',\n                               help='specify format for file listing '\n                                    '(default: \"{mode} {user:6} {group:6} {size:8d} {mtime} {path}{extra}{NL}\")')\n        subparser.add_argument('--json', action='store_true',\n                               help='Only valid for listing repository contents. Format output as JSON. '\n                                    'The form of ``--format`` is ignored, '\n                                    'but keys used in it are added to the JSON output. '\n                                    'Some keys are always present. Note: JSON can only represent text. '\n                                    'A \"barchive\" key is therefore not available.')\n        subparser.add_argument('--json-lines', action='store_true',\n                               help='Only valid for listing archive contents. Format output as JSON Lines. '\n                                    'The form of ``--format`` is ignored, '\n                                    'but keys used in it are added to the JSON output. '\n                                    'Some keys are always present. Note: JSON can only represent text. '\n                                    'A \"bpath\" key is therefore not available.')\n        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',\n                               type=location_validator(),\n                               help='repository or archive to list contents of')\n        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                               help='paths to list; patterns are supported')\n        define_archive_filters_group(subparser)\n        define_exclusion_group(subparser)\n\n        subparser = subparsers.add_parser('mount', parents=[common_parser], add_help=False,\n                                        description=self.do_mount.__doc__,\n                                        epilog=mount_epilog,\n                                        formatter_class=argparse.RawDescriptionHelpFormatter,\n                                        help='mount repository')\n        define_borg_mount(subparser)\n\n        # borg prune\n        prune_epilog = process_epilog(\"\"\"\n        The prune command prunes a repository by deleting all archives not matching\n        any of the specified retention options.\n\n        Important: Repository disk space is **not** freed until you run ``borg compact``.\n\n        This command is normally used by automated backup scripts wanting to keep a\n        certain number of historic backups.\n\n        Also, prune automatically removes checkpoint archives (incomplete archives left\n        behind by interrupted backup runs) except if the checkpoint is the latest\n        archive (and thus still needed). Checkpoint archives are not considered when\n        comparing archive counts against the retention limits (``--keep-X``).\n\n        If a prefix is set with -P, then only archives that start with the prefix are\n        considered for deletion and only those archives count towards the totals\n        specified by the rules.\n        Otherwise, *all* archives in the repository are candidates for deletion!\n        There is no automatic distinction between archives representing different\n        contents. These need to be distinguished by specifying matching prefixes.\n\n        If you have multiple sequences of archives with different data sets (e.g.\n        from different machines) in one shared repository, use one prune call per\n        data set that matches only the respective archives using the -P option.\n\n        The ``--keep-within`` option takes an argument of the form \"<int><char>\",\n        where char is \"H\", \"d\", \"w\", \"m\", \"y\". For example, ``--keep-within 2d`` means\n        to keep all archives that were created within the past 48 hours.\n        \"1m\" is taken to mean \"31d\". The archives kept with this option do not\n        count towards the totals specified by any other options.\n\n        A good procedure is to thin out more and more the older your backups get.\n        As an example, ``--keep-daily 7`` means to keep the latest backup on each day,\n        up to 7 most recent days with backups (days without backups do not count).\n        The rules are applied from secondly to yearly, and backups selected by previous\n        rules do not count towards those of later rules. The time that each backup\n        starts is used for pruning purposes. Dates and times are interpreted in\n        the local timezone, and weeks go from Monday to Sunday. Specifying a\n        negative number of archives to keep means that there is no limit.\n\n        The ``--keep-last N`` option is doing the same as ``--keep-secondly N`` (and it will\n        keep the last N archives under the assumption that you do not create more than one\n        backup archive in the same second).\n\n        When using ``--stats``, you will get some statistics about how much data was\n        deleted - the \"Deleted data\" deduplicated size there is most interesting as\n        that is how much your repository will shrink.\n        Please note that the \"All archives\" stats refer to the state after pruning.\n        \"\"\")\n        subparser = subparsers.add_parser('prune', parents=[common_parser], add_help=False,\n                                          description=self.do_prune.__doc__,\n                                          epilog=prune_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='prune archives')\n        subparser.set_defaults(func=self.do_prune)\n        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                               help='do not change repository')\n        subparser.add_argument('--force', dest='forced', action='store_true',\n                               help='force pruning of corrupted archives')\n        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',\n                               help='print statistics for the deleted archive')\n        subparser.add_argument('--list', dest='output_list', action='store_true',\n                               help='output verbose list of archives it keeps/prunes')\n        subparser.add_argument('--keep-within', metavar='INTERVAL', dest='within', type=interval,\n                               help='keep all archives within this time interval')\n        subparser.add_argument('--keep-last', '--keep-secondly', dest='secondly', type=int, default=0,\n                               help='number of secondly archives to keep')\n        subparser.add_argument('--keep-minutely', dest='minutely', type=int, default=0,\n                               help='number of minutely archives to keep')\n        subparser.add_argument('-H', '--keep-hourly', dest='hourly', type=int, default=0,\n                               help='number of hourly archives to keep')\n        subparser.add_argument('-d', '--keep-daily', dest='daily', type=int, default=0,\n                               help='number of daily archives to keep')\n        subparser.add_argument('-w', '--keep-weekly', dest='weekly', type=int, default=0,\n                               help='number of weekly archives to keep')\n        subparser.add_argument('-m', '--keep-monthly', dest='monthly', type=int, default=0,\n                               help='number of monthly archives to keep')\n        subparser.add_argument('-y', '--keep-yearly', dest='yearly', type=int, default=0,\n                               help='number of yearly archives to keep')\n        define_archive_filters_group(subparser, sort_by=False, first_last=False)\n        subparser.add_argument('--save-space', dest='save_space', action='store_true',\n                               help='work slower, but using less space')\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False),\n                               help='repository to prune')\n\n        # borg recreate\n        recreate_epilog = process_epilog(\"\"\"\n        Recreate the contents of existing archives.\n\n        This is an *experimental* feature. Do *not* use this on your only backup.\n\n        Important: Repository disk space is **not** freed until you run ``borg compact``.\n\n        ``--exclude``, ``--exclude-from``, ``--exclude-if-present``, ``--keep-exclude-tags``, and PATH\n        have the exact same semantics as in \"borg create\". If PATHs are specified the\n        resulting archive will only contain files from these PATHs.\n\n        Note that all paths in an archive are relative, therefore absolute patterns/paths\n        will *not* match (``--exclude``, ``--exclude-from``, PATHs).\n\n        ``--recompress`` allows one to change the compression of existing data in archives.\n        Due to how Borg stores compressed size information this might display\n        incorrect information for archives that were not recreated at the same time.\n        There is no risk of data loss by this.\n\n        ``--chunker-params`` will re-chunk all files in the archive, this can be\n        used to have upgraded Borg 0.xx or Attic archives deduplicate with\n        Borg 1.x archives.\n\n        **USE WITH CAUTION.**\n        Depending on the PATHs and patterns given, recreate can be used to permanently\n        delete files from archives.\n        When in doubt, use ``--dry-run --verbose --list`` to see how patterns/PATHS are\n        interpreted.\n\n        The archive being recreated is only removed after the operation completes. The\n        archive that is built during the operation exists at the same time at\n        \"<ARCHIVE>.recreate\". The new archive will have a different archive ID.\n\n        With ``--target`` the original archive is not replaced, instead a new archive is created.\n\n        When rechunking (or recompressing), space usage can be substantial - expect\n        at least the entire deduplicated size of the archives using the previous\n        chunker (or compression) params.\n\n        If you recently ran borg check --repair and it had to fix lost chunks with all-zero\n        replacement chunks, please first run another backup for the same data and re-run\n        borg check --repair afterwards to heal any archives that had lost chunks which are\n        still generated from the input data.\n\n        Important: running borg recreate to re-chunk will remove the chunks_healthy\n        metadata of all items with replacement chunks, so healing will not be possible\n        any more after re-chunking (it is also unlikely it would ever work: due to the\n        change of chunking parameters, the missing chunk likely will never be seen again\n        even if you still have the data that produced it).\n        \"\"\")\n        subparser = subparsers.add_parser('recreate', parents=[common_parser], add_help=False,\n                                          description=self.do_recreate.__doc__,\n                                          epilog=recreate_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help=self.do_recreate.__doc__)\n        subparser.set_defaults(func=self.do_recreate)\n        subparser.add_argument('--list', dest='output_list', action='store_true',\n                               help='output verbose list of items (files, dirs, ...)')\n        subparser.add_argument('--filter', metavar='STATUSCHARS', dest='output_filter',\n                               help='only display items with the given status characters (listed in borg create --help)')\n        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                               help='do not change anything')\n        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',\n                               help='print statistics at end')\n\n        define_exclusion_group(subparser, tag_files=True)\n\n        archive_group = subparser.add_argument_group('Archive options')\n        archive_group.add_argument('--target', dest='target', metavar='TARGET', default=None,\n                                   type=archivename_validator(),\n                                   help='create a new archive with the name ARCHIVE, do not replace existing archive '\n                                        '(only applies for a single archive)')\n        archive_group.add_argument('-c', '--checkpoint-interval', dest='checkpoint_interval',\n                                   type=int, default=1800, metavar='SECONDS',\n                                   help='write checkpoint every SECONDS seconds (Default: 1800)')\n        archive_group.add_argument('--comment', dest='comment', metavar='COMMENT', type=CommentSpec, default=None,\n                                   help='add a comment text to the archive')\n        archive_group.add_argument('--timestamp', metavar='TIMESTAMP', dest='timestamp',\n                                   type=timestamp, default=None,\n                                   help='manually specify the archive creation date/time (UTC, yyyy-mm-ddThh:mm:ss format). '\n                                        'alternatively, give a reference file/directory.')\n        archive_group.add_argument('-C', '--compression', metavar='COMPRESSION', dest='compression',\n                                   type=CompressionSpec, default=CompressionSpec('lz4'),\n                                   help='select compression algorithm, see the output of the '\n                                        '\"borg help compression\" command for details.')\n        archive_group.add_argument('--recompress', metavar='MODE', dest='recompress', nargs='?',\n                                   default='never', const='if-different', choices=('never', 'if-different', 'always'),\n                                   help='recompress data chunks according to ``--compression``. '\n                                        'MODE `if-different`: '\n                                        'recompress if current compression is with a different compression algorithm '\n                                        '(the level is not considered). '\n                                        'MODE `always`: '\n                                        'recompress even if current compression is with the same compression algorithm '\n                                        '(use this to change the compression level). '\n                                        'MODE `never` (default): '\n                                        'do not recompress.')\n        archive_group.add_argument('--chunker-params', metavar='PARAMS', dest='chunker_params',\n                                   type=ChunkerParams, default=CHUNKER_PARAMS,\n                                   help='specify the chunker parameters (ALGO, CHUNK_MIN_EXP, CHUNK_MAX_EXP, '\n                                        'HASH_MASK_BITS, HASH_WINDOW_SIZE) or `default` to use the current defaults. '\n                                        'default: %s,%d,%d,%d,%d' % CHUNKER_PARAMS)\n\n        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',\n                               type=location_validator(),\n                               help='repository or archive to recreate')\n        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,\n                               help='paths to recreate; patterns are supported')\n\n        # borg rename\n        rename_epilog = process_epilog(\"\"\"\n        This command renames an archive in the repository.\n\n        This results in a different archive ID.\n        \"\"\")\n        subparser = subparsers.add_parser('rename', parents=[common_parser], add_help=False,\n                                          description=self.do_rename.__doc__,\n                                          epilog=rename_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='rename archive')\n        subparser.set_defaults(func=self.do_rename)\n        subparser.add_argument('location', metavar='ARCHIVE',\n                               type=location_validator(archive=True),\n                               help='archive to rename')\n        subparser.add_argument('name', metavar='NEWNAME',\n                               type=archivename_validator(),\n                               help='the new archive name to use')\n\n        # borg serve\n        serve_epilog = process_epilog(\"\"\"\n        This command starts a repository server process. This command is usually not used manually.\n        \"\"\")\n        subparser = subparsers.add_parser('serve', parents=[common_parser], add_help=False,\n                                          description=self.do_serve.__doc__, epilog=serve_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='start repository server process')\n        subparser.set_defaults(func=self.do_serve)\n        subparser.add_argument('--restrict-to-path', metavar='PATH', dest='restrict_to_paths', action='append',\n                               help='restrict repository access to PATH. '\n                                    'Can be specified multiple times to allow the client access to several directories. '\n                                    'Access to all sub-directories is granted implicitly; PATH doesn\\'t need to directly point to a repository.')\n        subparser.add_argument('--restrict-to-repository', metavar='PATH', dest='restrict_to_repositories', action='append',\n                                help='restrict repository access. Only the repository located at PATH '\n                                     '(no sub-directories are considered) is accessible. '\n                                     'Can be specified multiple times to allow the client access to several repositories. '\n                                     'Unlike ``--restrict-to-path`` sub-directories are not accessible; '\n                                     'PATH needs to directly point at a repository location. '\n                                     'PATH may be an empty directory or the last element of PATH may not exist, in which case '\n                                     'the client may initialize a repository there.')\n        subparser.add_argument('--append-only', dest='append_only', action='store_true',\n                               help='only allow appending to repository segment files. Note that this only '\n                                    'affects the low level structure of the repository, and running `delete` '\n                                    'or `prune` will still be allowed. See :ref:`append_only_mode` in Additional '\n                                    'Notes for more details.')\n        subparser.add_argument('--storage-quota', metavar='QUOTA', dest='storage_quota',\n                               type=parse_storage_quota, default=None,\n                               help='Override storage quota of the repository (e.g. 5G, 1.5T). '\n                                    'When a new repository is initialized, sets the storage quota on the new '\n                                    'repository as well. Default: no quota.')\n\n        # borg umount\n        umount_epilog = process_epilog(\"\"\"\n        This command un-mounts a FUSE filesystem that was mounted with ``borg mount``.\n\n        This is a convenience wrapper that just calls the platform-specific shell\n        command - usually this is either umount or fusermount -u.\n        \"\"\")\n        subparser = subparsers.add_parser('umount', parents=[common_parser], add_help=False,\n                                          description=self.do_umount.__doc__,\n                                          epilog=umount_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='umount repository')\n        subparser.set_defaults(func=self.do_umount)\n        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,\n                               help='mountpoint of the filesystem to umount')\n\n        # borg upgrade\n        upgrade_epilog = process_epilog(\"\"\"\n        Upgrade an existing, local Borg repository.\n\n        When you do not need borg upgrade\n        +++++++++++++++++++++++++++++++++\n\n        Not every change requires that you run ``borg upgrade``.\n\n        You do **not** need to run it when:\n\n        - moving your repository to a different place\n        - upgrading to another point release (like 1.0.x to 1.0.y),\n          except when noted otherwise in the changelog\n        - upgrading from 1.0.x to 1.1.x,\n          except when noted otherwise in the changelog\n\n        Borg 1.x.y upgrades\n        +++++++++++++++++++\n\n        Use ``borg upgrade --tam REPO`` to require manifest authentication\n        introduced with Borg 1.0.9 to address security issues. This means\n        that modifying the repository after doing this with a version prior\n        to 1.0.9 will raise a validation error, so only perform this upgrade\n        after updating all clients using the repository to 1.0.9 or newer.\n\n        This upgrade should be done on each client for safety reasons.\n\n        If a repository is accidentally modified with a pre-1.0.9 client after\n        this upgrade, use ``borg upgrade --tam --force REPO`` to remedy it.\n\n        If you routinely do this you might not want to enable this upgrade\n        (which will leave you exposed to the security issue). You can\n        reverse the upgrade by issuing ``borg upgrade --disable-tam REPO``.\n\n        See\n        https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability\n        for details.\n\n        Attic and Borg 0.xx to Borg 1.x\n        +++++++++++++++++++++++++++++++\n\n        This currently supports converting an Attic repository to Borg and also\n        helps with converting Borg 0.xx to 1.0.\n\n        Currently, only LOCAL repositories can be upgraded (issue #465).\n\n        Please note that ``borg create`` (since 1.0.0) uses bigger chunks by\n        default than old borg or attic did, so the new chunks won't deduplicate\n        with the old chunks in the upgraded repository.\n        See ``--chunker-params`` option of ``borg create`` and ``borg recreate``.\n\n        ``borg upgrade`` will change the magic strings in the repository's\n        segments to match the new Borg magic strings. The keyfiles found in\n        $ATTIC_KEYS_DIR or ~/.attic/keys/ will also be converted and\n        copied to $BORG_KEYS_DIR or ~/.config/borg/keys.\n\n        The cache files are converted, from $ATTIC_CACHE_DIR or\n        ~/.cache/attic to $BORG_CACHE_DIR or ~/.cache/borg, but the\n        cache layout between Borg and Attic changed, so it is possible\n        the first backup after the conversion takes longer than expected\n        due to the cache resync.\n\n        Upgrade should be able to resume if interrupted, although it\n        will still iterate over all segments. If you want to start\n        from scratch, use `borg delete` over the copied repository to\n        make sure the cache files are also removed::\n\n            borg delete borg\n\n        Unless ``--inplace`` is specified, the upgrade process first creates a backup\n        copy of the repository, in REPOSITORY.before-upgrade-DATETIME, using hardlinks.\n        This requires that the repository and its parent directory reside on same\n        filesystem so the hardlink copy can work.\n        This takes longer than in place upgrades, but is much safer and gives\n        progress information (as opposed to ``cp -al``). Once you are satisfied\n        with the conversion, you can safely destroy the backup copy.\n\n        WARNING: Running the upgrade in place will make the current\n        copy unusable with older version, with no way of going back\n        to previous versions. This can PERMANENTLY DAMAGE YOUR\n        REPOSITORY!  Attic CAN NOT READ BORG REPOSITORIES, as the\n        magic strings have changed. You have been warned.\"\"\")\n        subparser = subparsers.add_parser('upgrade', parents=[common_parser], add_help=False,\n                                          description=self.do_upgrade.__doc__,\n                                          epilog=upgrade_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='upgrade repository format')\n        subparser.set_defaults(func=self.do_upgrade)\n        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',\n                               help='do not change repository')\n        subparser.add_argument('--inplace', dest='inplace', action='store_true',\n                               help='rewrite repository in place, with no chance of going back '\n                                    'to older versions of the repository.')\n        subparser.add_argument('--force', dest='force', action='store_true',\n                               help='Force upgrade')\n        subparser.add_argument('--tam', dest='tam', action='store_true',\n                               help='Enable manifest authentication (in key and cache) (Borg 1.0.9 and later).')\n        subparser.add_argument('--disable-tam', dest='disable_tam', action='store_true',\n                               help='Disable manifest authentication (in key and cache).')\n        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',\n                               type=location_validator(archive=False),\n                               help='path to the repository to be upgraded')\n\n        # borg with-lock\n        with_lock_epilog = process_epilog(\"\"\"\n        This command runs a user-specified command while the repository lock is held.\n\n        It will first try to acquire the lock (make sure that no other operation is\n        running in the repo), then execute the given command as a subprocess and wait\n        for its termination, release the lock and return the user command's return\n        code as borg's return code.\n\n        .. note::\n\n            If you copy a repository with the lock held, the lock will be present in\n            the copy. Thus, before using borg on the copy from a different host,\n            you need to use \"borg break-lock\" on the copied repository, because\n            Borg is cautious and does not automatically remove stale locks made by a different host.\n        \"\"\")\n        subparser = subparsers.add_parser('with-lock', parents=[common_parser], add_help=False,\n                                          description=self.do_with_lock.__doc__,\n                                          epilog=with_lock_epilog,\n                                          formatter_class=argparse.RawDescriptionHelpFormatter,\n                                          help='run user command with lock held')\n        subparser.set_defaults(func=self.do_with_lock)\n        subparser.add_argument('location', metavar='REPOSITORY',\n                               type=location_validator(archive=False),\n                               help='repository to lock')\n        subparser.add_argument('command', metavar='COMMAND',\n                               help='command to run')\n        subparser.add_argument('args', metavar='ARGS', nargs=argparse.REMAINDER,\n                               help='command arguments')\n\n        return parser\n\n    def get_args(self, argv, cmd):\n        \"\"\"usually, just returns argv, except if we deal with a ssh forced command for borg serve.\"\"\"\n        result = self.parse_args(argv[1:])\n        if cmd is not None and result.func == self.do_serve:\n            # borg serve case:\n            # - \"result\" is how borg got invoked (e.g. via forced command from authorized_keys),\n            # - \"client_result\" (from \"cmd\") refers to the command the client wanted to execute,\n            #   which might be different in the case of a forced command or same otherwise.\n            client_argv = shlex.split(cmd)\n            # Drop environment variables (do *not* interpret them) before trying to parse\n            # the borg command line.\n            client_argv = list(itertools.dropwhile(lambda arg: '=' in arg, client_argv))\n            client_result = self.parse_args(client_argv[1:])\n            if client_result.func == result.func:\n                # make sure we only process like normal if the client is executing\n                # the same command as specified in the forced command, otherwise\n                # just skip this block and return the forced command (== result).\n                # client is allowed to specify the whitelisted options,\n                # everything else comes from the forced \"borg serve\" command (or the defaults).\n                # stuff from blacklist must never be used from the client.\n                blacklist = {\n                    'restrict_to_paths',\n                    'restrict_to_repositories',\n                    'append_only',\n                    'storage_quota',\n                }\n                whitelist = {\n                    'debug_topics',\n                    'lock_wait',\n                    'log_level',\n                    'umask',\n                }\n                not_present = object()\n                for attr_name in whitelist:\n                    assert attr_name not in blacklist, 'whitelist has blacklisted attribute name %s' % attr_name\n                    value = getattr(client_result, attr_name, not_present)\n                    if value is not not_present:\n                        # note: it is not possible to specify a whitelisted option via a forced command,\n                        # it always gets overridden by the value specified (or defaulted to) by the client commmand.\n                        setattr(result, attr_name, value)\n\n        return result\n\n    def parse_args(self, args=None):\n        # We can't use argparse for \"serve\" since we don't want it to show up in \"Available commands\"\n        if args:\n            args = self.preprocess_args(args)\n        parser = self.build_parser()\n        args = parser.parse_args(args or ['-h'])\n        parser.common_options.resolve(args)\n        func = get_func(args)\n        if func == self.do_create and not args.paths:\n            # need at least 1 path but args.paths may also be populated from patterns\n            parser.error('Need at least one PATH argument.')\n        return args\n\n    def prerun_checks(self, logger, is_serve):\n        if not is_serve:\n            # this is the borg *client*, we need to check the python:\n            check_python()\n        check_extension_modules()\n        selftest(logger)\n\n    def _setup_implied_logging(self, args):\n        \"\"\" turn on INFO level logging for args that imply that they will produce output \"\"\"\n        # map of option name to name of logger for that option\n        option_logger = {\n            'output_list': 'borg.output.list',\n            'show_version': 'borg.output.show-version',\n            'show_rc': 'borg.output.show-rc',\n            'stats': 'borg.output.stats',\n            'progress': 'borg.output.progress',\n        }\n        for option, logger_name in option_logger.items():\n            option_set = args.get(option, False)\n            logging.getLogger(logger_name).setLevel('INFO' if option_set else 'WARN')\n\n    def _setup_topic_debugging(self, args):\n        \"\"\"Turn on DEBUG level logging for specified --debug-topics.\"\"\"\n        for topic in args.debug_topics:\n            if '.' not in topic:\n                topic = 'borg.debug.' + topic\n            logger.debug('Enabling debug topic %s', topic)\n            logging.getLogger(topic).setLevel('DEBUG')\n\n    def run(self, args):\n        os.umask(args.umask)  # early, before opening files\n        self.lock_wait = args.lock_wait\n        func = get_func(args)\n        # do not use loggers before this!\n        is_serve = func == self.do_serve\n        setup_logging(level=args.log_level, is_serve=is_serve, json=args.log_json)\n        self.log_json = args.log_json\n        args.progress |= is_serve\n        self._setup_implied_logging(vars(args))\n        self._setup_topic_debugging(args)\n        if getattr(args, 'stats', False) and getattr(args, 'dry_run', False):\n            # the data needed for --stats is not computed when using --dry-run, so we can't do it.\n            # for ease of scripting, we just ignore --stats when given with --dry-run.\n            logger.warning(\"Ignoring --stats. It is not supported when using --dry-run.\")\n            args.stats = False\n        if args.show_version:\n            logging.getLogger('borg.output.show-version').info('borgbackup version %s' % __version__)\n        self.prerun_checks(logger, is_serve)\n        if not is_supported_msgpack():\n            logger.error(\"You do not have a supported version of the msgpack python package installed. Terminating.\")\n            logger.error(\"This should never happen as specific, supported versions are required by our setup.py.\")\n            logger.error(\"Do not contact borgbackup support about this.\")\n            return set_ec(EXIT_ERROR)\n        if is_slow_msgpack():\n            logger.warning(PURE_PYTHON_MSGPACK_WARNING)\n        if args.debug_profile:\n            # Import only when needed - avoids a further increase in startup time\n            import cProfile\n            import marshal\n            logger.debug('Writing execution profile to %s', args.debug_profile)\n            # Open the file early, before running the main program, to avoid\n            # a very late crash in case the specified path is invalid.\n            with open(args.debug_profile, 'wb') as fd:\n                profiler = cProfile.Profile()\n                variables = dict(locals())\n                profiler.enable()\n                try:\n                    return set_ec(func(args))\n                finally:\n                    profiler.disable()\n                    profiler.snapshot_stats()\n                    if args.debug_profile.endswith('.pyprof'):\n                        marshal.dump(profiler.stats, fd)\n                    else:\n                        # We use msgpack here instead of the marshal module used by cProfile itself,\n                        # because the latter is insecure. Since these files may be shared over the\n                        # internet we don't want a format that is impossible to interpret outside\n                        # an insecure implementation.\n                        # See scripts/msgpack2marshal.py for a small script that turns a msgpack file\n                        # into a marshal file that can be read by e.g. pyprof2calltree.\n                        # For local use it's unnecessary hassle, though, that's why .pyprof makes\n                        # it compatible (see above).\n                        # We do not use our msgpack wrapper here, but directly call mp_pack.\n                        msgpack.mp_pack(profiler.stats, fd, use_bin_type=True)\n        else:\n            return set_ec(func(args))\n\n\ndef sig_info_handler(sig_no, stack):  # pragma: no cover\n    \"\"\"search the stack for infos about the currently processed file and print them\"\"\"\n    with signal_handler(sig_no, signal.SIG_IGN):\n        for frame in inspect.getouterframes(stack):\n            func, loc = frame[3], frame[0].f_locals\n            if func in ('process_file', '_process', ):  # create op\n                path = loc['path']\n                try:\n                    pos = loc['fd'].tell()\n                    total = loc['st'].st_size\n                except Exception:\n                    pos, total = 0, 0\n                logger.info(\"{0} {1}/{2}\".format(path, format_file_size(pos), format_file_size(total)))\n                break\n            if func in ('extract_item', ):  # extract op\n                path = loc['item'].path\n                try:\n                    pos = loc['fd'].tell()\n                except Exception:\n                    pos = 0\n                logger.info(\"{0} {1}/???\".format(path, format_file_size(pos)))\n                break\n\n\ndef sig_trace_handler(sig_no, stack):  # pragma: no cover\n    print('\\nReceived SIGUSR2 at %s, dumping trace...' % datetime.now().replace(microsecond=0), file=sys.stderr)\n    faulthandler.dump_traceback()\n\n\ndef main():  # pragma: no cover\n    # Make sure stdout and stderr have errors='replace' to avoid unicode\n    # issues when print()-ing unicode file names\n    sys.stdout = ErrorIgnoringTextIOWrapper(sys.stdout.buffer, sys.stdout.encoding, 'replace', line_buffering=True)\n    sys.stderr = ErrorIgnoringTextIOWrapper(sys.stderr.buffer, sys.stderr.encoding, 'replace', line_buffering=True)\n\n    # If we receive SIGINT (ctrl-c), SIGTERM (kill) or SIGHUP (kill -HUP),\n    # catch them and raise a proper exception that can be handled for an\n    # orderly exit.\n    # SIGHUP is important especially for systemd systems, where logind\n    # sends it when a session exits, in addition to any traditional use.\n    # Output some info if we receive SIGUSR1 or SIGINFO (ctrl-t).\n\n    # Register fault handler for SIGSEGV, SIGFPE, SIGABRT, SIGBUS and SIGILL.\n    faulthandler.enable()\n    with signal_handler('SIGINT', raising_signal_handler(KeyboardInterrupt)), \\\n         signal_handler('SIGHUP', raising_signal_handler(SigHup)), \\\n         signal_handler('SIGTERM', raising_signal_handler(SigTerm)), \\\n         signal_handler('SIGUSR1', sig_info_handler), \\\n         signal_handler('SIGUSR2', sig_trace_handler), \\\n         signal_handler('SIGINFO', sig_info_handler):\n        archiver = Archiver()\n        msg = msgid = tb = None\n        tb_log_level = logging.ERROR\n        try:\n            args = archiver.get_args(sys.argv, os.environ.get('SSH_ORIGINAL_COMMAND'))\n        except Error as e:\n            msg = e.get_message()\n            tb_log_level = logging.ERROR if e.traceback else logging.DEBUG\n            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())\n            # we might not have logging setup yet, so get out quickly\n            print(msg, file=sys.stderr)\n            if tb_log_level == logging.ERROR:\n                print(tb, file=sys.stderr)\n            sys.exit(e.exit_code)\n        try:\n            with sig_int:\n                exit_code = archiver.run(args)\n        except Error as e:\n            msg = e.get_message()\n            msgid = type(e).__qualname__\n            tb_log_level = logging.ERROR if e.traceback else logging.DEBUG\n            tb = \"%s\\n%s\" % (traceback.format_exc(), sysinfo())\n            exit_code = e.exit_code\n        except RemoteRepository.RPCError as e:\n            important = e.exception_class not in ('LockTimeout', ) and e.traceback\n            msgid = e.exception_class\n            tb_log_level = logging.ERROR if important else logging.DEBUG\n            if important:\n                msg = e.exception_full\n            else:\n                msg = e.get_message()\n            tb = '\\n'.join('Borg server: ' + l for l in e.sysinfo.splitlines())\n            tb += \"\\n\" + sysinfo()\n            exit_code = EXIT_ERROR\n        except Exception:\n            msg = 'Local Exception'\n            msgid = 'Exception'\n            tb_log_level = logging.ERROR\n            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())\n            exit_code = EXIT_ERROR\n        except KeyboardInterrupt:\n            msg = 'Keyboard interrupt'\n            tb_log_level = logging.DEBUG\n            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())\n            exit_code = EXIT_ERROR\n        except SigTerm:\n            msg = 'Received SIGTERM'\n            msgid = 'Signal.SIGTERM'\n            tb_log_level = logging.DEBUG\n            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())\n            exit_code = EXIT_ERROR\n        except SigHup:\n            msg = 'Received SIGHUP.'\n            msgid = 'Signal.SIGHUP'\n            exit_code = EXIT_ERROR\n        if msg:\n            logger.error(msg, msgid=msgid)\n        if tb:\n            logger.log(tb_log_level, tb)\n        if args.show_rc:\n            rc_logger = logging.getLogger('borg.output.show-rc')\n            exit_msg = 'terminating with %s status, rc %d'\n            if exit_code == EXIT_SUCCESS:\n                rc_logger.info(exit_msg % ('success', exit_code))\n            elif exit_code == EXIT_WARNING:\n                rc_logger.warning(exit_msg % ('warning', exit_code))\n            elif exit_code == EXIT_ERROR:\n                rc_logger.error(exit_msg % ('error', exit_code))\n            else:\n                rc_logger.error(exit_msg % ('abnormal', exit_code or 666))\n        sys.exit(exit_code)\n\n\nif __name__ == '__main__':\n    main()\n",
  "patch": "@@ -3383,10 +3383,11 @@ class Archiver:\n                                help='print statistics for the deleted archive')\n         subparser.add_argument('--cache-only', dest='cache_only', action='store_true',\n                                help='delete only the local cache for the given repository')\n-        subparser.add_argument('--force', dest='forced',\n-                               action='count', default=0,\n+        subparser.add_argument('--force', dest='forced', action='count', default=0,\n                                help='force deletion of corrupted archives, '\n                                     'use ``--force --force`` in case ``--force`` does not work.')\n+        subparser.add_argument('--keep-security-info', dest='keep_security_info', action='store_true',\n+                               help='keep the local security info when deleting a repository')\n         subparser.add_argument('--save-space', dest='save_space', action='store_true',\n                                help='work slower, but using less space')\n         subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',\n",
  "msg": "... **local** security info ... (see help text of --cache-only).",
  "id": 23795,
  "y": 1
}

--- Sample 5 ---
{
  "oldf": "",
  "patch": "@@ -0,0 +1,123 @@\n+using System;\n+using System.Collections.Generic;\n+using Content.Shared.GameObjects.Components.Sound;\n+using SS14.Client.GameObjects.EntitySystems;\n+using SS14.Shared.GameObjects;\n+using SS14.Shared.Interfaces.GameObjects;\n+using SS14.Shared.Interfaces.Network;\n+using SS14.Shared.Interfaces.Timers;\n+using SS14.Shared.IoC;\n+using SS14.Shared.Log;\n+using SS14.Shared.Serialization;\n+using SS14.Shared.Timers;\n+\n+namespace Content.Client.GameObjects.Components.Sound\n+{\n+    public class SoundComponent : SharedSoundComponent\n+    {\n+        private readonly List<ScheduledSound> _schedules = new List<ScheduledSound>();\n+        private AudioSystem _audioSystem;\n+        private readonly Random Random = new Random();\n+\n+        public override void StopAllSounds()\n+        {\n+            foreach (var schedule in _schedules)\n+            {\n+                schedule.Play = false;\n+            }\n+            _schedules.Clear();\n+        }\n+\n+        public override void StopScheduledSound(string filename)\n+        {\n+            foreach (var schedule in _schedules)\n+            {\n+                if (schedule.Filename != filename) continue;\n+                schedule.Play = false;\n+                _schedules.Remove(schedule);\n+            }\n+        }\n+\n+        public override void AddScheduledSound(ScheduledSound schedule)\n+        {\n+            _schedules.Add(schedule);\n+            Play(schedule);\n+        }\n+\n+        public void Play(ScheduledSound schedule)\n+        {\n+            if (!schedule.Play) return;\n+\n+            Timer.Spawn((int) schedule.Delay + (Random.Next((int) schedule.RandomDelay)),() =>\n+                {\n+                    if (!schedule.Play) return; // We make sure this hasn't changed.\n+                    if (_audioSystem == null) _audioSystem = IoCManager.Resolve<IEntitySystemManager>().GetEntitySystem<AudioSystem>();\n+                    switch (schedule.SoundType)\n+                    {\n+                        case SoundType.Normal:\n+                            _audioSystem.Play(schedule.Filename,\n+                                schedule.EntityUid != null ? Owner.EntityManager.GetEntity((EntityUid) schedule.EntityUid)\n+                                    : Owner, schedule.AudioParams);\n+                            break;\n+                        case SoundType.Global:\n+                            _audioSystem.Play(schedule.Filename, schedule.AudioParams);\n+                            break;\n+                        case SoundType.Positional:\n+                            _audioSystem.Play(schedule.Filename, schedule.SoundPosition, schedule.AudioParams);\n+                            break;\n+                        default:\n+                            throw new ArgumentOutOfRangeException();\n+                    }\n+\n+                    if (schedule.Times == 0)\n+                    {\n+                        _schedules.Remove(schedule);\n+                        return;\n+                    }\n+\n+                    if (schedule.Times > 0)\n+                        schedule.Times--;\n+\n+                    Play(schedule);\n+                });\n+        }\n+\n+        public override void HandleMessage(ComponentMessage message, INetChannel netChannel = null, IComponent component = null)\n+        {\n+            base.HandleMessage(message, netChannel, component);\n+            switch (message)\n+            {\n+                case ScheduledSoundMessage msg:\n+                    AddScheduledSound(msg.Schedule);\n+                    break;\n+\n+                case StopSoundScheduleMessage msg:\n+                    StopScheduledSound(msg.Filename);\n+                    break;\n+\n+                case StopAllSoundsMessage msg:\n+                    StopAllSounds();\n+                    break;\n+            }\n+        }\n+\n+        public override void Initialize()\n+        {\n+            base.Initialize();\n+            IoCManager.Resolve<IEntitySystemManager>().TryGetEntitySystem(out _audioSystem);\n+        }\n+\n+        public override void ExposeData(ObjectSerializer serializer)\n+        {\n+            base.ExposeData(serializer);\n+            if (serializer.Writing) return;\n+            serializer.TryReadDataField(\"schedules\", out List<ScheduledSound> schedules);\n+            if (schedules == null) return;\n+            foreach (var schedule in schedules)\n+            {\n+                if (schedule == null) continue;\n+                AddScheduledSound(schedule);\n+            }\n+        }\n+    }\n+}\n",
  "msg": "You should avoid creating a random like this, since it'll be based on system time it could easily get the same seed as other sound components during say map load. Personally what I did to solve this on other components was to make it based on system time AND entity UID: `new Random(Owner.Uid.GetHashCode() ^ DateTime.Now.GetHashCode());`",
  "id": 26717,
  "y": 1
}



Cross-dataset key summary
-------------------------
Keys present in how many datasets (key: count):
  oldf: 3
  lang: 2
  patch: 2
  msg: 2
  id: 2
  y: 2
  repo: 1
  hunk: 1
  comment: 1
  ghid: 1
  new: 1
  ids: 1
  old: 1
  old_hunk: 1
  proj: 1
  idx: 1

Keys unique to each dataset:
  Code_Refinement unique keys (8): ['comment', 'ghid', 'hunk', 'ids', 'new', 'old', 'old_hunk', 'repo']
  Comment_Generation unique keys (0): []
  Diff_Quality_Estimation unique keys (2): ['idx', 'proj']
