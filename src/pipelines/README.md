# Pipeline Integration Guide

This guide shows how to integrate the HybridRetriever into your evaluation and UI pipelines.

## üìÅ Project Structure

```
src/
‚îú‚îÄ‚îÄ indexing/
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_retriever.py      # Main retriever class (PRODUCTION-READY)
‚îÇ   ‚îú‚îÄ‚îÄ demo_retriever.py         # CLI demo/test script
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ pipelines/
    ‚îú‚îÄ‚îÄ evaluation_pipeline_template.py  # Test dataset evaluation
    ‚îî‚îÄ‚îÄ ui_pipeline_template.py          # Real-time UI integration
```

---

## üöÄ Quick Start: Using HybridRetriever

### Basic Usage

```python
from src.indexing.hybrid_retriever import HybridRetriever

# Initialize (loads indexes automatically)
retriever = HybridRetriever()

# Retrieve examples
results = retriever.retrieve(patch=code_patch, top_k=5)

# Format for LLM prompt
formatted_prompt = retriever.format_for_llm_prompt(results)
```

### Optimal Configuration (From Evaluation)

```python
retriever = HybridRetriever(
    similarity_threshold=0.6,  # Filters low-similarity results
    use_ivf_index=True,        # Fast approximate search
    parallel_search=True       # Parallel dense + sparse search
)
```

**Performance**: ~2 seconds per retrieval with K=5 on MPS (Apple Silicon)

**Evaluation Results** (23,422 validation samples):
- MAP@K=1.0000
- Recall@K=1.0000
- Precision@K=1.0000

---

## üìä Pipeline 1: Evaluation (Test Dataset)

**Purpose**: Evaluate LLM on test dataset with metrics

**Flow**: Test Data ‚Üí Retriever ‚Üí LLM ‚Üí Metrics

### File Structure

```python
# src/pipelines/evaluation_pipeline_template.py

class EvaluationPipeline:
    def initialize_retriever(self):
        """‚úÖ IMPLEMENTED - Ready to use"""
        self.retriever = HybridRetriever(
            similarity_threshold=0.6,
            use_ivf_index=True
        )
    
    def retrieve_examples(self, query_patch: str):
        """‚úÖ IMPLEMENTED - Ready to use"""
        return self.retriever.retrieve(patch=query_patch, top_k=5)
    
    def generate_review_with_llm(self, query_patch, examples):
        """‚ùå TODO: Your teammate implements LLM call"""
        formatted = self.retriever.format_for_llm_prompt(examples)
        # Call your LLM here with formatted examples
        pass
    
    def compute_evaluation_metrics(self, generated, ground_truth):
        """‚ùå TODO: Your teammate implements metrics (BLEU, ROUGE, etc.)"""
        pass
```

### Usage

```bash
# Quick test (100 samples)
python src/pipelines/evaluation_pipeline_template.py \
    --test-dataset Datasets/Unified_Dataset/test.jsonl \
    --max-samples 100

# Full evaluation (23,273 samples)
python src/pipelines/evaluation_pipeline_template.py \
    --test-dataset Datasets/Unified_Dataset/test.jsonl
```

### What You Need to Implement

1. **LLM Integration** (`generate_review_with_llm`):
   - Build prompt with formatted examples
   - Call your LLM API (OpenAI, Anthropic, local model, etc.)
   - Return generated review

2. **Evaluation Metrics** (`compute_evaluation_metrics`):
   - BLEU score
   - ROUGE scores (1, 2, L)
   - Semantic similarity
   - Any other metrics

---

## üñ•Ô∏è Pipeline 2: Streamlit UI (Local Demo)

**Purpose**: Real-time code review demo with Streamlit

**Flow**: Streamlit UI ‚Üí Retriever ‚Üí LLM ‚Üí Display Results

### File Structure

```python
# UI/codeReviewerGUI.py (your Streamlit app)

import streamlit as st
from src.indexing.hybrid_retriever import HybridRetriever

@st.cache_resource
def load_retriever():
    """‚úÖ IMPLEMENTED - Loads once at startup"""
    return HybridRetriever(
        similarity_threshold=0.6,
        use_ivf_index=True
    )

retriever = load_retriever()

def retrieve_examples(code_patch: str):
    """‚úÖ IMPLEMENTED - Ready to use"""
    return retriever.retrieve(patch=code_patch, top_k=5)

def generate_review_with_llm(code_patch, examples):
    """‚ùå TODO: Your teammate implements LLM call"""
    formatted = retriever.format_for_llm_prompt(examples)
    # Call your LLM here
    pass

# Streamlit UI components
if st.button("Review Code"):
    examples = retrieve_examples(user_code)
    review = generate_review_with_llm(user_code, examples)
    st.success(review)
```

### Usage

```bash
# Run Streamlit app (starts local server automatically)
streamlit run UI/codeReviewerGUI.py

# Opens browser at http://localhost:8501
# No separate server setup needed!
```

### What You Need to Implement

1. **LLM Integration** (same as evaluation pipeline)

2. **Streamlit UI Integration**:
   - Import retriever in your Streamlit code
   - Call `retrieve()` when user submits code
   - Display retrieved examples
   - Call LLM to generate review
   - Display LLM-generated review

**Note**: Streamlit has a built-in local server. No Flask/FastAPI needed!

---

## üîß HybridRetriever API Reference

### Initialization

```python
retriever = HybridRetriever(
    index_dir="data/indexes",           # Path to indexes
    embedding_model="microsoft/codebert-base",  # Model for embeddings
    similarity_threshold=0.6,           # Min similarity (0.6 optimal)
    use_ivf_index=True,                 # Use fast IVF FAISS
    parallel_search=True,               # Parallel dense+sparse
    device=None                         # Auto-detect MPS/CUDA/CPU
)
```

### Main Methods

#### `retrieve()`

```python
results = retriever.retrieve(
    patch="def foo(): return 1",  # Code patch to query
    top_k=5,                       # Number of results (5 optimal)
    apply_similarity_threshold=True  # Filter by threshold
)

# Returns: List[Dict] with keys:
# - 'original_patch': The patch from dataset
# - 'review_comment': The review comment
# - 'retrieval_score': RRF fusion score
# - 'semantic_similarity': Cosine similarity (if filtered)
# - 'source_dataset': Source (ref/msg/cls)
# - 'language': Programming language
# - Other metadata...
```

#### `format_for_llm_prompt()`

```python
formatted = retriever.format_for_llm_prompt(results)

# Returns formatted string:
# Example 1:
# Code Patch:
# <patch text>
# 
# Review Comment:
# <review text>
# 
# Example 2:
# ...
```

#### `retrieve_and_format()` (Convenience)

```python
results, formatted = retriever.retrieve_and_format(
    patch=code_patch,
    top_k=5
)

# Returns tuple: (results_list, formatted_string)
```

---

## üß™ Testing the Retriever

### CLI Demo

```bash
# Test with simple patch
python src/indexing/demo_retriever.py \
    --patch 'def calculate_sum(a, b): return a + b'

# Show LLM-formatted output
python src/indexing/demo_retriever.py \
    --patch 'def foo(): return 1' \
    --show-formatted

# Performance test with timing
python src/indexing/demo_retriever.py \
    --patch 'def test(): pass' \
    --no-timing
```

### Python Script

```python
from src.indexing.hybrid_retriever import HybridRetriever

# Initialize
retriever = HybridRetriever()

# Test retrieval
results = retriever.retrieve(
    patch="def calculate_average(numbers): return sum(numbers) / len(numbers)",
    top_k=5
)

# Print results
for i, result in enumerate(results, 1):
    print(f"\n=== Example {i} ===")
    print(f"Score: {result['retrieval_score']:.4f}")
    print(f"Similarity: {result.get('semantic_similarity', 'N/A'):.4f}")
    print(f"Review: {result['review_comment'][:100]}...")
```

---

## üìà Performance Metrics

| Component | Time (ms) | Notes |
|-----------|-----------|-------|
| Query Embedding | ~945 | MPS accelerated |
| FAISS Search | ~885 | IVF approximate |
| BM25 Search | ~12 | bm25s optimized |
| MongoDB Fetch | ~50 | Local connection |
| **Total** | **~2000** | Per retrieval |

**Scalability**: With 23,273 test samples:
- Sequential: ~13 hours
- With batching: ~2-3 hours (if needed)

---

## üîÑ Integration Workflow

### Your Part (‚úÖ DONE)

1. ‚úÖ HybridRetriever class production-ready
2. ‚úÖ Optimal K=5, threshold=0.6 configured
3. ‚úÖ Helper methods for LLM formatting
4. ‚úÖ Demo script for testing
5. ‚úÖ Pipeline templates created

### Your Teammate's Part (‚ùå TODO)

**For Evaluation Pipeline:**
1. ‚ùå Implement `generate_review_with_llm()` - Call LLM API with examples
2. ‚ùå Implement `compute_evaluation_metrics()` - BLEU, ROUGE, etc.
3. ‚ùå Run full evaluation on test dataset

**For UI Pipeline:**
1. ‚ùå Implement `generate_review_with_llm()` - Same as above
2. ‚ùå Implement `start_server()` - Flask/FastAPI endpoint
3. ‚ùå UI integration - Send/receive from API

### Integration Points

```python
# Your teammate imports:
from src.indexing.hybrid_retriever import HybridRetriever

# Initialize once:
retriever = HybridRetriever()

# In pipeline:
examples = retriever.retrieve(patch=code, top_k=5)
formatted = retriever.format_for_llm_prompt(examples)
llm_response = call_llm(formatted)  # ‚Üê Your teammate implements
```

---

## üìù Example: Complete Integration

```python
from src.indexing.hybrid_retriever import HybridRetriever
import openai  # or your LLM library

class MyPipeline:
    def __init__(self):
        # Initialize retriever
        self.retriever = HybridRetriever()
    
    def review_code(self, code_patch: str) -> str:
        # 1. Retrieve examples (YOUR PART - DONE)
        examples = self.retriever.retrieve(patch=code_patch, top_k=5)
        formatted = self.retriever.format_for_llm_prompt(examples)
        
        # 2. Build prompt
        prompt = f"""You are a code reviewer. Based on these examples:

{formatted}

Review this code:
{code_patch}

Review:"""
        
        # 3. Call LLM (YOUR TEAMMATE'S PART)
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.choices[0].message.content

# Usage
pipeline = MyPipeline()
review = pipeline.review_code("def foo(): return 1")
print(review)
```

---

## üêõ Troubleshooting

### "No module named 'src.indexing'"

```python
# Add to top of your file:
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
```

### "Indexes not found"

```bash
# Make sure indexes are built:
python src/indexing/build_indexes.py --dataset Datasets/Unified_Dataset/train.jsonl
```

### "MongoDB connection error"

```bash
# Start MongoDB:
brew services start mongodb-community@7.0
```

### Slow retrieval

- Verify IVF index is loaded: `use_ivf_index=True`
- Check device: `retriever.device` should show 'mps' or 'cuda'
- Verify bm25s installed: `pip list | grep bm25s`

---

## üìû Support

For questions about the retriever:
- Check demo: `python src/indexing/demo_retriever.py --help`
- Check code: See docstrings in `hybrid_retriever.py`
- Test it: Run demo script with sample patches

For questions about LLM/UI integration:
- Ask your teammate üòä

---

**Happy Coding! üöÄ**
